[
    {
        "label": "socket",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "socket",
        "description": "socket",
        "detail": "socket",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "cv2",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "cv2",
        "description": "cv2",
        "detail": "cv2",
        "documentation": {}
    },
    {
        "label": "io",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "io",
        "description": "io",
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "dot",
        "importPath": "numpy",
        "description": "numpy",
        "isExtraImport": true,
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "dot",
        "importPath": "numpy",
        "description": "numpy",
        "isExtraImport": true,
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "dot",
        "importPath": "numpy",
        "description": "numpy",
        "isExtraImport": true,
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "dot",
        "importPath": "numpy",
        "description": "numpy",
        "isExtraImport": true,
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "sleep",
        "importPath": "time",
        "description": "time",
        "isExtraImport": true,
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "time",
        "importPath": "time",
        "description": "time",
        "isExtraImport": true,
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "sleep",
        "importPath": "time",
        "description": "time",
        "isExtraImport": true,
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "time",
        "importPath": "time",
        "description": "time",
        "isExtraImport": true,
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "time",
        "importPath": "time",
        "description": "time",
        "isExtraImport": true,
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "datetime",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "datetime",
        "description": "datetime",
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "struct",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "struct",
        "description": "struct",
        "detail": "struct",
        "documentation": {}
    },
    {
        "label": "mediapipe",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "mediapipe",
        "description": "mediapipe",
        "detail": "mediapipe",
        "documentation": {}
    },
    {
        "label": "mediapipe_pose",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "mediapipe_pose",
        "description": "mediapipe_pose",
        "detail": "mediapipe_pose",
        "documentation": {}
    },
    {
        "label": "client",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "client",
        "description": "client",
        "detail": "client",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "listdir",
        "importPath": "os",
        "description": "os",
        "isExtraImport": true,
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "listdir",
        "importPath": "os",
        "description": "os",
        "isExtraImport": true,
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "listdir",
        "importPath": "os",
        "description": "os",
        "isExtraImport": true,
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "listdir",
        "importPath": "os",
        "description": "os",
        "isExtraImport": true,
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "listdir",
        "importPath": "os",
        "description": "os",
        "isExtraImport": true,
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "threading,",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "threading.",
        "description": "threading.",
        "detail": "threading.",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "csv",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "csv",
        "description": "csv",
        "detail": "csv",
        "documentation": {}
    },
    {
        "label": "math",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math",
        "description": "math",
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "collections",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "collections",
        "description": "collections",
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "namedtuple",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "yaml",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "yaml",
        "description": "yaml",
        "detail": "yaml",
        "documentation": {}
    },
    {
        "label": "pyConTextNLP.pyConText",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pyConTextNLP.pyConText",
        "description": "pyConTextNLP.pyConText",
        "detail": "pyConTextNLP.pyConText",
        "documentation": {}
    },
    {
        "label": "pyConTextNLP.itemData",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pyConTextNLP.itemData",
        "description": "pyConTextNLP.itemData",
        "detail": "pyConTextNLP.itemData",
        "documentation": {}
    },
    {
        "label": "networkx",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "networkx",
        "description": "networkx",
        "detail": "networkx",
        "documentation": {}
    },
    {
        "label": "urllib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "urllib",
        "description": "urllib",
        "detail": "urllib",
        "documentation": {}
    },
    {
        "label": "torch.nn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn",
        "description": "torch.nn",
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "seed_everything",
        "importPath": "EMSAgent.default_sets",
        "description": "EMSAgent.default_sets",
        "isExtraImport": true,
        "detail": "EMSAgent.default_sets",
        "documentation": {}
    },
    {
        "label": "device",
        "importPath": "EMSAgent.default_sets",
        "description": "EMSAgent.default_sets",
        "isExtraImport": true,
        "detail": "EMSAgent.default_sets",
        "documentation": {}
    },
    {
        "label": "ungroup_p_node",
        "importPath": "EMSAgent.default_sets",
        "description": "EMSAgent.default_sets",
        "isExtraImport": true,
        "detail": "EMSAgent.default_sets",
        "documentation": {}
    },
    {
        "label": "device",
        "importPath": "EMSAgent.default_sets",
        "description": "EMSAgent.default_sets",
        "isExtraImport": true,
        "detail": "EMSAgent.default_sets",
        "documentation": {}
    },
    {
        "label": "dataset",
        "importPath": "EMSAgent.default_sets",
        "description": "EMSAgent.default_sets",
        "isExtraImport": true,
        "detail": "EMSAgent.default_sets",
        "documentation": {}
    },
    {
        "label": "SAVE_RESULT_ROOT",
        "importPath": "EMSAgent.default_sets",
        "description": "EMSAgent.default_sets",
        "isExtraImport": true,
        "detail": "EMSAgent.default_sets",
        "documentation": {}
    },
    {
        "label": "group_p_dict",
        "importPath": "EMSAgent.default_sets",
        "description": "EMSAgent.default_sets",
        "isExtraImport": true,
        "detail": "EMSAgent.default_sets",
        "documentation": {}
    },
    {
        "label": "device",
        "importPath": "EMSAgent.default_sets",
        "description": "EMSAgent.default_sets",
        "isExtraImport": true,
        "detail": "EMSAgent.default_sets",
        "documentation": {}
    },
    {
        "label": "p_node",
        "importPath": "EMSAgent.default_sets",
        "description": "EMSAgent.default_sets",
        "isExtraImport": true,
        "detail": "EMSAgent.default_sets",
        "documentation": {}
    },
    {
        "label": "reverse_group_p_dict",
        "importPath": "EMSAgent.default_sets",
        "description": "EMSAgent.default_sets",
        "isExtraImport": true,
        "detail": "EMSAgent.default_sets",
        "documentation": {}
    },
    {
        "label": "seed_everything",
        "importPath": "EMSAgent.default_sets",
        "description": "EMSAgent.default_sets",
        "isExtraImport": true,
        "detail": "EMSAgent.default_sets",
        "documentation": {}
    },
    {
        "label": "device",
        "importPath": "EMSAgent.default_sets",
        "description": "EMSAgent.default_sets",
        "isExtraImport": true,
        "detail": "EMSAgent.default_sets",
        "documentation": {}
    },
    {
        "label": "multi_graph",
        "importPath": "EMSAgent.default_sets",
        "description": "EMSAgent.default_sets",
        "isExtraImport": true,
        "detail": "EMSAgent.default_sets",
        "documentation": {}
    },
    {
        "label": "dataset",
        "importPath": "EMSAgent.default_sets",
        "description": "EMSAgent.default_sets",
        "isExtraImport": true,
        "detail": "EMSAgent.default_sets",
        "documentation": {}
    },
    {
        "label": "dataset",
        "importPath": "EMSAgent.default_sets",
        "description": "EMSAgent.default_sets",
        "isExtraImport": true,
        "detail": "EMSAgent.default_sets",
        "documentation": {}
    },
    {
        "label": "SAVE_RESULT_ROOT",
        "importPath": "EMSAgent.default_sets",
        "description": "EMSAgent.default_sets",
        "isExtraImport": true,
        "detail": "EMSAgent.default_sets",
        "documentation": {}
    },
    {
        "label": "ungroup_p_node",
        "importPath": "EMSAgent.default_sets",
        "description": "EMSAgent.default_sets",
        "isExtraImport": true,
        "detail": "EMSAgent.default_sets",
        "documentation": {}
    },
    {
        "label": "warnings",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "warnings",
        "description": "warnings",
        "detail": "warnings",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "AttrDict",
        "importPath": "EMSAgent.utils",
        "description": "EMSAgent.utils",
        "isExtraImport": true,
        "detail": "EMSAgent.utils",
        "documentation": {}
    },
    {
        "label": "onehot2p",
        "importPath": "EMSAgent.utils",
        "description": "EMSAgent.utils",
        "isExtraImport": true,
        "detail": "EMSAgent.utils",
        "documentation": {}
    },
    {
        "label": "convert_label",
        "importPath": "EMSAgent.utils",
        "description": "EMSAgent.utils",
        "isExtraImport": true,
        "detail": "EMSAgent.utils",
        "documentation": {}
    },
    {
        "label": "sortby",
        "importPath": "EMSAgent.utils",
        "description": "EMSAgent.utils",
        "isExtraImport": true,
        "detail": "EMSAgent.utils",
        "documentation": {}
    },
    {
        "label": "removePunctuation",
        "importPath": "EMSAgent.utils",
        "description": "EMSAgent.utils",
        "isExtraImport": true,
        "detail": "EMSAgent.utils",
        "documentation": {}
    },
    {
        "label": "text_remove_double_space",
        "importPath": "EMSAgent.utils",
        "description": "EMSAgent.utils",
        "isExtraImport": true,
        "detail": "EMSAgent.utils",
        "documentation": {}
    },
    {
        "label": "AttrDict",
        "importPath": "EMSAgent.utils",
        "description": "EMSAgent.utils",
        "isExtraImport": true,
        "detail": "EMSAgent.utils",
        "documentation": {}
    },
    {
        "label": "onehot2p",
        "importPath": "EMSAgent.utils",
        "description": "EMSAgent.utils",
        "isExtraImport": true,
        "detail": "EMSAgent.utils",
        "documentation": {}
    },
    {
        "label": "HeteroGraph",
        "importPath": "EMSAgent.Heterogeneous_graph",
        "description": "EMSAgent.Heterogeneous_graph",
        "isExtraImport": true,
        "detail": "EMSAgent.Heterogeneous_graph",
        "documentation": {}
    },
    {
        "label": "HeteroGraph",
        "importPath": "EMSAgent.Heterogeneous_graph",
        "description": "EMSAgent.Heterogeneous_graph",
        "isExtraImport": true,
        "detail": "EMSAgent.Heterogeneous_graph",
        "documentation": {}
    },
    {
        "label": "EMSMultiModel",
        "importPath": "EMSAgent.model",
        "description": "EMSAgent.model",
        "isExtraImport": true,
        "detail": "EMSAgent.model",
        "documentation": {}
    },
    {
        "label": "EMSMultiModel",
        "importPath": "EMSAgent.model",
        "description": "EMSAgent.model",
        "isExtraImport": true,
        "detail": "EMSAgent.model",
        "documentation": {}
    },
    {
        "label": "BertTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoModel",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "BertTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoModel",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoConfig",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoModelForSequenceClassification",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "BertTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "BertModel",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "pipeline",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "pipeline",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "WhisperForConditionalGeneration",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "WhisperProcessor",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tqdm",
        "description": "tqdm",
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "FeedbackObj",
        "importPath": "classes",
        "description": "classes",
        "isExtraImport": true,
        "detail": "classes",
        "documentation": {}
    },
    {
        "label": "TranscriptItem",
        "importPath": "classes",
        "description": "classes",
        "isExtraImport": true,
        "detail": "classes",
        "documentation": {}
    },
    {
        "label": "TranscriptItem",
        "importPath": "classes",
        "description": "classes",
        "isExtraImport": true,
        "detail": "classes",
        "documentation": {}
    },
    {
        "label": "SpeechNLPItem",
        "importPath": "classes",
        "description": "classes",
        "isExtraImport": true,
        "detail": "classes",
        "documentation": {}
    },
    {
        "label": "GUISignal",
        "importPath": "classes",
        "description": "classes",
        "isExtraImport": true,
        "detail": "classes",
        "documentation": {}
    },
    {
        "label": "SpeechNLPItem",
        "importPath": "classes",
        "description": "classes",
        "isExtraImport": true,
        "detail": "classes",
        "documentation": {}
    },
    {
        "label": "GUISignal",
        "importPath": "classes",
        "description": "classes",
        "isExtraImport": true,
        "detail": "classes",
        "documentation": {}
    },
    {
        "label": "SpeechNLPItem",
        "importPath": "classes",
        "description": "classes",
        "isExtraImport": true,
        "detail": "classes",
        "documentation": {}
    },
    {
        "label": "GUISignal",
        "importPath": "classes",
        "description": "classes",
        "isExtraImport": true,
        "detail": "classes",
        "documentation": {}
    },
    {
        "label": "TranscriptItem",
        "importPath": "classes",
        "description": "classes",
        "isExtraImport": true,
        "detail": "classes",
        "documentation": {}
    },
    {
        "label": "GUISignal",
        "importPath": "classes",
        "description": "classes",
        "isExtraImport": true,
        "detail": "classes",
        "documentation": {}
    },
    {
        "label": "SpeechNLPItem",
        "importPath": "classes",
        "description": "classes",
        "isExtraImport": true,
        "detail": "classes",
        "documentation": {}
    },
    {
        "label": "GUISignal",
        "importPath": "classes",
        "description": "classes",
        "isExtraImport": true,
        "detail": "classes",
        "documentation": {}
    },
    {
        "label": "SpeechNLPItem",
        "importPath": "classes",
        "description": "classes",
        "isExtraImport": true,
        "detail": "classes",
        "documentation": {}
    },
    {
        "label": "GUISignal",
        "importPath": "classes",
        "description": "classes",
        "isExtraImport": true,
        "detail": "classes",
        "documentation": {}
    },
    {
        "label": "SpeechNLPItem",
        "importPath": "classes",
        "description": "classes",
        "isExtraImport": true,
        "detail": "classes",
        "documentation": {}
    },
    {
        "label": "GUISignal",
        "importPath": "classes",
        "description": "classes",
        "isExtraImport": true,
        "detail": "classes",
        "documentation": {}
    },
    {
        "label": "TranscriptItem",
        "importPath": "classes",
        "description": "classes",
        "isExtraImport": true,
        "detail": "classes",
        "documentation": {}
    },
    {
        "label": "SpeechNLPItem",
        "importPath": "classes",
        "description": "classes",
        "isExtraImport": true,
        "detail": "classes",
        "documentation": {}
    },
    {
        "label": "GUISignal",
        "importPath": "classes",
        "description": "classes",
        "isExtraImport": true,
        "detail": "classes",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "pipeline_config",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pipeline_config",
        "description": "pipeline_config",
        "detail": "pipeline_config",
        "documentation": {}
    },
    {
        "label": "HeteroData",
        "importPath": "torch_geometric.data",
        "description": "torch_geometric.data",
        "isExtraImport": true,
        "detail": "torch_geometric.data",
        "documentation": {}
    },
    {
        "label": "Data",
        "importPath": "torch_geometric.data",
        "description": "torch_geometric.data",
        "isExtraImport": true,
        "detail": "torch_geometric.data",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "torch.multiprocessing",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.multiprocessing",
        "description": "torch.multiprocessing",
        "detail": "torch.multiprocessing",
        "documentation": {}
    },
    {
        "label": "HGTConv",
        "importPath": "torch_geometric.nn",
        "description": "torch_geometric.nn",
        "isExtraImport": true,
        "detail": "torch_geometric.nn",
        "documentation": {}
    },
    {
        "label": "GATConv",
        "importPath": "torch_geometric.nn",
        "description": "torch_geometric.nn",
        "isExtraImport": true,
        "detail": "torch_geometric.nn",
        "documentation": {}
    },
    {
        "label": "GCNConv",
        "importPath": "torch_geometric.nn",
        "description": "torch_geometric.nn",
        "isExtraImport": true,
        "detail": "torch_geometric.nn",
        "documentation": {}
    },
    {
        "label": "Linear",
        "importPath": "torch_geometric.nn",
        "description": "torch_geometric.nn",
        "isExtraImport": true,
        "detail": "torch_geometric.nn",
        "documentation": {}
    },
    {
        "label": "torch.nn.functional",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn.functional",
        "description": "torch.nn.functional",
        "detail": "torch.nn.functional",
        "documentation": {}
    },
    {
        "label": "tensorflow",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tensorflow",
        "description": "tensorflow",
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "AugmentationMethod",
        "importPath": "tensorflow_asr.augmentations.methods.base_method",
        "description": "tensorflow_asr.augmentations.methods.base_method",
        "isExtraImport": true,
        "detail": "tensorflow_asr.augmentations.methods.base_method",
        "documentation": {}
    },
    {
        "label": "AugmentationMethod",
        "importPath": "tensorflow_asr.augmentations.methods.base_method",
        "description": "tensorflow_asr.augmentations.methods.base_method",
        "isExtraImport": true,
        "detail": "tensorflow_asr.augmentations.methods.base_method",
        "documentation": {}
    },
    {
        "label": "shape_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "file_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "data_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "feature_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "file_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "math_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "file_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "file_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "app_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "env_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "data_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "math_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "shape_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "layer_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "math_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "math_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "shape_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "math_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "math_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "shape_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "data_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "layer_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "math_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "shape_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "data_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "math_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "layer_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "math_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "shape_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "env_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "file_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "env_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "env_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "env_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "env_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "shape_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "file_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "data_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "feature_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "file_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "math_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "env_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "math_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "file_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "file_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "app_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "env_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "data_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "math_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "shape_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "layer_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "math_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "math_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "shape_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "math_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "math_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "shape_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "data_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "layer_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "math_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "shape_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "data_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "math_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "layer_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "math_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "shape_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "env_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "file_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "shape_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "math_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "env_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "env_util",
        "importPath": "tensorflow_asr.utils",
        "description": "tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "specaugment",
        "importPath": "tensorflow_asr.augmentations.methods",
        "description": "tensorflow_asr.augmentations.methods",
        "isExtraImport": true,
        "detail": "tensorflow_asr.augmentations.methods",
        "documentation": {}
    },
    {
        "label": "specaugment",
        "importPath": "tensorflow_asr.augmentations.methods",
        "description": "tensorflow_asr.augmentations.methods",
        "isExtraImport": true,
        "detail": "tensorflow_asr.augmentations.methods",
        "documentation": {}
    },
    {
        "label": "typing",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "typing",
        "description": "typing",
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Augmentation",
        "importPath": "tensorflow_asr.augmentations.augmentation",
        "description": "tensorflow_asr.augmentations.augmentation",
        "isExtraImport": true,
        "detail": "tensorflow_asr.augmentations.augmentation",
        "documentation": {}
    },
    {
        "label": "Augmentation",
        "importPath": "tensorflow_asr.augmentations.augmentation",
        "description": "tensorflow_asr.augmentations.augmentation",
        "isExtraImport": true,
        "detail": "tensorflow_asr.augmentations.augmentation",
        "documentation": {}
    },
    {
        "label": "Augmentation",
        "importPath": "tensorflow_asr.augmentations.augmentation",
        "description": "tensorflow_asr.augmentations.augmentation",
        "isExtraImport": true,
        "detail": "tensorflow_asr.augmentations.augmentation",
        "documentation": {}
    },
    {
        "label": "Augmentation",
        "importPath": "tensorflow_asr.augmentations.augmentation",
        "description": "tensorflow_asr.augmentations.augmentation",
        "isExtraImport": true,
        "detail": "tensorflow_asr.augmentations.augmentation",
        "documentation": {}
    },
    {
        "label": "Augmentation",
        "importPath": "tensorflow_asr.augmentations.augmentation",
        "description": "tensorflow_asr.augmentations.augmentation",
        "isExtraImport": true,
        "detail": "tensorflow_asr.augmentations.augmentation",
        "documentation": {}
    },
    {
        "label": "Augmentation",
        "importPath": "tensorflow_asr.augmentations.augmentation",
        "description": "tensorflow_asr.augmentations.augmentation",
        "isExtraImport": true,
        "detail": "tensorflow_asr.augmentations.augmentation",
        "documentation": {}
    },
    {
        "label": "AUTOTUNE",
        "importPath": "tensorflow_asr.datasets.base_dataset",
        "description": "tensorflow_asr.datasets.base_dataset",
        "isExtraImport": true,
        "detail": "tensorflow_asr.datasets.base_dataset",
        "documentation": {}
    },
    {
        "label": "BUFFER_SIZE",
        "importPath": "tensorflow_asr.datasets.base_dataset",
        "description": "tensorflow_asr.datasets.base_dataset",
        "isExtraImport": true,
        "detail": "tensorflow_asr.datasets.base_dataset",
        "documentation": {}
    },
    {
        "label": "TFRECORD_SHARDS",
        "importPath": "tensorflow_asr.datasets.base_dataset",
        "description": "tensorflow_asr.datasets.base_dataset",
        "isExtraImport": true,
        "detail": "tensorflow_asr.datasets.base_dataset",
        "documentation": {}
    },
    {
        "label": "BaseDataset",
        "importPath": "tensorflow_asr.datasets.base_dataset",
        "description": "tensorflow_asr.datasets.base_dataset",
        "isExtraImport": true,
        "detail": "tensorflow_asr.datasets.base_dataset",
        "documentation": {}
    },
    {
        "label": "AUTOTUNE",
        "importPath": "tensorflow_asr.datasets.base_dataset",
        "description": "tensorflow_asr.datasets.base_dataset",
        "isExtraImport": true,
        "detail": "tensorflow_asr.datasets.base_dataset",
        "documentation": {}
    },
    {
        "label": "BUFFER_SIZE",
        "importPath": "tensorflow_asr.datasets.base_dataset",
        "description": "tensorflow_asr.datasets.base_dataset",
        "isExtraImport": true,
        "detail": "tensorflow_asr.datasets.base_dataset",
        "documentation": {}
    },
    {
        "label": "TFRECORD_SHARDS",
        "importPath": "tensorflow_asr.datasets.base_dataset",
        "description": "tensorflow_asr.datasets.base_dataset",
        "isExtraImport": true,
        "detail": "tensorflow_asr.datasets.base_dataset",
        "documentation": {}
    },
    {
        "label": "BaseDataset",
        "importPath": "tensorflow_asr.datasets.base_dataset",
        "description": "tensorflow_asr.datasets.base_dataset",
        "isExtraImport": true,
        "detail": "tensorflow_asr.datasets.base_dataset",
        "documentation": {}
    },
    {
        "label": "SpeechFeaturizer",
        "importPath": "tensorflow_asr.featurizers.speech_featurizers",
        "description": "tensorflow_asr.featurizers.speech_featurizers",
        "isExtraImport": true,
        "detail": "tensorflow_asr.featurizers.speech_featurizers",
        "documentation": {}
    },
    {
        "label": "load_and_convert_to_wav",
        "importPath": "tensorflow_asr.featurizers.speech_featurizers",
        "description": "tensorflow_asr.featurizers.speech_featurizers",
        "isExtraImport": true,
        "detail": "tensorflow_asr.featurizers.speech_featurizers",
        "documentation": {}
    },
    {
        "label": "read_raw_audio",
        "importPath": "tensorflow_asr.featurizers.speech_featurizers",
        "description": "tensorflow_asr.featurizers.speech_featurizers",
        "isExtraImport": true,
        "detail": "tensorflow_asr.featurizers.speech_featurizers",
        "documentation": {}
    },
    {
        "label": "tf_read_raw_audio",
        "importPath": "tensorflow_asr.featurizers.speech_featurizers",
        "description": "tensorflow_asr.featurizers.speech_featurizers",
        "isExtraImport": true,
        "detail": "tensorflow_asr.featurizers.speech_featurizers",
        "documentation": {}
    },
    {
        "label": "SpeechFeaturizer",
        "importPath": "tensorflow_asr.featurizers.speech_featurizers",
        "description": "tensorflow_asr.featurizers.speech_featurizers",
        "isExtraImport": true,
        "detail": "tensorflow_asr.featurizers.speech_featurizers",
        "documentation": {}
    },
    {
        "label": "TFSpeechFeaturizer",
        "importPath": "tensorflow_asr.featurizers.speech_featurizers",
        "description": "tensorflow_asr.featurizers.speech_featurizers",
        "isExtraImport": true,
        "detail": "tensorflow_asr.featurizers.speech_featurizers",
        "documentation": {}
    },
    {
        "label": "SpeechFeaturizer",
        "importPath": "tensorflow_asr.featurizers.speech_featurizers",
        "description": "tensorflow_asr.featurizers.speech_featurizers",
        "isExtraImport": true,
        "detail": "tensorflow_asr.featurizers.speech_featurizers",
        "documentation": {}
    },
    {
        "label": "read_raw_audio",
        "importPath": "tensorflow_asr.featurizers.speech_featurizers",
        "description": "tensorflow_asr.featurizers.speech_featurizers",
        "isExtraImport": true,
        "detail": "tensorflow_asr.featurizers.speech_featurizers",
        "documentation": {}
    },
    {
        "label": "read_raw_audio",
        "importPath": "tensorflow_asr.featurizers.speech_featurizers",
        "description": "tensorflow_asr.featurizers.speech_featurizers",
        "isExtraImport": true,
        "detail": "tensorflow_asr.featurizers.speech_featurizers",
        "documentation": {}
    },
    {
        "label": "read_raw_audio",
        "importPath": "tensorflow_asr.featurizers.speech_featurizers",
        "description": "tensorflow_asr.featurizers.speech_featurizers",
        "isExtraImport": true,
        "detail": "tensorflow_asr.featurizers.speech_featurizers",
        "documentation": {}
    },
    {
        "label": "read_raw_audio",
        "importPath": "tensorflow_asr.featurizers.speech_featurizers",
        "description": "tensorflow_asr.featurizers.speech_featurizers",
        "isExtraImport": true,
        "detail": "tensorflow_asr.featurizers.speech_featurizers",
        "documentation": {}
    },
    {
        "label": "read_raw_audio",
        "importPath": "tensorflow_asr.featurizers.speech_featurizers",
        "description": "tensorflow_asr.featurizers.speech_featurizers",
        "isExtraImport": true,
        "detail": "tensorflow_asr.featurizers.speech_featurizers",
        "documentation": {}
    },
    {
        "label": "SpeechFeaturizer",
        "importPath": "tensorflow_asr.featurizers.speech_featurizers",
        "description": "tensorflow_asr.featurizers.speech_featurizers",
        "isExtraImport": true,
        "detail": "tensorflow_asr.featurizers.speech_featurizers",
        "documentation": {}
    },
    {
        "label": "load_and_convert_to_wav",
        "importPath": "tensorflow_asr.featurizers.speech_featurizers",
        "description": "tensorflow_asr.featurizers.speech_featurizers",
        "isExtraImport": true,
        "detail": "tensorflow_asr.featurizers.speech_featurizers",
        "documentation": {}
    },
    {
        "label": "read_raw_audio",
        "importPath": "tensorflow_asr.featurizers.speech_featurizers",
        "description": "tensorflow_asr.featurizers.speech_featurizers",
        "isExtraImport": true,
        "detail": "tensorflow_asr.featurizers.speech_featurizers",
        "documentation": {}
    },
    {
        "label": "tf_read_raw_audio",
        "importPath": "tensorflow_asr.featurizers.speech_featurizers",
        "description": "tensorflow_asr.featurizers.speech_featurizers",
        "isExtraImport": true,
        "detail": "tensorflow_asr.featurizers.speech_featurizers",
        "documentation": {}
    },
    {
        "label": "SpeechFeaturizer",
        "importPath": "tensorflow_asr.featurizers.speech_featurizers",
        "description": "tensorflow_asr.featurizers.speech_featurizers",
        "isExtraImport": true,
        "detail": "tensorflow_asr.featurizers.speech_featurizers",
        "documentation": {}
    },
    {
        "label": "TFSpeechFeaturizer",
        "importPath": "tensorflow_asr.featurizers.speech_featurizers",
        "description": "tensorflow_asr.featurizers.speech_featurizers",
        "isExtraImport": true,
        "detail": "tensorflow_asr.featurizers.speech_featurizers",
        "documentation": {}
    },
    {
        "label": "SpeechFeaturizer",
        "importPath": "tensorflow_asr.featurizers.speech_featurizers",
        "description": "tensorflow_asr.featurizers.speech_featurizers",
        "isExtraImport": true,
        "detail": "tensorflow_asr.featurizers.speech_featurizers",
        "documentation": {}
    },
    {
        "label": "TextFeaturizer",
        "importPath": "tensorflow_asr.featurizers.text_featurizers",
        "description": "tensorflow_asr.featurizers.text_featurizers",
        "isExtraImport": true,
        "detail": "tensorflow_asr.featurizers.text_featurizers",
        "documentation": {}
    },
    {
        "label": "TextFeaturizer",
        "importPath": "tensorflow_asr.featurizers.text_featurizers",
        "description": "tensorflow_asr.featurizers.text_featurizers",
        "isExtraImport": true,
        "detail": "tensorflow_asr.featurizers.text_featurizers",
        "documentation": {}
    },
    {
        "label": "TextFeaturizer",
        "importPath": "tensorflow_asr.featurizers.text_featurizers",
        "description": "tensorflow_asr.featurizers.text_featurizers",
        "isExtraImport": true,
        "detail": "tensorflow_asr.featurizers.text_featurizers",
        "documentation": {}
    },
    {
        "label": "TextFeaturizer",
        "importPath": "tensorflow_asr.featurizers.text_featurizers",
        "description": "tensorflow_asr.featurizers.text_featurizers",
        "isExtraImport": true,
        "detail": "tensorflow_asr.featurizers.text_featurizers",
        "documentation": {}
    },
    {
        "label": "TextFeaturizer",
        "importPath": "tensorflow_asr.featurizers.text_featurizers",
        "description": "tensorflow_asr.featurizers.text_featurizers",
        "isExtraImport": true,
        "detail": "tensorflow_asr.featurizers.text_featurizers",
        "documentation": {}
    },
    {
        "label": "TextFeaturizer",
        "importPath": "tensorflow_asr.featurizers.text_featurizers",
        "description": "tensorflow_asr.featurizers.text_featurizers",
        "isExtraImport": true,
        "detail": "tensorflow_asr.featurizers.text_featurizers",
        "documentation": {}
    },
    {
        "label": "TextFeaturizer",
        "importPath": "tensorflow_asr.featurizers.text_featurizers",
        "description": "tensorflow_asr.featurizers.text_featurizers",
        "isExtraImport": true,
        "detail": "tensorflow_asr.featurizers.text_featurizers",
        "documentation": {}
    },
    {
        "label": "TextFeaturizer",
        "importPath": "tensorflow_asr.featurizers.text_featurizers",
        "description": "tensorflow_asr.featurizers.text_featurizers",
        "isExtraImport": true,
        "detail": "tensorflow_asr.featurizers.text_featurizers",
        "documentation": {}
    },
    {
        "label": "abc",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "abc",
        "description": "abc",
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "shape_list",
        "importPath": "EMSConformer.inference.tensorflow_asr.utils.shape_util",
        "description": "EMSConformer.inference.tensorflow_asr.utils.shape_util",
        "isExtraImport": true,
        "detail": "EMSConformer.inference.tensorflow_asr.utils.shape_util",
        "documentation": {}
    },
    {
        "label": "librosa",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "librosa",
        "description": "librosa",
        "detail": "librosa",
        "documentation": {}
    },
    {
        "label": "soundfile",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "soundfile",
        "description": "soundfile",
        "detail": "soundfile",
        "documentation": {}
    },
    {
        "label": "tensorflow_io",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tensorflow_io",
        "description": "tensorflow_io",
        "detail": "tensorflow_io",
        "documentation": {}
    },
    {
        "label": "env_util",
        "importPath": "EMSConformer.inference.tensorflow_asr.utils",
        "description": "EMSConformer.inference.tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "EMSConformer.inference.tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "math_util",
        "importPath": "EMSConformer.inference.tensorflow_asr.utils",
        "description": "EMSConformer.inference.tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "EMSConformer.inference.tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "shape_util",
        "importPath": "EMSConformer.inference.tensorflow_asr.utils",
        "description": "EMSConformer.inference.tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "EMSConformer.inference.tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "math_util",
        "importPath": "EMSConformer.inference.tensorflow_asr.utils",
        "description": "EMSConformer.inference.tensorflow_asr.utils",
        "isExtraImport": true,
        "detail": "EMSConformer.inference.tensorflow_asr.utils",
        "documentation": {}
    },
    {
        "label": "gammatone",
        "importPath": "EMSConformer.inference.tensorflow_asr.featurizers.methods",
        "description": "EMSConformer.inference.tensorflow_asr.featurizers.methods",
        "isExtraImport": true,
        "detail": "EMSConformer.inference.tensorflow_asr.featurizers.methods",
        "documentation": {}
    },
    {
        "label": "codecs",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "codecs",
        "description": "codecs",
        "detail": "codecs",
        "documentation": {}
    },
    {
        "label": "unicodedata",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "unicodedata",
        "description": "unicodedata",
        "detail": "unicodedata",
        "documentation": {}
    },
    {
        "label": "multiprocessing",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "multiprocessing",
        "description": "multiprocessing",
        "detail": "multiprocessing",
        "documentation": {}
    },
    {
        "label": "cpu_count",
        "importPath": "multiprocessing",
        "description": "multiprocessing",
        "isExtraImport": true,
        "detail": "multiprocessing",
        "documentation": {}
    },
    {
        "label": "cpu_count",
        "importPath": "multiprocessing",
        "description": "multiprocessing",
        "isExtraImport": true,
        "detail": "multiprocessing",
        "documentation": {}
    },
    {
        "label": "sentencepiece",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sentencepiece",
        "description": "sentencepiece",
        "detail": "sentencepiece",
        "documentation": {}
    },
    {
        "label": "tensorflow_datasets",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tensorflow_datasets",
        "description": "tensorflow_datasets",
        "detail": "tensorflow_datasets",
        "documentation": {}
    },
    {
        "label": "DecoderConfig",
        "importPath": "tensorflow_asr.configs.config",
        "description": "tensorflow_asr.configs.config",
        "isExtraImport": true,
        "detail": "tensorflow_asr.configs.config",
        "documentation": {}
    },
    {
        "label": "Config",
        "importPath": "tensorflow_asr.configs.config",
        "description": "tensorflow_asr.configs.config",
        "isExtraImport": true,
        "detail": "tensorflow_asr.configs.config",
        "documentation": {}
    },
    {
        "label": "Config",
        "importPath": "tensorflow_asr.configs.config",
        "description": "tensorflow_asr.configs.config",
        "isExtraImport": true,
        "detail": "tensorflow_asr.configs.config",
        "documentation": {}
    },
    {
        "label": "Config",
        "importPath": "tensorflow_asr.configs.config",
        "description": "tensorflow_asr.configs.config",
        "isExtraImport": true,
        "detail": "tensorflow_asr.configs.config",
        "documentation": {}
    },
    {
        "label": "Config",
        "importPath": "tensorflow_asr.configs.config",
        "description": "tensorflow_asr.configs.config",
        "isExtraImport": true,
        "detail": "tensorflow_asr.configs.config",
        "documentation": {}
    },
    {
        "label": "Config",
        "importPath": "tensorflow_asr.configs.config",
        "description": "tensorflow_asr.configs.config",
        "isExtraImport": true,
        "detail": "tensorflow_asr.configs.config",
        "documentation": {}
    },
    {
        "label": "DecoderConfig",
        "importPath": "tensorflow_asr.configs.config",
        "description": "tensorflow_asr.configs.config",
        "isExtraImport": true,
        "detail": "tensorflow_asr.configs.config",
        "documentation": {}
    },
    {
        "label": "Config",
        "importPath": "tensorflow_asr.configs.config",
        "description": "tensorflow_asr.configs.config",
        "isExtraImport": true,
        "detail": "tensorflow_asr.configs.config",
        "documentation": {}
    },
    {
        "label": "Config",
        "importPath": "tensorflow_asr.configs.config",
        "description": "tensorflow_asr.configs.config",
        "isExtraImport": true,
        "detail": "tensorflow_asr.configs.config",
        "documentation": {}
    },
    {
        "label": "Config",
        "importPath": "tensorflow_asr.configs.config",
        "description": "tensorflow_asr.configs.config",
        "isExtraImport": true,
        "detail": "tensorflow_asr.configs.config",
        "documentation": {}
    },
    {
        "label": "Config",
        "importPath": "tensorflow_asr.configs.config",
        "description": "tensorflow_asr.configs.config",
        "isExtraImport": true,
        "detail": "tensorflow_asr.configs.config",
        "documentation": {}
    },
    {
        "label": "asr_dataset",
        "importPath": "tensorflow_asr.datasets",
        "description": "tensorflow_asr.datasets",
        "isExtraImport": true,
        "detail": "tensorflow_asr.datasets",
        "documentation": {}
    },
    {
        "label": "asr_dataset",
        "importPath": "tensorflow_asr.datasets",
        "description": "tensorflow_asr.datasets",
        "isExtraImport": true,
        "detail": "tensorflow_asr.datasets",
        "documentation": {}
    },
    {
        "label": "ASRSliceDataset",
        "importPath": "tensorflow_asr.datasets.asr_dataset",
        "description": "tensorflow_asr.datasets.asr_dataset",
        "isExtraImport": true,
        "detail": "tensorflow_asr.datasets.asr_dataset",
        "documentation": {}
    },
    {
        "label": "ASRSliceDataset",
        "importPath": "tensorflow_asr.datasets.asr_dataset",
        "description": "tensorflow_asr.datasets.asr_dataset",
        "isExtraImport": true,
        "detail": "tensorflow_asr.datasets.asr_dataset",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "tensorflow_asr.models.base_model",
        "description": "tensorflow_asr.models.base_model",
        "isExtraImport": true,
        "detail": "tensorflow_asr.models.base_model",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "tensorflow_asr.models.base_model",
        "description": "tensorflow_asr.models.base_model",
        "isExtraImport": true,
        "detail": "tensorflow_asr.models.base_model",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "tensorflow_asr.models.base_model",
        "description": "tensorflow_asr.models.base_model",
        "isExtraImport": true,
        "detail": "tensorflow_asr.models.base_model",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "tensorflow_asr.models.base_model",
        "description": "tensorflow_asr.models.base_model",
        "isExtraImport": true,
        "detail": "tensorflow_asr.models.base_model",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "tensorflow_asr.models.base_model",
        "description": "tensorflow_asr.models.base_model",
        "isExtraImport": true,
        "detail": "tensorflow_asr.models.base_model",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "tensorflow_asr.models.base_model",
        "description": "tensorflow_asr.models.base_model",
        "isExtraImport": true,
        "detail": "tensorflow_asr.models.base_model",
        "documentation": {}
    },
    {
        "label": "tempfile",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tempfile",
        "description": "tempfile",
        "detail": "tempfile",
        "documentation": {}
    },
    {
        "label": "speech_featurizers",
        "importPath": "tensorflow_asr.featurizers",
        "description": "tensorflow_asr.featurizers",
        "isExtraImport": true,
        "detail": "tensorflow_asr.featurizers",
        "documentation": {}
    },
    {
        "label": "text_featurizers",
        "importPath": "tensorflow_asr.featurizers",
        "description": "tensorflow_asr.featurizers",
        "isExtraImport": true,
        "detail": "tensorflow_asr.featurizers",
        "documentation": {}
    },
    {
        "label": "speech_featurizers",
        "importPath": "tensorflow_asr.featurizers",
        "description": "tensorflow_asr.featurizers",
        "isExtraImport": true,
        "detail": "tensorflow_asr.featurizers",
        "documentation": {}
    },
    {
        "label": "text_featurizers",
        "importPath": "tensorflow_asr.featurizers",
        "description": "tensorflow_asr.featurizers",
        "isExtraImport": true,
        "detail": "tensorflow_asr.featurizers",
        "documentation": {}
    },
    {
        "label": "matrix_diag_part_v2",
        "importPath": "tensorflow.python.ops.gen_array_ops",
        "description": "tensorflow.python.ops.gen_array_ops",
        "isExtraImport": true,
        "detail": "tensorflow.python.ops.gen_array_ops",
        "documentation": {}
    },
    {
        "label": "matrix_diag_part_v2",
        "importPath": "tensorflow.python.ops.gen_array_ops",
        "description": "tensorflow.python.ops.gen_array_ops",
        "isExtraImport": true,
        "detail": "tensorflow.python.ops.gen_array_ops",
        "documentation": {}
    },
    {
        "label": "CtcLoss",
        "importPath": "tensorflow_asr.losses.ctc_loss",
        "description": "tensorflow_asr.losses.ctc_loss",
        "isExtraImport": true,
        "detail": "tensorflow_asr.losses.ctc_loss",
        "documentation": {}
    },
    {
        "label": "CtcLoss",
        "importPath": "tensorflow_asr.losses.ctc_loss",
        "description": "tensorflow_asr.losses.ctc_loss",
        "isExtraImport": true,
        "detail": "tensorflow_asr.losses.ctc_loss",
        "documentation": {}
    },
    {
        "label": "RowConv1D",
        "importPath": "tensorflow_asr.models.layers.row_conv_1d",
        "description": "tensorflow_asr.models.layers.row_conv_1d",
        "isExtraImport": true,
        "detail": "tensorflow_asr.models.layers.row_conv_1d",
        "documentation": {}
    },
    {
        "label": "RowConv1D",
        "importPath": "tensorflow_asr.models.layers.row_conv_1d",
        "description": "tensorflow_asr.models.layers.row_conv_1d",
        "isExtraImport": true,
        "detail": "tensorflow_asr.models.layers.row_conv_1d",
        "documentation": {}
    },
    {
        "label": "SequenceBatchNorm",
        "importPath": "tensorflow_asr.models.layers.sequence_wise_bn",
        "description": "tensorflow_asr.models.layers.sequence_wise_bn",
        "isExtraImport": true,
        "detail": "tensorflow_asr.models.layers.sequence_wise_bn",
        "documentation": {}
    },
    {
        "label": "SequenceBatchNorm",
        "importPath": "tensorflow_asr.models.layers.sequence_wise_bn",
        "description": "tensorflow_asr.models.layers.sequence_wise_bn",
        "isExtraImport": true,
        "detail": "tensorflow_asr.models.layers.sequence_wise_bn",
        "documentation": {}
    },
    {
        "label": "CtcModel",
        "importPath": "tensorflow_asr.models.ctc.base_ctc",
        "description": "tensorflow_asr.models.ctc.base_ctc",
        "isExtraImport": true,
        "detail": "tensorflow_asr.models.ctc.base_ctc",
        "documentation": {}
    },
    {
        "label": "CtcModel",
        "importPath": "tensorflow_asr.models.ctc.base_ctc",
        "description": "tensorflow_asr.models.ctc.base_ctc",
        "isExtraImport": true,
        "detail": "tensorflow_asr.models.ctc.base_ctc",
        "documentation": {}
    },
    {
        "label": "CtcModel",
        "importPath": "tensorflow_asr.models.ctc.base_ctc",
        "description": "tensorflow_asr.models.ctc.base_ctc",
        "isExtraImport": true,
        "detail": "tensorflow_asr.models.ctc.base_ctc",
        "documentation": {}
    },
    {
        "label": "CtcModel",
        "importPath": "tensorflow_asr.models.ctc.base_ctc",
        "description": "tensorflow_asr.models.ctc.base_ctc",
        "isExtraImport": true,
        "detail": "tensorflow_asr.models.ctc.base_ctc",
        "documentation": {}
    },
    {
        "label": "GLU",
        "importPath": "tensorflow_asr.models.activations.glu",
        "description": "tensorflow_asr.models.activations.glu",
        "isExtraImport": true,
        "detail": "tensorflow_asr.models.activations.glu",
        "documentation": {}
    },
    {
        "label": "GLU",
        "importPath": "tensorflow_asr.models.activations.glu",
        "description": "tensorflow_asr.models.activations.glu",
        "isExtraImport": true,
        "detail": "tensorflow_asr.models.activations.glu",
        "documentation": {}
    },
    {
        "label": "MultiHeadAttention",
        "importPath": "tensorflow_asr.models.layers.multihead_attention",
        "description": "tensorflow_asr.models.layers.multihead_attention",
        "isExtraImport": true,
        "detail": "tensorflow_asr.models.layers.multihead_attention",
        "documentation": {}
    },
    {
        "label": "RelPositionMultiHeadAttention",
        "importPath": "tensorflow_asr.models.layers.multihead_attention",
        "description": "tensorflow_asr.models.layers.multihead_attention",
        "isExtraImport": true,
        "detail": "tensorflow_asr.models.layers.multihead_attention",
        "documentation": {}
    },
    {
        "label": "MultiHeadAttention",
        "importPath": "tensorflow_asr.models.layers.multihead_attention",
        "description": "tensorflow_asr.models.layers.multihead_attention",
        "isExtraImport": true,
        "detail": "tensorflow_asr.models.layers.multihead_attention",
        "documentation": {}
    },
    {
        "label": "RelPositionMultiHeadAttention",
        "importPath": "tensorflow_asr.models.layers.multihead_attention",
        "description": "tensorflow_asr.models.layers.multihead_attention",
        "isExtraImport": true,
        "detail": "tensorflow_asr.models.layers.multihead_attention",
        "documentation": {}
    },
    {
        "label": "PositionalEncoding",
        "importPath": "tensorflow_asr.models.layers.positional_encoding",
        "description": "tensorflow_asr.models.layers.positional_encoding",
        "isExtraImport": true,
        "detail": "tensorflow_asr.models.layers.positional_encoding",
        "documentation": {}
    },
    {
        "label": "PositionalEncodingConcat",
        "importPath": "tensorflow_asr.models.layers.positional_encoding",
        "description": "tensorflow_asr.models.layers.positional_encoding",
        "isExtraImport": true,
        "detail": "tensorflow_asr.models.layers.positional_encoding",
        "documentation": {}
    },
    {
        "label": "PositionalEncoding",
        "importPath": "tensorflow_asr.models.layers.positional_encoding",
        "description": "tensorflow_asr.models.layers.positional_encoding",
        "isExtraImport": true,
        "detail": "tensorflow_asr.models.layers.positional_encoding",
        "documentation": {}
    },
    {
        "label": "PositionalEncodingConcat",
        "importPath": "tensorflow_asr.models.layers.positional_encoding",
        "description": "tensorflow_asr.models.layers.positional_encoding",
        "isExtraImport": true,
        "detail": "tensorflow_asr.models.layers.positional_encoding",
        "documentation": {}
    },
    {
        "label": "Conv2dSubsampling",
        "importPath": "tensorflow_asr.models.layers.subsampling",
        "description": "tensorflow_asr.models.layers.subsampling",
        "isExtraImport": true,
        "detail": "tensorflow_asr.models.layers.subsampling",
        "documentation": {}
    },
    {
        "label": "VggSubsampling",
        "importPath": "tensorflow_asr.models.layers.subsampling",
        "description": "tensorflow_asr.models.layers.subsampling",
        "isExtraImport": true,
        "detail": "tensorflow_asr.models.layers.subsampling",
        "documentation": {}
    },
    {
        "label": "TimeReduction",
        "importPath": "tensorflow_asr.models.layers.subsampling",
        "description": "tensorflow_asr.models.layers.subsampling",
        "isExtraImport": true,
        "detail": "tensorflow_asr.models.layers.subsampling",
        "documentation": {}
    },
    {
        "label": "Conv2dSubsampling",
        "importPath": "tensorflow_asr.models.layers.subsampling",
        "description": "tensorflow_asr.models.layers.subsampling",
        "isExtraImport": true,
        "detail": "tensorflow_asr.models.layers.subsampling",
        "documentation": {}
    },
    {
        "label": "VggSubsampling",
        "importPath": "tensorflow_asr.models.layers.subsampling",
        "description": "tensorflow_asr.models.layers.subsampling",
        "isExtraImport": true,
        "detail": "tensorflow_asr.models.layers.subsampling",
        "documentation": {}
    },
    {
        "label": "TimeReduction",
        "importPath": "tensorflow_asr.models.layers.subsampling",
        "description": "tensorflow_asr.models.layers.subsampling",
        "isExtraImport": true,
        "detail": "tensorflow_asr.models.layers.subsampling",
        "documentation": {}
    },
    {
        "label": "tensorflow.keras.backend",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tensorflow.keras.backend",
        "description": "tensorflow.keras.backend",
        "detail": "tensorflow.keras.backend",
        "documentation": {}
    },
    {
        "label": "array_ops",
        "importPath": "tensorflow.python.ops",
        "description": "tensorflow.python.ops",
        "isExtraImport": true,
        "detail": "tensorflow.python.ops",
        "documentation": {}
    },
    {
        "label": "nn_ops",
        "importPath": "tensorflow.python.ops",
        "description": "tensorflow.python.ops",
        "isExtraImport": true,
        "detail": "tensorflow.python.ops",
        "documentation": {}
    },
    {
        "label": "array_ops",
        "importPath": "tensorflow.python.ops",
        "description": "tensorflow.python.ops",
        "isExtraImport": true,
        "detail": "tensorflow.python.ops",
        "documentation": {}
    },
    {
        "label": "nn_ops",
        "importPath": "tensorflow.python.ops",
        "description": "tensorflow.python.ops",
        "isExtraImport": true,
        "detail": "tensorflow.python.ops",
        "documentation": {}
    },
    {
        "label": "shape_list",
        "importPath": "tensorflow_asr.utils.shape_util",
        "description": "tensorflow_asr.utils.shape_util",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils.shape_util",
        "documentation": {}
    },
    {
        "label": "shape_list",
        "importPath": "tensorflow_asr.utils.shape_util",
        "description": "tensorflow_asr.utils.shape_util",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils.shape_util",
        "documentation": {}
    },
    {
        "label": "shape_list",
        "importPath": "tensorflow_asr.utils.shape_util",
        "description": "tensorflow_asr.utils.shape_util",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils.shape_util",
        "documentation": {}
    },
    {
        "label": "conv_utils",
        "importPath": "tensorflow.python.keras.utils",
        "description": "tensorflow.python.keras.utils",
        "isExtraImport": true,
        "detail": "tensorflow.python.keras.utils",
        "documentation": {}
    },
    {
        "label": "conv_utils",
        "importPath": "tensorflow.python.keras.utils",
        "description": "tensorflow.python.keras.utils",
        "isExtraImport": true,
        "detail": "tensorflow.python.keras.utils",
        "documentation": {}
    },
    {
        "label": "RnntLoss",
        "importPath": "tensorflow_asr.losses.rnnt_loss",
        "description": "tensorflow_asr.losses.rnnt_loss",
        "isExtraImport": true,
        "detail": "tensorflow_asr.losses.rnnt_loss",
        "documentation": {}
    },
    {
        "label": "RnntLoss",
        "importPath": "tensorflow_asr.losses.rnnt_loss",
        "description": "tensorflow_asr.losses.rnnt_loss",
        "isExtraImport": true,
        "detail": "tensorflow_asr.losses.rnnt_loss",
        "documentation": {}
    },
    {
        "label": "Embedding",
        "importPath": "tensorflow_asr.models.layers.embedding",
        "description": "tensorflow_asr.models.layers.embedding",
        "isExtraImport": true,
        "detail": "tensorflow_asr.models.layers.embedding",
        "documentation": {}
    },
    {
        "label": "Embedding",
        "importPath": "tensorflow_asr.models.layers.embedding",
        "description": "tensorflow_asr.models.layers.embedding",
        "isExtraImport": true,
        "detail": "tensorflow_asr.models.layers.embedding",
        "documentation": {}
    },
    {
        "label": "L2",
        "importPath": "tensorflow_asr.models.encoders.conformer",
        "description": "tensorflow_asr.models.encoders.conformer",
        "isExtraImport": true,
        "detail": "tensorflow_asr.models.encoders.conformer",
        "documentation": {}
    },
    {
        "label": "ConformerEncoder",
        "importPath": "tensorflow_asr.models.encoders.conformer",
        "description": "tensorflow_asr.models.encoders.conformer",
        "isExtraImport": true,
        "detail": "tensorflow_asr.models.encoders.conformer",
        "documentation": {}
    },
    {
        "label": "L2",
        "importPath": "tensorflow_asr.models.encoders.conformer",
        "description": "tensorflow_asr.models.encoders.conformer",
        "isExtraImport": true,
        "detail": "tensorflow_asr.models.encoders.conformer",
        "documentation": {}
    },
    {
        "label": "ConformerEncoder",
        "importPath": "tensorflow_asr.models.encoders.conformer",
        "description": "tensorflow_asr.models.encoders.conformer",
        "isExtraImport": true,
        "detail": "tensorflow_asr.models.encoders.conformer",
        "documentation": {}
    },
    {
        "label": "Transducer",
        "importPath": "tensorflow_asr.models.transducer.base_transducer",
        "description": "tensorflow_asr.models.transducer.base_transducer",
        "isExtraImport": true,
        "detail": "tensorflow_asr.models.transducer.base_transducer",
        "documentation": {}
    },
    {
        "label": "Transducer",
        "importPath": "tensorflow_asr.models.transducer.base_transducer",
        "description": "tensorflow_asr.models.transducer.base_transducer",
        "isExtraImport": true,
        "detail": "tensorflow_asr.models.transducer.base_transducer",
        "documentation": {}
    },
    {
        "label": "Transducer",
        "importPath": "tensorflow_asr.models.transducer.base_transducer",
        "description": "tensorflow_asr.models.transducer.base_transducer",
        "isExtraImport": true,
        "detail": "tensorflow_asr.models.transducer.base_transducer",
        "documentation": {}
    },
    {
        "label": "Transducer",
        "importPath": "tensorflow_asr.models.transducer.base_transducer",
        "description": "tensorflow_asr.models.transducer.base_transducer",
        "isExtraImport": true,
        "detail": "tensorflow_asr.models.transducer.base_transducer",
        "documentation": {}
    },
    {
        "label": "Transducer",
        "importPath": "tensorflow_asr.models.transducer.base_transducer",
        "description": "tensorflow_asr.models.transducer.base_transducer",
        "isExtraImport": true,
        "detail": "tensorflow_asr.models.transducer.base_transducer",
        "documentation": {}
    },
    {
        "label": "Transducer",
        "importPath": "tensorflow_asr.models.transducer.base_transducer",
        "description": "tensorflow_asr.models.transducer.base_transducer",
        "isExtraImport": true,
        "detail": "tensorflow_asr.models.transducer.base_transducer",
        "documentation": {}
    },
    {
        "label": "L2",
        "importPath": "tensorflow_asr.models.encoders.contextnet",
        "description": "tensorflow_asr.models.encoders.contextnet",
        "isExtraImport": true,
        "detail": "tensorflow_asr.models.encoders.contextnet",
        "documentation": {}
    },
    {
        "label": "ContextNetEncoder",
        "importPath": "tensorflow_asr.models.encoders.contextnet",
        "description": "tensorflow_asr.models.encoders.contextnet",
        "isExtraImport": true,
        "detail": "tensorflow_asr.models.encoders.contextnet",
        "documentation": {}
    },
    {
        "label": "L2",
        "importPath": "tensorflow_asr.models.encoders.contextnet",
        "description": "tensorflow_asr.models.encoders.contextnet",
        "isExtraImport": true,
        "detail": "tensorflow_asr.models.encoders.contextnet",
        "documentation": {}
    },
    {
        "label": "ContextNetEncoder",
        "importPath": "tensorflow_asr.models.encoders.contextnet",
        "description": "tensorflow_asr.models.encoders.contextnet",
        "isExtraImport": true,
        "detail": "tensorflow_asr.models.encoders.contextnet",
        "documentation": {}
    },
    {
        "label": "ExponentialDecay",
        "importPath": "tensorflow.keras.optimizers.schedules",
        "description": "tensorflow.keras.optimizers.schedules",
        "isExtraImport": true,
        "detail": "tensorflow.keras.optimizers.schedules",
        "documentation": {}
    },
    {
        "label": "ExponentialDecay",
        "importPath": "tensorflow.keras.optimizers.schedules",
        "description": "tensorflow.keras.optimizers.schedules",
        "isExtraImport": true,
        "detail": "tensorflow.keras.optimizers.schedules",
        "documentation": {}
    },
    {
        "label": "ErrorRate",
        "importPath": "tensorflow_asr.metrics.error_rates",
        "description": "tensorflow_asr.metrics.error_rates",
        "isExtraImport": true,
        "detail": "tensorflow_asr.metrics.error_rates",
        "documentation": {}
    },
    {
        "label": "ErrorRate",
        "importPath": "tensorflow_asr.metrics.error_rates",
        "description": "tensorflow_asr.metrics.error_rates",
        "isExtraImport": true,
        "detail": "tensorflow_asr.metrics.error_rates",
        "documentation": {}
    },
    {
        "label": "ErrorRate",
        "importPath": "tensorflow_asr.metrics.error_rates",
        "description": "tensorflow_asr.metrics.error_rates",
        "isExtraImport": true,
        "detail": "tensorflow_asr.metrics.error_rates",
        "documentation": {}
    },
    {
        "label": "ErrorRate",
        "importPath": "tensorflow_asr.metrics.error_rates",
        "description": "tensorflow_asr.metrics.error_rates",
        "isExtraImport": true,
        "detail": "tensorflow_asr.metrics.error_rates",
        "documentation": {}
    },
    {
        "label": "ErrorRate",
        "importPath": "tensorflow_asr.metrics.error_rates",
        "description": "tensorflow_asr.metrics.error_rates",
        "isExtraImport": true,
        "detail": "tensorflow_asr.metrics.error_rates",
        "documentation": {}
    },
    {
        "label": "ErrorRate",
        "importPath": "tensorflow_asr.metrics.error_rates",
        "description": "tensorflow_asr.metrics.error_rates",
        "isExtraImport": true,
        "detail": "tensorflow_asr.metrics.error_rates",
        "documentation": {}
    },
    {
        "label": "read_file",
        "importPath": "tensorflow_asr.utils.file_util",
        "description": "tensorflow_asr.utils.file_util",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils.file_util",
        "documentation": {}
    },
    {
        "label": "read_file",
        "importPath": "tensorflow_asr.utils.file_util",
        "description": "tensorflow_asr.utils.file_util",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils.file_util",
        "documentation": {}
    },
    {
        "label": "read_file",
        "importPath": "tensorflow_asr.utils.file_util",
        "description": "tensorflow_asr.utils.file_util",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils.file_util",
        "documentation": {}
    },
    {
        "label": "read_file",
        "importPath": "tensorflow_asr.utils.file_util",
        "description": "tensorflow_asr.utils.file_util",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils.file_util",
        "documentation": {}
    },
    {
        "label": "read_file",
        "importPath": "tensorflow_asr.utils.file_util",
        "description": "tensorflow_asr.utils.file_util",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils.file_util",
        "documentation": {}
    },
    {
        "label": "read_file",
        "importPath": "tensorflow_asr.utils.file_util",
        "description": "tensorflow_asr.utils.file_util",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils.file_util",
        "documentation": {}
    },
    {
        "label": "read_file",
        "importPath": "tensorflow_asr.utils.file_util",
        "description": "tensorflow_asr.utils.file_util",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils.file_util",
        "documentation": {}
    },
    {
        "label": "read_file",
        "importPath": "tensorflow_asr.utils.file_util",
        "description": "tensorflow_asr.utils.file_util",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils.file_util",
        "documentation": {}
    },
    {
        "label": "cer",
        "importPath": "tensorflow_asr.utils.metric_util",
        "description": "tensorflow_asr.utils.metric_util",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils.metric_util",
        "documentation": {}
    },
    {
        "label": "wer",
        "importPath": "tensorflow_asr.utils.metric_util",
        "description": "tensorflow_asr.utils.metric_util",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils.metric_util",
        "documentation": {}
    },
    {
        "label": "cer",
        "importPath": "tensorflow_asr.utils.metric_util",
        "description": "tensorflow_asr.utils.metric_util",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils.metric_util",
        "documentation": {}
    },
    {
        "label": "wer",
        "importPath": "tensorflow_asr.utils.metric_util",
        "description": "tensorflow_asr.utils.metric_util",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils.metric_util",
        "documentation": {}
    },
    {
        "label": "cer",
        "importPath": "tensorflow_asr.utils.metric_util",
        "description": "tensorflow_asr.utils.metric_util",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils.metric_util",
        "documentation": {}
    },
    {
        "label": "wer",
        "importPath": "tensorflow_asr.utils.metric_util",
        "description": "tensorflow_asr.utils.metric_util",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils.metric_util",
        "documentation": {}
    },
    {
        "label": "cer",
        "importPath": "tensorflow_asr.utils.metric_util",
        "description": "tensorflow_asr.utils.metric_util",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils.metric_util",
        "documentation": {}
    },
    {
        "label": "wer",
        "importPath": "tensorflow_asr.utils.metric_util",
        "description": "tensorflow_asr.utils.metric_util",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils.metric_util",
        "documentation": {}
    },
    {
        "label": "cer",
        "importPath": "tensorflow_asr.utils.metric_util",
        "description": "tensorflow_asr.utils.metric_util",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils.metric_util",
        "documentation": {}
    },
    {
        "label": "wer",
        "importPath": "tensorflow_asr.utils.metric_util",
        "description": "tensorflow_asr.utils.metric_util",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils.metric_util",
        "documentation": {}
    },
    {
        "label": "cer",
        "importPath": "tensorflow_asr.utils.metric_util",
        "description": "tensorflow_asr.utils.metric_util",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils.metric_util",
        "documentation": {}
    },
    {
        "label": "wer",
        "importPath": "tensorflow_asr.utils.metric_util",
        "description": "tensorflow_asr.utils.metric_util",
        "isExtraImport": true,
        "detail": "tensorflow_asr.utils.metric_util",
        "documentation": {}
    },
    {
        "label": "contextlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "contextlib",
        "description": "contextlib",
        "detail": "contextlib",
        "documentation": {}
    },
    {
        "label": "distance",
        "importPath": "nltk.metrics",
        "description": "nltk.metrics",
        "isExtraImport": true,
        "detail": "nltk.metrics",
        "documentation": {}
    },
    {
        "label": "distance",
        "importPath": "nltk.metrics",
        "description": "nltk.metrics",
        "isExtraImport": true,
        "detail": "nltk.metrics",
        "documentation": {}
    },
    {
        "label": "fire",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "fire",
        "description": "fire",
        "detail": "fire",
        "documentation": {}
    },
    {
        "label": "featurizer_helpers",
        "importPath": "tensorflow_asr.helpers",
        "description": "tensorflow_asr.helpers",
        "isExtraImport": true,
        "detail": "tensorflow_asr.helpers",
        "documentation": {}
    },
    {
        "label": "exec_helpers",
        "importPath": "tensorflow_asr.helpers",
        "description": "tensorflow_asr.helpers",
        "isExtraImport": true,
        "detail": "tensorflow_asr.helpers",
        "documentation": {}
    },
    {
        "label": "featurizer_helpers",
        "importPath": "tensorflow_asr.helpers",
        "description": "tensorflow_asr.helpers",
        "isExtraImport": true,
        "detail": "tensorflow_asr.helpers",
        "documentation": {}
    },
    {
        "label": "exec_helpers",
        "importPath": "tensorflow_asr.helpers",
        "description": "tensorflow_asr.helpers",
        "isExtraImport": true,
        "detail": "tensorflow_asr.helpers",
        "documentation": {}
    },
    {
        "label": "featurizer_helpers",
        "importPath": "tensorflow_asr.helpers",
        "description": "tensorflow_asr.helpers",
        "isExtraImport": true,
        "detail": "tensorflow_asr.helpers",
        "documentation": {}
    },
    {
        "label": "dataset_helpers",
        "importPath": "tensorflow_asr.helpers",
        "description": "tensorflow_asr.helpers",
        "isExtraImport": true,
        "detail": "tensorflow_asr.helpers",
        "documentation": {}
    },
    {
        "label": "exec_helpers",
        "importPath": "tensorflow_asr.helpers",
        "description": "tensorflow_asr.helpers",
        "isExtraImport": true,
        "detail": "tensorflow_asr.helpers",
        "documentation": {}
    },
    {
        "label": "featurizer_helpers",
        "importPath": "tensorflow_asr.helpers",
        "description": "tensorflow_asr.helpers",
        "isExtraImport": true,
        "detail": "tensorflow_asr.helpers",
        "documentation": {}
    },
    {
        "label": "featurizer_helpers",
        "importPath": "tensorflow_asr.helpers",
        "description": "tensorflow_asr.helpers",
        "isExtraImport": true,
        "detail": "tensorflow_asr.helpers",
        "documentation": {}
    },
    {
        "label": "dataset_helpers",
        "importPath": "tensorflow_asr.helpers",
        "description": "tensorflow_asr.helpers",
        "isExtraImport": true,
        "detail": "tensorflow_asr.helpers",
        "documentation": {}
    },
    {
        "label": "Conformer",
        "importPath": "tensorflow_asr.models.transducer.conformer",
        "description": "tensorflow_asr.models.transducer.conformer",
        "isExtraImport": true,
        "detail": "tensorflow_asr.models.transducer.conformer",
        "documentation": {}
    },
    {
        "label": "Conformer",
        "importPath": "tensorflow_asr.models.transducer.conformer",
        "description": "tensorflow_asr.models.transducer.conformer",
        "isExtraImport": true,
        "detail": "tensorflow_asr.models.transducer.conformer",
        "documentation": {}
    },
    {
        "label": "Conformer",
        "importPath": "tensorflow_asr.models.transducer.conformer",
        "description": "tensorflow_asr.models.transducer.conformer",
        "isExtraImport": true,
        "detail": "tensorflow_asr.models.transducer.conformer",
        "documentation": {}
    },
    {
        "label": "Conformer",
        "importPath": "tensorflow_asr.models.transducer.conformer",
        "description": "tensorflow_asr.models.transducer.conformer",
        "isExtraImport": true,
        "detail": "tensorflow_asr.models.transducer.conformer",
        "documentation": {}
    },
    {
        "label": "Conformer",
        "importPath": "tensorflow_asr.models.transducer.conformer",
        "description": "tensorflow_asr.models.transducer.conformer",
        "isExtraImport": true,
        "detail": "tensorflow_asr.models.transducer.conformer",
        "documentation": {}
    },
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "AudioSegment",
        "importPath": "pydub",
        "description": "pydub",
        "isExtraImport": true,
        "detail": "pydub",
        "documentation": {}
    },
    {
        "label": "AudioSegment",
        "importPath": "pydub",
        "description": "pydub",
        "isExtraImport": true,
        "detail": "pydub",
        "documentation": {}
    },
    {
        "label": "AudioSegment",
        "importPath": "pydub",
        "description": "pydub",
        "isExtraImport": true,
        "detail": "pydub",
        "documentation": {}
    },
    {
        "label": "AudioSegment",
        "importPath": "pydub",
        "description": "pydub",
        "isExtraImport": true,
        "detail": "pydub",
        "documentation": {}
    },
    {
        "label": "operator",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "operator",
        "description": "operator",
        "detail": "operator",
        "documentation": {}
    },
    {
        "label": "itemgetter",
        "importPath": "operator",
        "description": "operator",
        "isExtraImport": true,
        "detail": "operator",
        "documentation": {}
    },
    {
        "label": "queue",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "queue",
        "description": "queue",
        "detail": "queue",
        "documentation": {}
    },
    {
        "label": "ErrorRate",
        "importPath": "EMSConformer.inference.tensorflow_asr.metrics.error_rates",
        "description": "EMSConformer.inference.tensorflow_asr.metrics.error_rates",
        "isExtraImport": true,
        "detail": "EMSConformer.inference.tensorflow_asr.metrics.error_rates",
        "documentation": {}
    },
    {
        "label": "read_file",
        "importPath": "EMSConformer.inference.tensorflow_asr.utils.file_util",
        "description": "EMSConformer.inference.tensorflow_asr.utils.file_util",
        "isExtraImport": true,
        "detail": "EMSConformer.inference.tensorflow_asr.utils.file_util",
        "documentation": {}
    },
    {
        "label": "read_file",
        "importPath": "EMSConformer.inference.tensorflow_asr.utils.file_util",
        "description": "EMSConformer.inference.tensorflow_asr.utils.file_util",
        "isExtraImport": true,
        "detail": "EMSConformer.inference.tensorflow_asr.utils.file_util",
        "documentation": {}
    },
    {
        "label": "cer",
        "importPath": "EMSConformer.inference.tensorflow_asr.utils.metric_util",
        "description": "EMSConformer.inference.tensorflow_asr.utils.metric_util",
        "isExtraImport": true,
        "detail": "EMSConformer.inference.tensorflow_asr.utils.metric_util",
        "documentation": {}
    },
    {
        "label": "wer",
        "importPath": "EMSConformer.inference.tensorflow_asr.utils.metric_util",
        "description": "EMSConformer.inference.tensorflow_asr.utils.metric_util",
        "isExtraImport": true,
        "detail": "EMSConformer.inference.tensorflow_asr.utils.metric_util",
        "documentation": {}
    },
    {
        "label": "read_raw_audio",
        "importPath": "EMSConformer.inference.tensorflow_asr.featurizers.speech_featurizers",
        "description": "EMSConformer.inference.tensorflow_asr.featurizers.speech_featurizers",
        "isExtraImport": true,
        "detail": "EMSConformer.inference.tensorflow_asr.featurizers.speech_featurizers",
        "documentation": {}
    },
    {
        "label": "pyaudio",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pyaudio",
        "description": "pyaudio",
        "detail": "pyaudio",
        "documentation": {}
    },
    {
        "label": "wave",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "wave",
        "description": "wave",
        "detail": "wave",
        "documentation": {}
    },
    {
        "label": "sounddevice",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sounddevice",
        "description": "sounddevice",
        "detail": "sounddevice",
        "documentation": {}
    },
    {
        "label": "scipy.io.wavfile",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "scipy.io.wavfile",
        "description": "scipy.io.wavfile",
        "detail": "scipy.io.wavfile",
        "documentation": {}
    },
    {
        "label": "write",
        "importPath": "scipy.io.wavfile",
        "description": "scipy.io.wavfile",
        "isExtraImport": true,
        "detail": "scipy.io.wavfile",
        "documentation": {}
    },
    {
        "label": "write",
        "importPath": "scipy.io.wavfile",
        "description": "scipy.io.wavfile",
        "isExtraImport": true,
        "detail": "scipy.io.wavfile",
        "documentation": {}
    },
    {
        "label": "gammatone",
        "importPath": "tensorflow_asr.featurizers.methods",
        "description": "tensorflow_asr.featurizers.methods",
        "isExtraImport": true,
        "detail": "tensorflow_asr.featurizers.methods",
        "documentation": {}
    },
    {
        "label": "TransformerSchedule",
        "importPath": "tensorflow_asr.optimizers.schedules",
        "description": "tensorflow_asr.optimizers.schedules",
        "isExtraImport": true,
        "detail": "tensorflow_asr.optimizers.schedules",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "EMSVision.utils",
        "description": "EMSVision.utils",
        "isExtraImport": true,
        "detail": "EMSVision.utils",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "EMSVision.ems_action_keywords",
        "description": "EMSVision.ems_action_keywords",
        "isExtraImport": true,
        "detail": "EMSVision.ems_action_keywords",
        "documentation": {}
    },
    {
        "label": "glob",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "glob",
        "description": "glob",
        "detail": "glob",
        "documentation": {}
    },
    {
        "label": "itertools",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "itertools",
        "description": "itertools",
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "repeat",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "repeat",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "repeat",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "repeat",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "repeat",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "repeat",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "repeat",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "repeat",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "repeat",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "EMSVision.ems_knowledge",
        "description": "EMSVision.ems_knowledge",
        "isExtraImport": true,
        "detail": "EMSVision.ems_knowledge",
        "documentation": {}
    },
    {
        "label": "threading",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "threading",
        "description": "threading",
        "detail": "threading",
        "documentation": {}
    },
    {
        "label": "Thread",
        "importPath": "threading",
        "description": "threading",
        "isExtraImport": true,
        "detail": "threading",
        "documentation": {}
    },
    {
        "label": "Thread",
        "importPath": "threading",
        "description": "threading",
        "isExtraImport": true,
        "detail": "threading",
        "documentation": {}
    },
    {
        "label": "MetaMap",
        "importPath": "pymetamap",
        "description": "pymetamap",
        "isExtraImport": true,
        "detail": "pymetamap",
        "documentation": {}
    },
    {
        "label": "MetaMap",
        "importPath": "pymetamap",
        "description": "pymetamap",
        "isExtraImport": true,
        "detail": "pymetamap",
        "documentation": {}
    },
    {
        "label": "MetaMap",
        "importPath": "pymetamap",
        "description": "pymetamap",
        "isExtraImport": true,
        "detail": "pymetamap",
        "documentation": {}
    },
    {
        "label": "FPDF",
        "importPath": "fpdf",
        "description": "fpdf",
        "isExtraImport": true,
        "detail": "fpdf",
        "documentation": {}
    },
    {
        "label": "metamap2",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "metamap2",
        "description": "metamap2",
        "detail": "metamap2",
        "documentation": {}
    },
    {
        "label": "stanfordNER2",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "stanfordNER2",
        "description": "stanfordNER2",
        "detail": "stanfordNER2",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "sent_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "sent_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "sent_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "sent_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "sent_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "sent_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "sent_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "RegexpTokenizer",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "RegexpTokenizer",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "pos_tag",
        "importPath": "nltk.tag",
        "description": "nltk.tag",
        "isExtraImport": true,
        "detail": "nltk.tag",
        "documentation": {}
    },
    {
        "label": "pos_tag",
        "importPath": "nltk.tag",
        "description": "nltk.tag",
        "isExtraImport": true,
        "detail": "nltk.tag",
        "documentation": {}
    },
    {
        "label": "StanfordNERTagger",
        "importPath": "nltk.tag",
        "description": "nltk.tag",
        "isExtraImport": true,
        "detail": "nltk.tag",
        "documentation": {}
    },
    {
        "label": "arrow",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "arrow",
        "description": "arrow",
        "detail": "arrow",
        "documentation": {}
    },
    {
        "label": "commands",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "commands",
        "description": "commands",
        "detail": "commands",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "listToText",
        "importPath": "textParse2",
        "description": "textParse2",
        "isExtraImport": true,
        "detail": "textParse2",
        "documentation": {}
    },
    {
        "label": "pyttsx",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pyttsx",
        "description": "pyttsx",
        "detail": "pyttsx",
        "documentation": {}
    },
    {
        "label": "subprocess",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "subprocess",
        "description": "subprocess",
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "setup",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "code",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "code",
        "description": "code",
        "detail": "code",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "base64",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "base64",
        "description": "base64",
        "detail": "base64",
        "documentation": {}
    },
    {
        "label": "coremltools",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "coremltools",
        "description": "coremltools",
        "detail": "coremltools",
        "documentation": {}
    },
    {
        "label": "LayerNormANE",
        "importPath": "ane_transformers.reference.layer_norm",
        "description": "ane_transformers.reference.layer_norm",
        "isExtraImport": true,
        "detail": "ane_transformers.reference.layer_norm",
        "documentation": {}
    },
    {
        "label": "quantize_weights",
        "importPath": "coremltools.models.neural_network.quantization_utils",
        "description": "coremltools.models.neural_network.quantization_utils",
        "isExtraImport": true,
        "detail": "coremltools.models.neural_network.quantization_utils",
        "documentation": {}
    },
    {
        "label": "Whisper",
        "importPath": "whisper.model",
        "description": "whisper.model",
        "isExtraImport": true,
        "detail": "whisper.model",
        "documentation": {}
    },
    {
        "label": "AudioEncoder",
        "importPath": "whisper.model",
        "description": "whisper.model",
        "isExtraImport": true,
        "detail": "whisper.model",
        "documentation": {}
    },
    {
        "label": "TextDecoder",
        "importPath": "whisper.model",
        "description": "whisper.model",
        "isExtraImport": true,
        "detail": "whisper.model",
        "documentation": {}
    },
    {
        "label": "ResidualAttentionBlock",
        "importPath": "whisper.model",
        "description": "whisper.model",
        "isExtraImport": true,
        "detail": "whisper.model",
        "documentation": {}
    },
    {
        "label": "MultiHeadAttention",
        "importPath": "whisper.model",
        "description": "whisper.model",
        "isExtraImport": true,
        "detail": "whisper.model",
        "documentation": {}
    },
    {
        "label": "ModelDimensions",
        "importPath": "whisper.model",
        "description": "whisper.model",
        "isExtraImport": true,
        "detail": "whisper.model",
        "documentation": {}
    },
    {
        "label": "load_model",
        "importPath": "whisper",
        "description": "whisper",
        "isExtraImport": true,
        "detail": "whisper",
        "documentation": {}
    },
    {
        "label": "load_model",
        "importPath": "whisper",
        "description": "whisper",
        "isExtraImport": true,
        "detail": "whisper",
        "documentation": {}
    },
    {
        "label": "Whisper",
        "importPath": "whisper",
        "description": "whisper",
        "isExtraImport": true,
        "detail": "whisper",
        "documentation": {}
    },
    {
        "label": "ModelDimensions",
        "importPath": "whisper",
        "description": "whisper",
        "isExtraImport": true,
        "detail": "whisper",
        "documentation": {}
    },
    {
        "label": "mo",
        "importPath": "openvino.tools",
        "description": "openvino.tools",
        "isExtraImport": true,
        "detail": "openvino.tools",
        "documentation": {}
    },
    {
        "label": "serialize",
        "importPath": "openvino.runtime",
        "description": "openvino.runtime",
        "isExtraImport": true,
        "detail": "openvino.runtime",
        "documentation": {}
    },
    {
        "label": "shutil",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "shutil",
        "description": "shutil",
        "detail": "shutil",
        "documentation": {}
    },
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "lxml.html",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "lxml.html",
        "description": "lxml.html",
        "detail": "lxml.html",
        "documentation": {}
    },
    {
        "label": "fromstring",
        "importPath": "lxml.html",
        "description": "lxml.html",
        "isExtraImport": true,
        "detail": "lxml.html",
        "documentation": {}
    },
    {
        "label": "py_trees",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "py_trees",
        "description": "py_trees",
        "detail": "py_trees",
        "documentation": {}
    },
    {
        "label": "blackboard",
        "importPath": "py_trees",
        "description": "py_trees",
        "isExtraImport": true,
        "detail": "py_trees",
        "documentation": {}
    },
    {
        "label": "blackboard",
        "importPath": "py_trees",
        "description": "py_trees",
        "isExtraImport": true,
        "detail": "py_trees",
        "documentation": {}
    },
    {
        "label": "queue",
        "importPath": "six.moves",
        "description": "six.moves",
        "isExtraImport": true,
        "detail": "six.moves",
        "documentation": {}
    },
    {
        "label": "queue",
        "importPath": "six.moves",
        "description": "six.moves",
        "isExtraImport": true,
        "detail": "six.moves",
        "documentation": {}
    },
    {
        "label": "queue",
        "importPath": "six.moves",
        "description": "six.moves",
        "isExtraImport": true,
        "detail": "six.moves",
        "documentation": {}
    },
    {
        "label": "queue",
        "importPath": "six.moves",
        "description": "six.moves",
        "isExtraImport": true,
        "detail": "six.moves",
        "documentation": {}
    },
    {
        "label": "queue",
        "importPath": "six.moves",
        "description": "six.moves",
        "isExtraImport": true,
        "detail": "six.moves",
        "documentation": {}
    },
    {
        "label": "queue",
        "importPath": "six.moves",
        "description": "six.moves",
        "isExtraImport": true,
        "detail": "six.moves",
        "documentation": {}
    },
    {
        "label": "queue",
        "importPath": "six.moves",
        "description": "six.moves",
        "isExtraImport": true,
        "detail": "six.moves",
        "documentation": {}
    },
    {
        "label": "behaviours_m",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "behaviours_m",
        "description": "behaviours_m",
        "detail": "behaviours_m",
        "documentation": {}
    },
    {
        "label": "blackboard",
        "importPath": "behaviours_m",
        "description": "behaviours_m",
        "isExtraImport": true,
        "detail": "behaviours_m",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "behaviours_m",
        "description": "behaviours_m",
        "isExtraImport": true,
        "detail": "behaviours_m",
        "documentation": {}
    },
    {
        "label": "Blackboard",
        "importPath": "py_trees.blackboard",
        "description": "py_trees.blackboard",
        "isExtraImport": true,
        "detail": "py_trees.blackboard",
        "documentation": {}
    },
    {
        "label": "Blackboard",
        "importPath": "py_trees.blackboard",
        "description": "py_trees.blackboard",
        "isExtraImport": true,
        "detail": "py_trees.blackboard",
        "documentation": {}
    },
    {
        "label": "text_clf_utils",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "text_clf_utils",
        "description": "text_clf_utils",
        "detail": "text_clf_utils",
        "documentation": {}
    },
    {
        "label": "rank",
        "importPath": "ranking_func",
        "description": "ranking_func",
        "isExtraImport": true,
        "detail": "ranking_func",
        "documentation": {}
    },
    {
        "label": "rank",
        "importPath": "ranking_func",
        "description": "ranking_func",
        "isExtraImport": true,
        "detail": "ranking_func",
        "documentation": {}
    },
    {
        "label": "textParse2",
        "importPath": "Form_Filling",
        "description": "Form_Filling",
        "isExtraImport": true,
        "detail": "Form_Filling",
        "documentation": {}
    },
    {
        "label": "textParse2",
        "importPath": "Form_Filling",
        "description": "Form_Filling",
        "isExtraImport": true,
        "detail": "Form_Filling",
        "documentation": {}
    },
    {
        "label": "prescription_form2",
        "importPath": "Form_Filling",
        "description": "Form_Filling",
        "isExtraImport": true,
        "detail": "Form_Filling",
        "documentation": {}
    },
    {
        "label": "textParse2",
        "importPath": "Form_Filling",
        "description": "Form_Filling",
        "isExtraImport": true,
        "detail": "Form_Filling",
        "documentation": {}
    },
    {
        "label": "nltk",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "nltk",
        "description": "nltk",
        "detail": "nltk",
        "documentation": {}
    },
    {
        "label": "ngrams",
        "importPath": "nltk",
        "description": "nltk",
        "isExtraImport": true,
        "detail": "nltk",
        "documentation": {}
    },
    {
        "label": "bigrams",
        "importPath": "nltk",
        "description": "nltk",
        "isExtraImport": true,
        "detail": "nltk",
        "documentation": {}
    },
    {
        "label": "trigrams",
        "importPath": "nltk",
        "description": "nltk",
        "isExtraImport": true,
        "detail": "nltk",
        "documentation": {}
    },
    {
        "label": "bigrams",
        "importPath": "nltk",
        "description": "nltk",
        "isExtraImport": true,
        "detail": "nltk",
        "documentation": {}
    },
    {
        "label": "trigrams",
        "importPath": "nltk",
        "description": "nltk",
        "isExtraImport": true,
        "detail": "nltk",
        "documentation": {}
    },
    {
        "label": "bigrams",
        "importPath": "nltk",
        "description": "nltk",
        "isExtraImport": true,
        "detail": "nltk",
        "documentation": {}
    },
    {
        "label": "trigrams",
        "importPath": "nltk",
        "description": "nltk",
        "isExtraImport": true,
        "detail": "nltk",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "default_timer",
        "importPath": "timeit",
        "description": "timeit",
        "isExtraImport": true,
        "detail": "timeit",
        "documentation": {}
    },
    {
        "label": "default_timer",
        "importPath": "timeit",
        "description": "timeit",
        "isExtraImport": true,
        "detail": "timeit",
        "documentation": {}
    },
    {
        "label": "default_timer",
        "importPath": "timeit",
        "description": "timeit",
        "isExtraImport": true,
        "detail": "timeit",
        "documentation": {}
    },
    {
        "label": "default_timer",
        "importPath": "timeit",
        "description": "timeit",
        "isExtraImport": true,
        "detail": "timeit",
        "documentation": {}
    },
    {
        "label": "default_timer",
        "importPath": "timeit",
        "description": "timeit",
        "isExtraImport": true,
        "detail": "timeit",
        "documentation": {}
    },
    {
        "label": "default_timer",
        "importPath": "timeit",
        "description": "timeit",
        "isExtraImport": true,
        "detail": "timeit",
        "documentation": {}
    },
    {
        "label": "default_timer",
        "importPath": "timeit",
        "description": "timeit",
        "isExtraImport": true,
        "detail": "timeit",
        "documentation": {}
    },
    {
        "label": "default_timer",
        "importPath": "timeit",
        "description": "timeit",
        "isExtraImport": true,
        "detail": "timeit",
        "documentation": {}
    },
    {
        "label": "shlex",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "shlex",
        "description": "shlex",
        "detail": "shlex",
        "documentation": {}
    },
    {
        "label": "traceback",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "traceback",
        "description": "traceback",
        "detail": "traceback",
        "documentation": {}
    },
    {
        "label": "Pipeline",
        "importPath": "Pipeline",
        "description": "Pipeline",
        "isExtraImport": true,
        "detail": "Pipeline",
        "documentation": {}
    },
    {
        "label": "load",
        "importPath": "evaluate",
        "description": "evaluate",
        "isExtraImport": true,
        "detail": "evaluate",
        "documentation": {}
    },
    {
        "label": "classification_report",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "confusion_matrix",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "pickle",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pickle",
        "description": "pickle",
        "detail": "pickle",
        "documentation": {}
    },
    {
        "label": "time,",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time.",
        "description": "time.",
        "detail": "time.",
        "documentation": {}
    },
    {
        "label": "test_collection",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "test_collection",
        "description": "test_collection",
        "detail": "test_collection",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "PyQt5.QtCore",
        "description": "PyQt5.QtCore",
        "isExtraImport": true,
        "detail": "PyQt5.QtCore",
        "documentation": {}
    },
    {
        "label": "Qt",
        "importPath": "PyQt5.QtCore",
        "description": "PyQt5.QtCore",
        "isExtraImport": true,
        "detail": "PyQt5.QtCore",
        "documentation": {}
    },
    {
        "label": "pyqtSlot",
        "importPath": "PyQt5.QtCore",
        "description": "PyQt5.QtCore",
        "isExtraImport": true,
        "detail": "PyQt5.QtCore",
        "documentation": {}
    },
    {
        "label": "Qt",
        "importPath": "PyQt5.QtCore",
        "description": "PyQt5.QtCore",
        "isExtraImport": true,
        "detail": "PyQt5.QtCore",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "PyQt5.QtCore",
        "description": "PyQt5.QtCore",
        "isExtraImport": true,
        "detail": "PyQt5.QtCore",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "PyQt5.QtCore",
        "description": "PyQt5.QtCore",
        "isExtraImport": true,
        "detail": "PyQt5.QtCore",
        "documentation": {}
    },
    {
        "label": "QThread",
        "importPath": "PyQt5.QtCore",
        "description": "PyQt5.QtCore",
        "isExtraImport": true,
        "detail": "PyQt5.QtCore",
        "documentation": {}
    },
    {
        "label": "Qt",
        "importPath": "PyQt5.QtCore",
        "description": "PyQt5.QtCore",
        "isExtraImport": true,
        "detail": "PyQt5.QtCore",
        "documentation": {}
    },
    {
        "label": "pyqtSignal",
        "importPath": "PyQt5.QtCore",
        "description": "PyQt5.QtCore",
        "isExtraImport": true,
        "detail": "PyQt5.QtCore",
        "documentation": {}
    },
    {
        "label": "pyqtSlot",
        "importPath": "PyQt5.QtCore",
        "description": "PyQt5.QtCore",
        "isExtraImport": true,
        "detail": "PyQt5.QtCore",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "PyQt5.QtCore",
        "description": "PyQt5.QtCore",
        "isExtraImport": true,
        "detail": "PyQt5.QtCore",
        "documentation": {}
    },
    {
        "label": "Qt",
        "importPath": "PyQt5.QtCore",
        "description": "PyQt5.QtCore",
        "isExtraImport": true,
        "detail": "PyQt5.QtCore",
        "documentation": {}
    },
    {
        "label": "QThread",
        "importPath": "PyQt5.QtCore",
        "description": "PyQt5.QtCore",
        "isExtraImport": true,
        "detail": "PyQt5.QtCore",
        "documentation": {}
    },
    {
        "label": "Qt",
        "importPath": "PyQt5.QtCore",
        "description": "PyQt5.QtCore",
        "isExtraImport": true,
        "detail": "PyQt5.QtCore",
        "documentation": {}
    },
    {
        "label": "pyqtSignal",
        "importPath": "PyQt5.QtCore",
        "description": "PyQt5.QtCore",
        "isExtraImport": true,
        "detail": "PyQt5.QtCore",
        "documentation": {}
    },
    {
        "label": "pyqtSlot",
        "importPath": "PyQt5.QtCore",
        "description": "PyQt5.QtCore",
        "isExtraImport": true,
        "detail": "PyQt5.QtCore",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "PyQt5.QtCore",
        "description": "PyQt5.QtCore",
        "isExtraImport": true,
        "detail": "PyQt5.QtCore",
        "documentation": {}
    },
    {
        "label": "Qt",
        "importPath": "PyQt5.QtCore",
        "description": "PyQt5.QtCore",
        "isExtraImport": true,
        "detail": "PyQt5.QtCore",
        "documentation": {}
    },
    {
        "label": "QThread",
        "importPath": "PyQt5.QtCore",
        "description": "PyQt5.QtCore",
        "isExtraImport": true,
        "detail": "PyQt5.QtCore",
        "documentation": {}
    },
    {
        "label": "Qt",
        "importPath": "PyQt5.QtCore",
        "description": "PyQt5.QtCore",
        "isExtraImport": true,
        "detail": "PyQt5.QtCore",
        "documentation": {}
    },
    {
        "label": "pyqtSignal",
        "importPath": "PyQt5.QtCore",
        "description": "PyQt5.QtCore",
        "isExtraImport": true,
        "detail": "PyQt5.QtCore",
        "documentation": {}
    },
    {
        "label": "pyqtSlot",
        "importPath": "PyQt5.QtCore",
        "description": "PyQt5.QtCore",
        "isExtraImport": true,
        "detail": "PyQt5.QtCore",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QApplication",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QWidget",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QLabel",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QPushButton",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QLineEdit",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QWidget",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QLabel",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QApplication",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "PyQt5.QtGui",
        "description": "PyQt5.QtGui",
        "isExtraImport": true,
        "detail": "PyQt5.QtGui",
        "documentation": {}
    },
    {
        "label": "QIcon",
        "importPath": "PyQt5.QtGui",
        "description": "PyQt5.QtGui",
        "isExtraImport": true,
        "detail": "PyQt5.QtGui",
        "documentation": {}
    },
    {
        "label": "QPixmap",
        "importPath": "PyQt5.QtGui",
        "description": "PyQt5.QtGui",
        "isExtraImport": true,
        "detail": "PyQt5.QtGui",
        "documentation": {}
    },
    {
        "label": "QImage",
        "importPath": "PyQt5.QtGui",
        "description": "PyQt5.QtGui",
        "isExtraImport": true,
        "detail": "PyQt5.QtGui",
        "documentation": {}
    },
    {
        "label": "QPixmap",
        "importPath": "PyQt5.QtGui",
        "description": "PyQt5.QtGui",
        "isExtraImport": true,
        "detail": "PyQt5.QtGui",
        "documentation": {}
    },
    {
        "label": "QGuiApplication",
        "importPath": "PyQt5.QtGui",
        "description": "PyQt5.QtGui",
        "isExtraImport": true,
        "detail": "PyQt5.QtGui",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "PyQt5.QtGui",
        "description": "PyQt5.QtGui",
        "isExtraImport": true,
        "detail": "PyQt5.QtGui",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "PyQt5.QtGui",
        "description": "PyQt5.QtGui",
        "isExtraImport": true,
        "detail": "PyQt5.QtGui",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "PyQt5.QtGui",
        "description": "PyQt5.QtGui",
        "isExtraImport": true,
        "detail": "PyQt5.QtGui",
        "documentation": {}
    },
    {
        "label": "QImage",
        "importPath": "PyQt5.QtGui",
        "description": "PyQt5.QtGui",
        "isExtraImport": true,
        "detail": "PyQt5.QtGui",
        "documentation": {}
    },
    {
        "label": "QPixmap",
        "importPath": "PyQt5.QtGui",
        "description": "PyQt5.QtGui",
        "isExtraImport": true,
        "detail": "PyQt5.QtGui",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "PyQt5.QtGui",
        "description": "PyQt5.QtGui",
        "isExtraImport": true,
        "detail": "PyQt5.QtGui",
        "documentation": {}
    },
    {
        "label": "QImage",
        "importPath": "PyQt5.QtGui",
        "description": "PyQt5.QtGui",
        "isExtraImport": true,
        "detail": "PyQt5.QtGui",
        "documentation": {}
    },
    {
        "label": "QPixmap",
        "importPath": "PyQt5.QtGui",
        "description": "PyQt5.QtGui",
        "isExtraImport": true,
        "detail": "PyQt5.QtGui",
        "documentation": {}
    },
    {
        "label": "QtCore",
        "importPath": "PyQt5",
        "description": "PyQt5",
        "isExtraImport": true,
        "detail": "PyQt5",
        "documentation": {}
    },
    {
        "label": "QVideoWidget",
        "importPath": "PyQt5.QtMultimediaWidgets",
        "description": "PyQt5.QtMultimediaWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtMultimediaWidgets",
        "documentation": {}
    },
    {
        "label": "QMediaPlayer",
        "importPath": "PyQt5.QtMultimedia",
        "description": "PyQt5.QtMultimedia",
        "isExtraImport": true,
        "detail": "PyQt5.QtMultimedia",
        "documentation": {}
    },
    {
        "label": "PyQt5.QtWidgets,PyQt5.QtCore",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "PyQt5.QtWidgets.PyQt5.QtCore",
        "description": "PyQt5.QtWidgets.PyQt5.QtCore",
        "detail": "PyQt5.QtWidgets.PyQt5.QtCore",
        "documentation": {}
    },
    {
        "label": "Amplitude",
        "importPath": "DSP.amplitude",
        "description": "DSP.amplitude",
        "isExtraImport": true,
        "detail": "DSP.amplitude",
        "documentation": {}
    },
    {
        "label": "EMSAgentSystem",
        "importPath": "EMSAgent",
        "description": "EMSAgent",
        "isExtraImport": true,
        "detail": "EMSAgent",
        "documentation": {}
    },
    {
        "label": "EMSAgentSystem",
        "importPath": "EMSAgent",
        "description": "EMSAgent",
        "isExtraImport": true,
        "detail": "EMSAgent",
        "documentation": {}
    },
    {
        "label": "GoogleSpeechMicStream",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "GoogleSpeechMicStream",
        "description": "GoogleSpeechMicStream",
        "detail": "GoogleSpeechMicStream",
        "documentation": {}
    },
    {
        "label": "GoogleSpeechFileStream",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "GoogleSpeechFileStream",
        "description": "GoogleSpeechFileStream",
        "detail": "GoogleSpeechFileStream",
        "documentation": {}
    },
    {
        "label": "DeepSpeechMicStream",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "DeepSpeechMicStream",
        "description": "DeepSpeechMicStream",
        "detail": "DeepSpeechMicStream",
        "documentation": {}
    },
    {
        "label": "DeepSpeechFileStream",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "DeepSpeechFileStream",
        "description": "DeepSpeechFileStream",
        "detail": "DeepSpeechFileStream",
        "documentation": {}
    },
    {
        "label": "WhisperFileStream",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "WhisperFileStream",
        "description": "WhisperFileStream",
        "detail": "WhisperFileStream",
        "documentation": {}
    },
    {
        "label": "whisper_config",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "whisper_config",
        "description": "whisper_config",
        "detail": "whisper_config",
        "documentation": {}
    },
    {
        "label": "TextSpeechStream",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "TextSpeechStream",
        "description": "TextSpeechStream",
        "detail": "TextSpeechStream",
        "documentation": {}
    },
    {
        "label": "CognitiveSystem",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "CognitiveSystem",
        "description": "CognitiveSystem",
        "detail": "CognitiveSystem",
        "documentation": {}
    },
    {
        "label": "Feedback",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "Feedback",
        "description": "Feedback",
        "detail": "Feedback",
        "documentation": {}
    },
    {
        "label": "StoppableThread",
        "importPath": "StoppableThread.StoppableThread",
        "description": "StoppableThread.StoppableThread",
        "isExtraImport": true,
        "detail": "StoppableThread.StoppableThread",
        "documentation": {}
    },
    {
        "label": "Thread",
        "importPath": "video_streaming",
        "description": "video_streaming",
        "isExtraImport": true,
        "detail": "video_streaming",
        "documentation": {}
    },
    {
        "label": "Thread_Watch",
        "importPath": "smartwatch_streaming",
        "description": "smartwatch_streaming",
        "isExtraImport": true,
        "detail": "smartwatch_streaming",
        "documentation": {}
    },
    {
        "label": "speech_v1",
        "importPath": "google.cloud",
        "description": "google.cloud",
        "isExtraImport": true,
        "detail": "google.cloud",
        "documentation": {}
    },
    {
        "label": "speech_v1",
        "importPath": "google.cloud",
        "description": "google.cloud",
        "isExtraImport": true,
        "detail": "google.cloud",
        "documentation": {}
    },
    {
        "label": "exceptions",
        "importPath": "google.api_core",
        "description": "google.api_core",
        "isExtraImport": true,
        "detail": "google.api_core",
        "documentation": {}
    },
    {
        "label": "Retry",
        "importPath": "google.api_core.retry",
        "description": "google.api_core.retry",
        "isExtraImport": true,
        "detail": "google.api_core.retry",
        "documentation": {}
    },
    {
        "label": "copy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "nltk.collocations",
        "description": "nltk.collocations",
        "isExtraImport": true,
        "detail": "nltk.collocations",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "nltk.collocations",
        "description": "nltk.collocations",
        "isExtraImport": true,
        "detail": "nltk.collocations",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "nltk.collocations",
        "description": "nltk.collocations",
        "isExtraImport": true,
        "detail": "nltk.collocations",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "nltk.collocations",
        "description": "nltk.collocations",
        "isExtraImport": true,
        "detail": "nltk.collocations",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "nltk.collocations",
        "description": "nltk.collocations",
        "isExtraImport": true,
        "detail": "nltk.collocations",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "nltk.collocations",
        "description": "nltk.collocations",
        "isExtraImport": true,
        "detail": "nltk.collocations",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "nltk.collocations",
        "description": "nltk.collocations",
        "isExtraImport": true,
        "detail": "nltk.collocations",
        "documentation": {}
    },
    {
        "label": "WordNetLemmatizer",
        "importPath": "nltk.stem.wordnet",
        "description": "nltk.stem.wordnet",
        "isExtraImport": true,
        "detail": "nltk.stem.wordnet",
        "documentation": {}
    },
    {
        "label": "WordNetLemmatizer",
        "importPath": "nltk.stem.wordnet",
        "description": "nltk.stem.wordnet",
        "isExtraImport": true,
        "detail": "nltk.stem.wordnet",
        "documentation": {}
    },
    {
        "label": "WordNetLemmatizer",
        "importPath": "nltk.stem.wordnet",
        "description": "nltk.stem.wordnet",
        "isExtraImport": true,
        "detail": "nltk.stem.wordnet",
        "documentation": {}
    },
    {
        "label": "ngrams",
        "importPath": "nltk.util",
        "description": "nltk.util",
        "isExtraImport": true,
        "detail": "nltk.util",
        "documentation": {}
    },
    {
        "label": "ngrams",
        "importPath": "nltk.util",
        "description": "nltk.util",
        "isExtraImport": true,
        "detail": "nltk.util",
        "documentation": {}
    },
    {
        "label": "ngrams",
        "importPath": "nltk.util",
        "description": "nltk.util",
        "isExtraImport": true,
        "detail": "nltk.util",
        "documentation": {}
    },
    {
        "label": "csv,",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "csv.",
        "description": "csv.",
        "detail": "csv.",
        "documentation": {}
    },
    {
        "label": "gensim",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "gensim",
        "description": "gensim",
        "detail": "gensim",
        "documentation": {}
    },
    {
        "label": "utils",
        "importPath": "gensim",
        "description": "gensim",
        "isExtraImport": true,
        "detail": "gensim",
        "documentation": {}
    },
    {
        "label": "utils",
        "importPath": "gensim",
        "description": "gensim",
        "isExtraImport": true,
        "detail": "gensim",
        "documentation": {}
    },
    {
        "label": "Word2Vec",
        "importPath": "gensim.models",
        "description": "gensim.models",
        "isExtraImport": true,
        "detail": "gensim.models",
        "documentation": {}
    },
    {
        "label": "Doc2Vec",
        "importPath": "gensim.models",
        "description": "gensim.models",
        "isExtraImport": true,
        "detail": "gensim.models",
        "documentation": {}
    },
    {
        "label": "Word2Vec",
        "importPath": "gensim.models",
        "description": "gensim.models",
        "isExtraImport": true,
        "detail": "gensim.models",
        "documentation": {}
    },
    {
        "label": "Doc2Vec",
        "importPath": "gensim.models",
        "description": "gensim.models",
        "isExtraImport": true,
        "detail": "gensim.models",
        "documentation": {}
    },
    {
        "label": "LineSentence",
        "importPath": "gensim.models.word2vec",
        "description": "gensim.models.word2vec",
        "isExtraImport": true,
        "detail": "gensim.models.word2vec",
        "documentation": {}
    },
    {
        "label": "LineSentence",
        "importPath": "gensim.models.word2vec",
        "description": "gensim.models.word2vec",
        "isExtraImport": true,
        "detail": "gensim.models.word2vec",
        "documentation": {}
    },
    {
        "label": "LabeledSentence",
        "importPath": "gensim.models.doc2vec",
        "description": "gensim.models.doc2vec",
        "isExtraImport": true,
        "detail": "gensim.models.doc2vec",
        "documentation": {}
    },
    {
        "label": "Doc2Vec",
        "importPath": "gensim.models.doc2vec",
        "description": "gensim.models.doc2vec",
        "isExtraImport": true,
        "detail": "gensim.models.doc2vec",
        "documentation": {}
    },
    {
        "label": "LabeledSentence",
        "importPath": "gensim.models.doc2vec",
        "description": "gensim.models.doc2vec",
        "isExtraImport": true,
        "detail": "gensim.models.doc2vec",
        "documentation": {}
    },
    {
        "label": "EMSVisionSystem",
        "importPath": "EMSVision",
        "description": "EMSVision",
        "isExtraImport": true,
        "detail": "EMSVision",
        "documentation": {}
    },
    {
        "label": "VideoFileStream",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "VideoFileStream",
        "description": "VideoFileStream",
        "detail": "VideoFileStream",
        "documentation": {}
    },
    {
        "label": "WhisperAgent",
        "importPath": "EMSWhisper",
        "description": "EMSWhisper",
        "isExtraImport": true,
        "detail": "EMSWhisper",
        "documentation": {}
    },
    {
        "label": "structural_similarity",
        "importPath": "skimage.metrics",
        "description": "skimage.metrics",
        "isExtraImport": true,
        "detail": "skimage.metrics",
        "documentation": {}
    },
    {
        "label": "WhisperAPI",
        "importPath": "WhisperAPI",
        "description": "WhisperAPI",
        "isExtraImport": true,
        "detail": "WhisperAPI",
        "documentation": {}
    },
    {
        "label": "spatial",
        "importPath": "scipy",
        "description": "scipy",
        "isExtraImport": true,
        "detail": "scipy",
        "documentation": {}
    },
    {
        "label": "signal",
        "importPath": "scipy",
        "description": "scipy",
        "isExtraImport": true,
        "detail": "scipy",
        "documentation": {}
    },
    {
        "label": "signal",
        "importPath": "scipy",
        "description": "scipy",
        "isExtraImport": true,
        "detail": "scipy",
        "documentation": {}
    },
    {
        "label": "ConceptExtract",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "ConceptExtract",
        "description": "ConceptExtract",
        "detail": "ConceptExtract",
        "documentation": {}
    },
    {
        "label": "openpyxl",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "openpyxl",
        "description": "openpyxl",
        "detail": "openpyxl",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "find_peaks",
        "importPath": "scipy.signal",
        "description": "scipy.signal",
        "isExtraImport": true,
        "detail": "scipy.signal",
        "documentation": {}
    },
    {
        "label": "find_peaks_cwt",
        "importPath": "scipy.signal",
        "description": "scipy.signal",
        "isExtraImport": true,
        "detail": "scipy.signal",
        "documentation": {}
    },
    {
        "label": "peak_widths",
        "importPath": "scipy.signal",
        "description": "scipy.signal",
        "isExtraImport": true,
        "detail": "scipy.signal",
        "documentation": {}
    },
    {
        "label": "lombscargle",
        "importPath": "scipy.signal",
        "description": "scipy.signal",
        "isExtraImport": true,
        "detail": "scipy.signal",
        "documentation": {}
    },
    {
        "label": "spectrogram",
        "importPath": "scipy.signal",
        "description": "scipy.signal",
        "isExtraImport": true,
        "detail": "scipy.signal",
        "documentation": {}
    },
    {
        "label": "find_peaks",
        "importPath": "scipy.signal",
        "description": "scipy.signal",
        "isExtraImport": true,
        "detail": "scipy.signal",
        "documentation": {}
    },
    {
        "label": "find_peaks_cwt",
        "importPath": "scipy.signal",
        "description": "scipy.signal",
        "isExtraImport": true,
        "detail": "scipy.signal",
        "documentation": {}
    },
    {
        "label": "peak_widths",
        "importPath": "scipy.signal",
        "description": "scipy.signal",
        "isExtraImport": true,
        "detail": "scipy.signal",
        "documentation": {}
    },
    {
        "label": "pylab",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pylab",
        "description": "pylab",
        "detail": "pylab",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "Authentication",
        "description": "Authentication",
        "isExtraImport": true,
        "detail": "Authentication",
        "documentation": {}
    },
    {
        "label": "preprocess_data",
        "importPath": "cpr_calculation",
        "description": "cpr_calculation",
        "isExtraImport": true,
        "detail": "cpr_calculation",
        "documentation": {}
    },
    {
        "label": "find_cpr_rate",
        "importPath": "cpr_calculation",
        "description": "cpr_calculation",
        "isExtraImport": true,
        "detail": "cpr_calculation",
        "documentation": {}
    },
    {
        "label": "find_peaks_valleys",
        "importPath": "cpr_calculation",
        "description": "cpr_calculation",
        "isExtraImport": true,
        "detail": "cpr_calculation",
        "documentation": {}
    },
    {
        "label": "vid_streaming_Cpr",
        "importPath": "cpr_calculation",
        "description": "cpr_calculation",
        "isExtraImport": true,
        "detail": "cpr_calculation",
        "documentation": {}
    },
    {
        "label": "fft",
        "importPath": "scipy.fft",
        "description": "scipy.fft",
        "isExtraImport": true,
        "detail": "scipy.fft",
        "documentation": {}
    },
    {
        "label": "rfft",
        "importPath": "scipy.fft",
        "description": "scipy.fft",
        "isExtraImport": true,
        "detail": "scipy.fft",
        "documentation": {}
    },
    {
        "label": "fftfreq",
        "importPath": "scipy.fft",
        "description": "scipy.fft",
        "isExtraImport": true,
        "detail": "scipy.fft",
        "documentation": {}
    },
    {
        "label": "rfftfreq",
        "importPath": "scipy.fft",
        "description": "scipy.fft",
        "isExtraImport": true,
        "detail": "scipy.fft",
        "documentation": {}
    },
    {
        "label": "ifft",
        "importPath": "scipy.fft",
        "description": "scipy.fft",
        "isExtraImport": true,
        "detail": "scipy.fft",
        "documentation": {}
    },
    {
        "label": "fftshift",
        "importPath": "scipy.fft",
        "description": "scipy.fft",
        "isExtraImport": true,
        "detail": "scipy.fft",
        "documentation": {}
    },
    {
        "label": "SnowballStemmer",
        "importPath": "nltk.stem.snowball",
        "description": "nltk.stem.snowball",
        "isExtraImport": true,
        "detail": "nltk.stem.snowball",
        "documentation": {}
    },
    {
        "label": "EnglishStemmer",
        "importPath": "nltk.stem.snowball",
        "description": "nltk.stem.snowball",
        "isExtraImport": true,
        "detail": "nltk.stem.snowball",
        "documentation": {}
    },
    {
        "label": "sklearn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sklearn",
        "description": "sklearn",
        "detail": "sklearn",
        "documentation": {}
    },
    {
        "label": "gensim,",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "gensim.",
        "description": "gensim.",
        "detail": "gensim.",
        "documentation": {}
    },
    {
        "label": "isfile",
        "importPath": "os.path",
        "description": "os.path",
        "isExtraImport": true,
        "detail": "os.path",
        "documentation": {}
    },
    {
        "label": "join",
        "importPath": "os.path",
        "description": "os.path",
        "isExtraImport": true,
        "detail": "os.path",
        "documentation": {}
    },
    {
        "label": "isfile",
        "importPath": "os.path",
        "description": "os.path",
        "isExtraImport": true,
        "detail": "os.path",
        "documentation": {}
    },
    {
        "label": "join",
        "importPath": "os.path",
        "description": "os.path",
        "isExtraImport": true,
        "detail": "os.path",
        "documentation": {}
    },
    {
        "label": "isfile",
        "importPath": "os.path",
        "description": "os.path",
        "isExtraImport": true,
        "detail": "os.path",
        "documentation": {}
    },
    {
        "label": "join",
        "importPath": "os.path",
        "description": "os.path",
        "isExtraImport": true,
        "detail": "os.path",
        "documentation": {}
    },
    {
        "label": "isfile",
        "importPath": "os.path",
        "description": "os.path",
        "isExtraImport": true,
        "detail": "os.path",
        "documentation": {}
    },
    {
        "label": "join",
        "importPath": "os.path",
        "description": "os.path",
        "isExtraImport": true,
        "detail": "os.path",
        "documentation": {}
    },
    {
        "label": "isfile",
        "importPath": "os.path",
        "description": "os.path",
        "isExtraImport": true,
        "detail": "os.path",
        "documentation": {}
    },
    {
        "label": "join",
        "importPath": "os.path",
        "description": "os.path",
        "isExtraImport": true,
        "detail": "os.path",
        "documentation": {}
    },
    {
        "label": "BeautifulSoup",
        "importPath": "bs4",
        "description": "bs4",
        "isExtraImport": true,
        "detail": "bs4",
        "documentation": {}
    },
    {
        "label": "BeautifulSoup",
        "importPath": "bs4",
        "description": "bs4",
        "isExtraImport": true,
        "detail": "bs4",
        "documentation": {}
    },
    {
        "label": "BeautifulSoup",
        "importPath": "bs4",
        "description": "bs4",
        "isExtraImport": true,
        "detail": "bs4",
        "documentation": {}
    },
    {
        "label": "BeautifulSoup",
        "importPath": "bs4",
        "description": "bs4",
        "isExtraImport": true,
        "detail": "bs4",
        "documentation": {}
    },
    {
        "label": "BeautifulSoup",
        "importPath": "bs4",
        "description": "bs4",
        "isExtraImport": true,
        "detail": "bs4",
        "documentation": {}
    },
    {
        "label": "string",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "string",
        "description": "string",
        "detail": "string",
        "documentation": {}
    },
    {
        "label": "norm",
        "importPath": "numpy.linalg",
        "description": "numpy.linalg",
        "isExtraImport": true,
        "detail": "numpy.linalg",
        "documentation": {}
    },
    {
        "label": "norm",
        "importPath": "numpy.linalg",
        "description": "numpy.linalg",
        "isExtraImport": true,
        "detail": "numpy.linalg",
        "documentation": {}
    },
    {
        "label": "norm",
        "importPath": "numpy.linalg",
        "description": "numpy.linalg",
        "isExtraImport": true,
        "detail": "numpy.linalg",
        "documentation": {}
    },
    {
        "label": "norm",
        "importPath": "numpy.linalg",
        "description": "numpy.linalg",
        "isExtraImport": true,
        "detail": "numpy.linalg",
        "documentation": {}
    },
    {
        "label": "reduce",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "get_tp",
        "importPath": "interventions",
        "description": "interventions",
        "isExtraImport": true,
        "detail": "interventions",
        "documentation": {}
    },
    {
        "label": "get_fp",
        "importPath": "interventions",
        "description": "interventions",
        "isExtraImport": true,
        "detail": "interventions",
        "documentation": {}
    },
    {
        "label": "get_fn",
        "importPath": "interventions",
        "description": "interventions",
        "isExtraImport": true,
        "detail": "interventions",
        "documentation": {}
    },
    {
        "label": "calc_recall",
        "importPath": "interventions",
        "description": "interventions",
        "isExtraImport": true,
        "detail": "interventions",
        "documentation": {}
    },
    {
        "label": "calc_precision",
        "importPath": "interventions",
        "description": "interventions",
        "isExtraImport": true,
        "detail": "interventions",
        "documentation": {}
    },
    {
        "label": "calc_f1",
        "importPath": "interventions",
        "description": "interventions",
        "isExtraImport": true,
        "detail": "interventions",
        "documentation": {}
    },
    {
        "label": "pyDatalog",
        "importPath": "pyDatalog",
        "description": "pyDatalog",
        "isExtraImport": true,
        "detail": "pyDatalog",
        "documentation": {}
    },
    {
        "label": "connect",
        "kind": 2,
        "importPath": "DataCollection.AndroidDevelopment.AV_Streaming_App.camstrm.client",
        "description": "DataCollection.AndroidDevelopment.AV_Streaming_App.camstrm.client",
        "peekOfCode": "def connect():\n    read_int=-1\n    print(\"waiting for connection....\")\n    while(read_int!=100):\n        try:\n            s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n            s.connect((TCP_IP, TCP_PORT))\n            read_int=int.from_bytes(s.recv(1),\"big\")\n        except Exception as e:\n            print(e)",
        "detail": "DataCollection.AndroidDevelopment.AV_Streaming_App.camstrm.client",
        "documentation": {}
    },
    {
        "label": "get_next_image",
        "kind": 2,
        "importPath": "DataCollection.AndroidDevelopment.AV_Streaming_App.camstrm.client",
        "description": "DataCollection.AndroidDevelopment.AV_Streaming_App.camstrm.client",
        "peekOfCode": "def get_next_image(s):\n    try:\n        d_type=int.from_bytes(s.recv(1),\"big\")      \n        if(d_type!=22):\n            return -1\n        seq=int.from_bytes(s.recv(4),\"big\")\n        height=int.from_bytes(s.recv(4),\"big\")\n        width=int.from_bytes(s.recv(4),\"big\")\n        size=int.from_bytes(s.recv(4),\"big\")\n        if(size>1000000):",
        "detail": "DataCollection.AndroidDevelopment.AV_Streaming_App.camstrm.client",
        "documentation": {}
    },
    {
        "label": "TCP_IP",
        "kind": 5,
        "importPath": "DataCollection.AndroidDevelopment.AV_Streaming_App.camstrm.client",
        "description": "DataCollection.AndroidDevelopment.AV_Streaming_App.camstrm.client",
        "peekOfCode": "TCP_IP = '127.0.0.1'\nTCP_PORT = 9600\ndef connect():\n    read_int=-1\n    print(\"waiting for connection....\")\n    while(read_int!=100):\n        try:\n            s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n            s.connect((TCP_IP, TCP_PORT))\n            read_int=int.from_bytes(s.recv(1),\"big\")",
        "detail": "DataCollection.AndroidDevelopment.AV_Streaming_App.camstrm.client",
        "documentation": {}
    },
    {
        "label": "TCP_PORT",
        "kind": 5,
        "importPath": "DataCollection.AndroidDevelopment.AV_Streaming_App.camstrm.client",
        "description": "DataCollection.AndroidDevelopment.AV_Streaming_App.camstrm.client",
        "peekOfCode": "TCP_PORT = 9600\ndef connect():\n    read_int=-1\n    print(\"waiting for connection....\")\n    while(read_int!=100):\n        try:\n            s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n            s.connect((TCP_IP, TCP_PORT))\n            read_int=int.from_bytes(s.recv(1),\"big\")\n        except Exception as e:",
        "detail": "DataCollection.AndroidDevelopment.AV_Streaming_App.camstrm.client",
        "documentation": {}
    },
    {
        "label": "mp_drawing",
        "kind": 5,
        "importPath": "DataCollection.AndroidDevelopment.AV_Streaming_App.camstrm.stream_annotated_pose",
        "description": "DataCollection.AndroidDevelopment.AV_Streaming_App.camstrm.stream_annotated_pose",
        "peekOfCode": "mp_drawing = mp.solutions.drawing_utils\nmp_drawing_styles = mp.solutions.drawing_styles\nmp_pose = mp.solutions.pose\n#connect to server\ns=client.connect()\n#ask the server to send images\np = struct.pack('!i', 23)\ns.send(p)\n#processing all the frames in real time makes the code laggish\n#so we have to skip (and ignore) some frames",
        "detail": "DataCollection.AndroidDevelopment.AV_Streaming_App.camstrm.stream_annotated_pose",
        "documentation": {}
    },
    {
        "label": "mp_drawing_styles",
        "kind": 5,
        "importPath": "DataCollection.AndroidDevelopment.AV_Streaming_App.camstrm.stream_annotated_pose",
        "description": "DataCollection.AndroidDevelopment.AV_Streaming_App.camstrm.stream_annotated_pose",
        "peekOfCode": "mp_drawing_styles = mp.solutions.drawing_styles\nmp_pose = mp.solutions.pose\n#connect to server\ns=client.connect()\n#ask the server to send images\np = struct.pack('!i', 23)\ns.send(p)\n#processing all the frames in real time makes the code laggish\n#so we have to skip (and ignore) some frames\nSKIP_FRAMES=5",
        "detail": "DataCollection.AndroidDevelopment.AV_Streaming_App.camstrm.stream_annotated_pose",
        "documentation": {}
    },
    {
        "label": "mp_pose",
        "kind": 5,
        "importPath": "DataCollection.AndroidDevelopment.AV_Streaming_App.camstrm.stream_annotated_pose",
        "description": "DataCollection.AndroidDevelopment.AV_Streaming_App.camstrm.stream_annotated_pose",
        "peekOfCode": "mp_pose = mp.solutions.pose\n#connect to server\ns=client.connect()\n#ask the server to send images\np = struct.pack('!i', 23)\ns.send(p)\n#processing all the frames in real time makes the code laggish\n#so we have to skip (and ignore) some frames\nSKIP_FRAMES=5\ni=0",
        "detail": "DataCollection.AndroidDevelopment.AV_Streaming_App.camstrm.stream_annotated_pose",
        "documentation": {}
    },
    {
        "label": "p",
        "kind": 5,
        "importPath": "DataCollection.AndroidDevelopment.AV_Streaming_App.camstrm.stream_annotated_pose",
        "description": "DataCollection.AndroidDevelopment.AV_Streaming_App.camstrm.stream_annotated_pose",
        "peekOfCode": "p = struct.pack('!i', 23)\ns.send(p)\n#processing all the frames in real time makes the code laggish\n#so we have to skip (and ignore) some frames\nSKIP_FRAMES=5\ni=0\nwith mp_pose.Pose(\n    min_detection_confidence=0.5,\n    min_tracking_confidence=0.5) as pose:\n    while(True):",
        "detail": "DataCollection.AndroidDevelopment.AV_Streaming_App.camstrm.stream_annotated_pose",
        "documentation": {}
    },
    {
        "label": "connect",
        "kind": 2,
        "importPath": "DataCollection.AndroidDevelopment.AV_Streaming_App.camstrm.write_vid",
        "description": "DataCollection.AndroidDevelopment.AV_Streaming_App.camstrm.write_vid",
        "peekOfCode": "def connect():\n    read_int=-1\n    print(\"waiting for connection....\")\n    while(read_int!=100):\n        try:\n            s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n            s.connect((TCP_IP, TCP_PORT))\n            read_int=int.from_bytes(s.recv(1),\"big\")\n        except Exception as e:\n            print(e)",
        "detail": "DataCollection.AndroidDevelopment.AV_Streaming_App.camstrm.write_vid",
        "documentation": {}
    },
    {
        "label": "get_next_image",
        "kind": 2,
        "importPath": "DataCollection.AndroidDevelopment.AV_Streaming_App.camstrm.write_vid",
        "description": "DataCollection.AndroidDevelopment.AV_Streaming_App.camstrm.write_vid",
        "peekOfCode": "def get_next_image(s):\n    try:\n        d_type=int.from_bytes(s.recv(1),\"big\")      \n        if(d_type!=22):\n            return -1\n        seq=int.from_bytes(s.recv(4),\"big\")\n        height=int.from_bytes(s.recv(4),\"big\")\n        width=int.from_bytes(s.recv(4),\"big\")\n        size=int.from_bytes(s.recv(4),\"big\")\n        if(size>1000000):",
        "detail": "DataCollection.AndroidDevelopment.AV_Streaming_App.camstrm.write_vid",
        "documentation": {}
    },
    {
        "label": "TCP_IP",
        "kind": 5,
        "importPath": "DataCollection.AndroidDevelopment.AV_Streaming_App.camstrm.write_vid",
        "description": "DataCollection.AndroidDevelopment.AV_Streaming_App.camstrm.write_vid",
        "peekOfCode": "TCP_IP = '127.0.0.1'\nTCP_PORT = 8899\ndef connect():\n    read_int=-1\n    print(\"waiting for connection....\")\n    while(read_int!=100):\n        try:\n            s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n            s.connect((TCP_IP, TCP_PORT))\n            read_int=int.from_bytes(s.recv(1),\"big\")",
        "detail": "DataCollection.AndroidDevelopment.AV_Streaming_App.camstrm.write_vid",
        "documentation": {}
    },
    {
        "label": "TCP_PORT",
        "kind": 5,
        "importPath": "DataCollection.AndroidDevelopment.AV_Streaming_App.camstrm.write_vid",
        "description": "DataCollection.AndroidDevelopment.AV_Streaming_App.camstrm.write_vid",
        "peekOfCode": "TCP_PORT = 8899\ndef connect():\n    read_int=-1\n    print(\"waiting for connection....\")\n    while(read_int!=100):\n        try:\n            s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n            s.connect((TCP_IP, TCP_PORT))\n            read_int=int.from_bytes(s.recv(1),\"big\")\n        except Exception as e:",
        "detail": "DataCollection.AndroidDevelopment.AV_Streaming_App.camstrm.write_vid",
        "documentation": {}
    },
    {
        "label": "now",
        "kind": 5,
        "importPath": "DataCollection.AndroidDevelopment.AV_Streaming_App.camstrm.write_vid",
        "description": "DataCollection.AndroidDevelopment.AV_Streaming_App.camstrm.write_vid",
        "peekOfCode": "now = datetime.now()\ndt_string = now.strftime(\"%d-%m-%Y-%H-%M-%S\")\ns=connect()\np = struct.pack('!i', 23)\ns.send(p)\nimage=-1\nwhile(type(image)!=tuple):\n    try:\n        image=get_next_image(s)\n        height,width,layers=image[0].shape",
        "detail": "DataCollection.AndroidDevelopment.AV_Streaming_App.camstrm.write_vid",
        "documentation": {}
    },
    {
        "label": "dt_string",
        "kind": 5,
        "importPath": "DataCollection.AndroidDevelopment.AV_Streaming_App.camstrm.write_vid",
        "description": "DataCollection.AndroidDevelopment.AV_Streaming_App.camstrm.write_vid",
        "peekOfCode": "dt_string = now.strftime(\"%d-%m-%Y-%H-%M-%S\")\ns=connect()\np = struct.pack('!i', 23)\ns.send(p)\nimage=-1\nwhile(type(image)!=tuple):\n    try:\n        image=get_next_image(s)\n        height,width,layers=image[0].shape\n        #fourcc = cv2.VideoWriter_fourcc(*'mp4v') ",
        "detail": "DataCollection.AndroidDevelopment.AV_Streaming_App.camstrm.write_vid",
        "documentation": {}
    },
    {
        "label": "p",
        "kind": 5,
        "importPath": "DataCollection.AndroidDevelopment.AV_Streaming_App.camstrm.write_vid",
        "description": "DataCollection.AndroidDevelopment.AV_Streaming_App.camstrm.write_vid",
        "peekOfCode": "p = struct.pack('!i', 23)\ns.send(p)\nimage=-1\nwhile(type(image)!=tuple):\n    try:\n        image=get_next_image(s)\n        height,width,layers=image[0].shape\n        #fourcc = cv2.VideoWriter_fourcc(*'mp4v') \n        fourcc = cv2.VideoWriter_fourcc('M','J','P','G')\n        video = cv2.VideoWriter('/home/sleekeagle/vuzix/data/'+dt_string+'.avi', fourcc, 30, (width, height))",
        "detail": "DataCollection.AndroidDevelopment.AV_Streaming_App.camstrm.write_vid",
        "documentation": {}
    },
    {
        "label": "audio_stream_UDP",
        "kind": 2,
        "importPath": "DataCollection.Server.old_code.pythonServer",
        "description": "DataCollection.Server.old_code.pythonServer",
        "peekOfCode": "def audio_stream_UDP():\n    BUFF_SIZE = 1280 #65536\n    client_socket = socket.socket(socket.AF_INET,socket.SOCK_DGRAM)\n    # client_socket.setsockopt(socket.SOL_SOCKET,socket.SO_RCVBUF,BUFF_SIZE)\n    client_socket.bind((host_ip,port))\n    p = pyaudio.PyAudio()\n    CHUNK = 128\n    stream = p.open(format=p.get_format_from_width(2),\n                    channels=1,\n                    rate=16000,#44100,",
        "detail": "DataCollection.Server.old_code.pythonServer",
        "documentation": {}
    },
    {
        "label": "host_name",
        "kind": 5,
        "importPath": "DataCollection.Server.old_code.pythonServer",
        "description": "DataCollection.Server.old_code.pythonServer",
        "peekOfCode": "host_name = socket.gethostname()\nhost_ip = '0.0.0.0' #'172.25.149.100'#'192.168.1.102'#  socket.gethostbyname(host_name)\nprint(host_ip)\nport = 50005\n# For details visit: www.pyshine.com\nq = queue.Queue(maxsize=12800)\ndef audio_stream_UDP():\n    BUFF_SIZE = 1280 #65536\n    client_socket = socket.socket(socket.AF_INET,socket.SOCK_DGRAM)\n    # client_socket.setsockopt(socket.SOL_SOCKET,socket.SO_RCVBUF,BUFF_SIZE)",
        "detail": "DataCollection.Server.old_code.pythonServer",
        "documentation": {}
    },
    {
        "label": "host_ip",
        "kind": 5,
        "importPath": "DataCollection.Server.old_code.pythonServer",
        "description": "DataCollection.Server.old_code.pythonServer",
        "peekOfCode": "host_ip = '0.0.0.0' #'172.25.149.100'#'192.168.1.102'#  socket.gethostbyname(host_name)\nprint(host_ip)\nport = 50005\n# For details visit: www.pyshine.com\nq = queue.Queue(maxsize=12800)\ndef audio_stream_UDP():\n    BUFF_SIZE = 1280 #65536\n    client_socket = socket.socket(socket.AF_INET,socket.SOCK_DGRAM)\n    # client_socket.setsockopt(socket.SOL_SOCKET,socket.SO_RCVBUF,BUFF_SIZE)\n    client_socket.bind((host_ip,port))",
        "detail": "DataCollection.Server.old_code.pythonServer",
        "documentation": {}
    },
    {
        "label": "port",
        "kind": 5,
        "importPath": "DataCollection.Server.old_code.pythonServer",
        "description": "DataCollection.Server.old_code.pythonServer",
        "peekOfCode": "port = 50005\n# For details visit: www.pyshine.com\nq = queue.Queue(maxsize=12800)\ndef audio_stream_UDP():\n    BUFF_SIZE = 1280 #65536\n    client_socket = socket.socket(socket.AF_INET,socket.SOCK_DGRAM)\n    # client_socket.setsockopt(socket.SOL_SOCKET,socket.SO_RCVBUF,BUFF_SIZE)\n    client_socket.bind((host_ip,port))\n    p = pyaudio.PyAudio()\n    CHUNK = 128",
        "detail": "DataCollection.Server.old_code.pythonServer",
        "documentation": {}
    },
    {
        "label": "q",
        "kind": 5,
        "importPath": "DataCollection.Server.old_code.pythonServer",
        "description": "DataCollection.Server.old_code.pythonServer",
        "peekOfCode": "q = queue.Queue(maxsize=12800)\ndef audio_stream_UDP():\n    BUFF_SIZE = 1280 #65536\n    client_socket = socket.socket(socket.AF_INET,socket.SOCK_DGRAM)\n    # client_socket.setsockopt(socket.SOL_SOCKET,socket.SO_RCVBUF,BUFF_SIZE)\n    client_socket.bind((host_ip,port))\n    p = pyaudio.PyAudio()\n    CHUNK = 128\n    stream = p.open(format=p.get_format_from_width(2),\n                    channels=1,",
        "detail": "DataCollection.Server.old_code.pythonServer",
        "documentation": {}
    },
    {
        "label": "t1",
        "kind": 5,
        "importPath": "DataCollection.Server.old_code.pythonServer",
        "description": "DataCollection.Server.old_code.pythonServer",
        "peekOfCode": "t1 = threading.Thread(target=audio_stream_UDP, args=())\nt1.start()",
        "detail": "DataCollection.Server.old_code.pythonServer",
        "documentation": {}
    },
    {
        "label": "UDPServerSocket",
        "kind": 5,
        "importPath": "DataCollection.Server.old_code.server",
        "description": "DataCollection.Server.old_code.server",
        "peekOfCode": "UDPServerSocket = socket.socket(family=socket.AF_INET, type=socket.SOCK_DGRAM)\n# Bind to address and ip\nUDPServerSocket.bind((localIP, localPort))\nprint(\"UDP server up and listening\")\n# Listen for incoming datagrams\nwhile(True):\n    print(time.time()*1e3)\n    bytesAddressPair = UDPServerSocket.recvfrom(bufferSize)\n    message = bytesAddressPair[0]\n    address = bytesAddressPair[1]",
        "detail": "DataCollection.Server.old_code.server",
        "documentation": {}
    },
    {
        "label": "UDPClientSocket",
        "kind": 5,
        "importPath": "DataCollection.Server.old_code.udp_client",
        "description": "DataCollection.Server.old_code.udp_client",
        "peekOfCode": "UDPClientSocket = socket.socket(family=socket.AF_INET, type=socket.SOCK_DGRAM)\nwhile True:\n    # Send to server using created UDP socket\n    UDPClientSocket.sendto(bytesToSend, serverAddressPort)\n    time.sleep(2)\n# msgFromServer = UDPClientSocket.recvfrom(bufferSize)\n# msg = \"Message from Server {}\".format(msgFromServer[0])\n# print(msg)",
        "detail": "DataCollection.Server.old_code.udp_client",
        "documentation": {}
    },
    {
        "label": "connect",
        "kind": 2,
        "importPath": "DataCollection.Server.old_code.write_vid",
        "description": "DataCollection.Server.old_code.write_vid",
        "peekOfCode": "def connect():\n    read_int=-1\n    print(\"waiting for connection....\")\n    while(read_int!=100):\n        try:\n            s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n            s.connect((TCP_IP, TCP_PORT))\n            read_int=int.from_bytes(s.recv(1),\"big\")\n        except Exception as e:\n            print(e)",
        "detail": "DataCollection.Server.old_code.write_vid",
        "documentation": {}
    },
    {
        "label": "get_next_image",
        "kind": 2,
        "importPath": "DataCollection.Server.old_code.write_vid",
        "description": "DataCollection.Server.old_code.write_vid",
        "peekOfCode": "def get_next_image(s):\n    try:\n        d_type=int.from_bytes(s.recv(1),\"big\")      \n        if(d_type!=22):\n            return -1\n        seq=int.from_bytes(s.recv(4),\"big\")\n        height=int.from_bytes(s.recv(4),\"big\")\n        width=int.from_bytes(s.recv(4),\"big\")\n        size=int.from_bytes(s.recv(4),\"big\")\n        if(size>1000000):",
        "detail": "DataCollection.Server.old_code.write_vid",
        "documentation": {}
    },
    {
        "label": "TCP_IP",
        "kind": 5,
        "importPath": "DataCollection.Server.old_code.write_vid",
        "description": "DataCollection.Server.old_code.write_vid",
        "peekOfCode": "TCP_IP = '127.0.0.1'\nTCP_PORT = 8899\ndef connect():\n    read_int=-1\n    print(\"waiting for connection....\")\n    while(read_int!=100):\n        try:\n            s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n            s.connect((TCP_IP, TCP_PORT))\n            read_int=int.from_bytes(s.recv(1),\"big\")",
        "detail": "DataCollection.Server.old_code.write_vid",
        "documentation": {}
    },
    {
        "label": "TCP_PORT",
        "kind": 5,
        "importPath": "DataCollection.Server.old_code.write_vid",
        "description": "DataCollection.Server.old_code.write_vid",
        "peekOfCode": "TCP_PORT = 8899\ndef connect():\n    read_int=-1\n    print(\"waiting for connection....\")\n    while(read_int!=100):\n        try:\n            s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n            s.connect((TCP_IP, TCP_PORT))\n            read_int=int.from_bytes(s.recv(1),\"big\")\n        except Exception as e:",
        "detail": "DataCollection.Server.old_code.write_vid",
        "documentation": {}
    },
    {
        "label": "now",
        "kind": 5,
        "importPath": "DataCollection.Server.old_code.write_vid",
        "description": "DataCollection.Server.old_code.write_vid",
        "peekOfCode": "now = datetime.now()\ndt_string = now.strftime(\"%d-%m-%Y-%H-%M-%S\")\ns=connect()\np = struct.pack('!i', 23)\ns.send(p)\nimage=-1\nwhile(type(image)!=tuple):\n    try:\n        image=get_next_image(s)\n        height,width,layers=image[0].shape",
        "detail": "DataCollection.Server.old_code.write_vid",
        "documentation": {}
    },
    {
        "label": "dt_string",
        "kind": 5,
        "importPath": "DataCollection.Server.old_code.write_vid",
        "description": "DataCollection.Server.old_code.write_vid",
        "peekOfCode": "dt_string = now.strftime(\"%d-%m-%Y-%H-%M-%S\")\ns=connect()\np = struct.pack('!i', 23)\ns.send(p)\nimage=-1\nwhile(type(image)!=tuple):\n    try:\n        image=get_next_image(s)\n        height,width,layers=image[0].shape\n        #fourcc = cv2.VideoWriter_fourcc(*'mp4v') ",
        "detail": "DataCollection.Server.old_code.write_vid",
        "documentation": {}
    },
    {
        "label": "p",
        "kind": 5,
        "importPath": "DataCollection.Server.old_code.write_vid",
        "description": "DataCollection.Server.old_code.write_vid",
        "peekOfCode": "p = struct.pack('!i', 23)\ns.send(p)\nimage=-1\nwhile(type(image)!=tuple):\n    try:\n        image=get_next_image(s)\n        height,width,layers=image[0].shape\n        #fourcc = cv2.VideoWriter_fourcc(*'mp4v') \n        fourcc = cv2.VideoWriter_fourcc('M','J','P','G')\n        video = cv2.VideoWriter('/home/sleekeagle/vuzix/data/'+dt_string+'.avi', fourcc, 30, (width, height))",
        "detail": "DataCollection.Server.old_code.write_vid",
        "documentation": {}
    },
    {
        "label": "get_image",
        "kind": 2,
        "importPath": "DataCollection.Server.tcp_video_server",
        "description": "DataCollection.Server.tcp_video_server",
        "peekOfCode": "def get_image():\n    try:\n        buf = connection.recv(4)\n        if( not buf):\n            return -1\n        d_type=int.from_bytes(buf,\"big\")\n        if(d_type!=22):\n            return -1\n        # print(\"received: \",d_type)     \n        seq=int.from_bytes(connection.recv(4),\"big\")",
        "detail": "DataCollection.Server.tcp_video_server",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "DataCollection.Server.tcp_video_server",
        "description": "DataCollection.Server.tcp_video_server",
        "peekOfCode": "logger = logging.getLogger()\n# Setting the threshold of logger to DEBUG\nlogger.setLevel(logging.DEBUG)\n# Test messages\n# logger.debug(\"Harmless debug Message\")\n# logger.info(\"Just an information\")\n# logger.warning(\"Its a Warning\")\n# logger.error(\"Did you try to divide by zero\")\n# logger.critical(\"Internet is down\")\nsock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)  ",
        "detail": "DataCollection.Server.tcp_video_server",
        "documentation": {}
    },
    {
        "label": "sock",
        "kind": 5,
        "importPath": "DataCollection.Server.tcp_video_server",
        "description": "DataCollection.Server.tcp_video_server",
        "peekOfCode": "sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)  \nsock.bind(('0.0.0.0', 8899))  \nsock.listen(5)  \nprint(\"Waiting for client...\")\nconnection,address = sock.accept()  \nprint(\"Client connected: \",address)\n# connection.send(b'Oi you sent something to me')\ndef get_image():\n    try:\n        buf = connection.recv(4)",
        "detail": "DataCollection.Server.tcp_video_server",
        "documentation": {}
    },
    {
        "label": "connection,address",
        "kind": 5,
        "importPath": "DataCollection.Server.tcp_video_server",
        "description": "DataCollection.Server.tcp_video_server",
        "peekOfCode": "connection,address = sock.accept()  \nprint(\"Client connected: \",address)\n# connection.send(b'Oi you sent something to me')\ndef get_image():\n    try:\n        buf = connection.recv(4)\n        if( not buf):\n            return -1\n        d_type=int.from_bytes(buf,\"big\")\n        if(d_type!=22):",
        "detail": "DataCollection.Server.tcp_video_server",
        "documentation": {}
    },
    {
        "label": "recording_enabled",
        "kind": 5,
        "importPath": "DataCollection.Server.tcp_video_server",
        "description": "DataCollection.Server.tcp_video_server",
        "peekOfCode": "recording_enabled = True\nframe_index = 0\ncurr_date = datetime.now()\ndt_string = curr_date.strftime(\"%d-%m-%Y-%H-%M-%S\")\nnewpath = \"./video_data/\"+dt_string\nif(recording_enabled):\n    if not os.path.exists(newpath):\n        os.makedirs(newpath)\n    with open('./video_data/'+dt_string+'/'+dt_string+'data.csv', 'w', newline='') as file:\n        writer = csv.writer(file)",
        "detail": "DataCollection.Server.tcp_video_server",
        "documentation": {}
    },
    {
        "label": "frame_index",
        "kind": 5,
        "importPath": "DataCollection.Server.tcp_video_server",
        "description": "DataCollection.Server.tcp_video_server",
        "peekOfCode": "frame_index = 0\ncurr_date = datetime.now()\ndt_string = curr_date.strftime(\"%d-%m-%Y-%H-%M-%S\")\nnewpath = \"./video_data/\"+dt_string\nif(recording_enabled):\n    if not os.path.exists(newpath):\n        os.makedirs(newpath)\n    with open('./video_data/'+dt_string+'/'+dt_string+'data.csv', 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow([\"frame\", \"recieved_ts\", \"origin_ts\"])",
        "detail": "DataCollection.Server.tcp_video_server",
        "documentation": {}
    },
    {
        "label": "curr_date",
        "kind": 5,
        "importPath": "DataCollection.Server.tcp_video_server",
        "description": "DataCollection.Server.tcp_video_server",
        "peekOfCode": "curr_date = datetime.now()\ndt_string = curr_date.strftime(\"%d-%m-%Y-%H-%M-%S\")\nnewpath = \"./video_data/\"+dt_string\nif(recording_enabled):\n    if not os.path.exists(newpath):\n        os.makedirs(newpath)\n    with open('./video_data/'+dt_string+'/'+dt_string+'data.csv', 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow([\"frame\", \"recieved_ts\", \"origin_ts\"])\n        is_video_created = False",
        "detail": "DataCollection.Server.tcp_video_server",
        "documentation": {}
    },
    {
        "label": "dt_string",
        "kind": 5,
        "importPath": "DataCollection.Server.tcp_video_server",
        "description": "DataCollection.Server.tcp_video_server",
        "peekOfCode": "dt_string = curr_date.strftime(\"%d-%m-%Y-%H-%M-%S\")\nnewpath = \"./video_data/\"+dt_string\nif(recording_enabled):\n    if not os.path.exists(newpath):\n        os.makedirs(newpath)\n    with open('./video_data/'+dt_string+'/'+dt_string+'data.csv', 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow([\"frame\", \"recieved_ts\", \"origin_ts\"])\n        is_video_created = False\n        while True:",
        "detail": "DataCollection.Server.tcp_video_server",
        "documentation": {}
    },
    {
        "label": "newpath",
        "kind": 5,
        "importPath": "DataCollection.Server.tcp_video_server",
        "description": "DataCollection.Server.tcp_video_server",
        "peekOfCode": "newpath = \"./video_data/\"+dt_string\nif(recording_enabled):\n    if not os.path.exists(newpath):\n        os.makedirs(newpath)\n    with open('./video_data/'+dt_string+'/'+dt_string+'data.csv', 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow([\"frame\", \"recieved_ts\", \"origin_ts\"])\n        is_video_created = False\n        while True:\n            if( int.from_bytes(connection.recv(1),\"big\") == 22):",
        "detail": "DataCollection.Server.tcp_video_server",
        "documentation": {}
    },
    {
        "label": "UDPServerSocket",
        "kind": 5,
        "importPath": "DataCollection.Server.udp_smartwatch_server",
        "description": "DataCollection.Server.udp_smartwatch_server",
        "peekOfCode": "UDPServerSocket = socket.socket(family=socket.AF_INET, type=socket.SOCK_DGRAM)\n# Bind to address and ip\nUDPServerSocket.bind((localIP, localPort))\nprint(\"UDP server up and listening\")\nbuffer = collections.deque([])\n# Listen for incoming datagrams\ncolumns = ['epoch_ms','wrist_position','sensor_type','value_X_Axis','value_Y_Axis','value_Z_Axis']\ncurr_date = datetime.now()\ndt_string = curr_date.strftime(\"%d-%m-%Y-%H-%M-%S\")\nnewpath = \"./smartwatch_data/\"+dt_string",
        "detail": "DataCollection.Server.udp_smartwatch_server",
        "documentation": {}
    },
    {
        "label": "buffer",
        "kind": 5,
        "importPath": "DataCollection.Server.udp_smartwatch_server",
        "description": "DataCollection.Server.udp_smartwatch_server",
        "peekOfCode": "buffer = collections.deque([])\n# Listen for incoming datagrams\ncolumns = ['epoch_ms','wrist_position','sensor_type','value_X_Axis','value_Y_Axis','value_Z_Axis']\ncurr_date = datetime.now()\ndt_string = curr_date.strftime(\"%d-%m-%Y-%H-%M-%S\")\nnewpath = \"./smartwatch_data/\"+dt_string\nif not os.path.exists(newpath):\n    os.makedirs(newpath)\n    with open('./smartwatch_data/'+dt_string+'/'+dt_string+'data.csv', 'w', newline='') as file:\n        writer = csv.writer(file)",
        "detail": "DataCollection.Server.udp_smartwatch_server",
        "documentation": {}
    },
    {
        "label": "columns",
        "kind": 5,
        "importPath": "DataCollection.Server.udp_smartwatch_server",
        "description": "DataCollection.Server.udp_smartwatch_server",
        "peekOfCode": "columns = ['epoch_ms','wrist_position','sensor_type','value_X_Axis','value_Y_Axis','value_Z_Axis']\ncurr_date = datetime.now()\ndt_string = curr_date.strftime(\"%d-%m-%Y-%H-%M-%S\")\nnewpath = \"./smartwatch_data/\"+dt_string\nif not os.path.exists(newpath):\n    os.makedirs(newpath)\n    with open('./smartwatch_data/'+dt_string+'/'+dt_string+'data.csv', 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(columns)\n        while(True):",
        "detail": "DataCollection.Server.udp_smartwatch_server",
        "documentation": {}
    },
    {
        "label": "curr_date",
        "kind": 5,
        "importPath": "DataCollection.Server.udp_smartwatch_server",
        "description": "DataCollection.Server.udp_smartwatch_server",
        "peekOfCode": "curr_date = datetime.now()\ndt_string = curr_date.strftime(\"%d-%m-%Y-%H-%M-%S\")\nnewpath = \"./smartwatch_data/\"+dt_string\nif not os.path.exists(newpath):\n    os.makedirs(newpath)\n    with open('./smartwatch_data/'+dt_string+'/'+dt_string+'data.csv', 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(columns)\n        while(True):\n            bytesAddressPair = UDPServerSocket.recvfrom(bufferSize)",
        "detail": "DataCollection.Server.udp_smartwatch_server",
        "documentation": {}
    },
    {
        "label": "dt_string",
        "kind": 5,
        "importPath": "DataCollection.Server.udp_smartwatch_server",
        "description": "DataCollection.Server.udp_smartwatch_server",
        "peekOfCode": "dt_string = curr_date.strftime(\"%d-%m-%Y-%H-%M-%S\")\nnewpath = \"./smartwatch_data/\"+dt_string\nif not os.path.exists(newpath):\n    os.makedirs(newpath)\n    with open('./smartwatch_data/'+dt_string+'/'+dt_string+'data.csv', 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(columns)\n        while(True):\n            bytesAddressPair = UDPServerSocket.recvfrom(bufferSize)\n            message = bytesAddressPair[0]",
        "detail": "DataCollection.Server.udp_smartwatch_server",
        "documentation": {}
    },
    {
        "label": "newpath",
        "kind": 5,
        "importPath": "DataCollection.Server.udp_smartwatch_server",
        "description": "DataCollection.Server.udp_smartwatch_server",
        "peekOfCode": "newpath = \"./smartwatch_data/\"+dt_string\nif not os.path.exists(newpath):\n    os.makedirs(newpath)\n    with open('./smartwatch_data/'+dt_string+'/'+dt_string+'data.csv', 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(columns)\n        while(True):\n            bytesAddressPair = UDPServerSocket.recvfrom(bufferSize)\n            message = bytesAddressPair[0]\n            address = bytesAddressPair[1]",
        "detail": "DataCollection.Server.udp_smartwatch_server",
        "documentation": {}
    },
    {
        "label": "Amplitude",
        "kind": 6,
        "importPath": "Demo.DSP.amplitude",
        "description": "Demo.DSP.amplitude",
        "peekOfCode": "class Amplitude(object):\n    ''' an abstraction for Amplitudes (with an underlying float value)\n    that packages a display function and many more '''\n    def __init__(self, value=0):\n        self.value = value\n    def __add__(self, other):\n        return Amplitude(self.value + other.value)\n    def __sub__(self, other):\n        return Amplitude(self.value - other.value)\n    def __gt__(self, other):",
        "detail": "Demo.DSP.amplitude",
        "documentation": {}
    },
    {
        "label": "RATE",
        "kind": 5,
        "importPath": "Demo.DSP.vu_constants",
        "description": "Demo.DSP.vu_constants",
        "peekOfCode": "RATE = 1600\nINPUT_BLOCK_TIME = .1\nINPUT_FRAMES_PER_BLOCK = int(RATE*INPUT_BLOCK_TIME)\nSHORT_NORMALIZE = 1.0 / 32768.0",
        "detail": "Demo.DSP.vu_constants",
        "documentation": {}
    },
    {
        "label": "INPUT_BLOCK_TIME",
        "kind": 5,
        "importPath": "Demo.DSP.vu_constants",
        "description": "Demo.DSP.vu_constants",
        "peekOfCode": "INPUT_BLOCK_TIME = .1\nINPUT_FRAMES_PER_BLOCK = int(RATE*INPUT_BLOCK_TIME)\nSHORT_NORMALIZE = 1.0 / 32768.0",
        "detail": "Demo.DSP.vu_constants",
        "documentation": {}
    },
    {
        "label": "INPUT_FRAMES_PER_BLOCK",
        "kind": 5,
        "importPath": "Demo.DSP.vu_constants",
        "description": "Demo.DSP.vu_constants",
        "peekOfCode": "INPUT_FRAMES_PER_BLOCK = int(RATE*INPUT_BLOCK_TIME)\nSHORT_NORMALIZE = 1.0 / 32768.0",
        "detail": "Demo.DSP.vu_constants",
        "documentation": {}
    },
    {
        "label": "SHORT_NORMALIZE",
        "kind": 5,
        "importPath": "Demo.DSP.vu_constants",
        "description": "Demo.DSP.vu_constants",
        "peekOfCode": "SHORT_NORMALIZE = 1.0 / 32768.0",
        "detail": "Demo.DSP.vu_constants",
        "documentation": {}
    },
    {
        "label": "update_concept_targets",
        "kind": 2,
        "importPath": "Demo.EMS-Pipeline-pycontextnlp-negation-detector.EMS_Ontology_Concept_Insertion",
        "description": "Demo.EMS-Pipeline-pycontextnlp-negation-detector.EMS_Ontology_Concept_Insertion",
        "peekOfCode": "def update_concept_targets(ems_ontology_df):\n    # FIRST: read the existing yml file into a map\n    # map relates strings -> sets\n    dict_concepts_to_sets = {}\n    dict_metadata = {}\n    with open(r'data/pycontextnlp_targets.yml') as file:\n        concepts_list = yaml.load_all(file, Loader=yaml.FullLoader)\n        for concept in concepts_list:\n            concept_to_add = ''\n            for k, v in concept.items():",
        "detail": "Demo.EMS-Pipeline-pycontextnlp-negation-detector.EMS_Ontology_Concept_Insertion",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Demo.EMS-Pipeline-pycontextnlp-negation-detector.EMS_Ontology_Concept_Insertion",
        "description": "Demo.EMS-Pipeline-pycontextnlp-negation-detector.EMS_Ontology_Concept_Insertion",
        "peekOfCode": "def main():\n    ems_ontology_df = pd.read_excel(\"data/All_In_One_Extensions_ProtocolModeling_Weighted.xls\")\n    update_concept_targets(ems_ontology_df)\nif __name__ == \"__main__\":\n    main()",
        "detail": "Demo.EMS-Pipeline-pycontextnlp-negation-detector.EMS_Ontology_Concept_Insertion",
        "documentation": {}
    },
    {
        "label": "transcript_iterator",
        "kind": 2,
        "importPath": "Demo.EMS-Pipeline-pycontextnlp-negation-detector.TranscriptToSentencesOfTokens",
        "description": "Demo.EMS-Pipeline-pycontextnlp-negation-detector.TranscriptToSentencesOfTokens",
        "peekOfCode": "def transcript_iterator(clinical_text_df):\n    for index, row in clinical_text_df.iterrows():\n        # if the row has a NaN value in the transcripts column, skip it\n        if pd.isna(clinical_text_df.iloc[index, 0]):\n            continue\n        print(\"Transcript \" + str(index) + \", row \" + str(index + 2) + \":\")\n        transcript_to_process = row[0]\n        sentences = transcript_to_sentences_of_tokens(transcript_to_process, False)\n        for sentence in sentences:\n            print(sentence)",
        "detail": "Demo.EMS-Pipeline-pycontextnlp-negation-detector.TranscriptToSentencesOfTokens",
        "documentation": {}
    },
    {
        "label": "transcript_to_sentences_of_tokens",
        "kind": 2,
        "importPath": "Demo.EMS-Pipeline-pycontextnlp-negation-detector.TranscriptToSentencesOfTokens",
        "description": "Demo.EMS-Pipeline-pycontextnlp-negation-detector.TranscriptToSentencesOfTokens",
        "peekOfCode": "def transcript_to_sentences_of_tokens(transcript_to_process, return_tokens=True):\n    # first split the transcript by whitespace into a list\n    list_tokens = transcript_to_process.split()\n    # if a tokens last character is ':' or '-', add a special character ^ to the end of the token before it\n    # indicating end of sentence.\n    for i in range(len(list_tokens)):\n        if (list_tokens[i][-1] == ':' or list_tokens[i][-1] == '-') \\\n                and (i != 0) \\\n                and (list_tokens[i - 1][-1] != '.') \\\n                and (list_tokens[i - 1][-1] != '!') \\",
        "detail": "Demo.EMS-Pipeline-pycontextnlp-negation-detector.TranscriptToSentencesOfTokens",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Demo.EMS-Pipeline-pycontextnlp-negation-detector.TranscriptToSentencesOfTokens",
        "description": "Demo.EMS-Pipeline-pycontextnlp-negation-detector.TranscriptToSentencesOfTokens",
        "peekOfCode": "def main():\n    clinical_text_df = pd.read_excel(\"data/eimara_annotations.xls\")\n    transcript_iterator(clinical_text_df)\nif __name__ == \"__main__\":\n    main()",
        "detail": "Demo.EMS-Pipeline-pycontextnlp-negation-detector.TranscriptToSentencesOfTokens",
        "documentation": {}
    },
    {
        "label": "negations_pycontextnlp",
        "kind": 2,
        "importPath": "Demo.EMS-Pipeline-pycontextnlp-negation-detector.pyConTextNLP_Annotation_Creator",
        "description": "Demo.EMS-Pipeline-pycontextnlp-negation-detector.pyConTextNLP_Annotation_Creator",
        "peekOfCode": "def negations_pycontextnlp(clinical_text_df):\n    for index, row in clinical_text_df.iterrows():\n        # if the row has a NaN value in the transcripts column, skip it\n        if (pd.isna(clinical_text_df.iloc[index, 0])):\n            continue\n        print(\"Transcript \" + str(index) + \", row \" + str(index + 2) + \":\")\n        # print(\"Detected negated edges:\")\n        list_detected_negated_edges, list_positions = negations_pycontextnlp_individual_transcript(row[0])\n        print(\"Detected negated concepts:\\n\")\n        # # UNCOMMENT THIS BLOCK TO DETECT ALL ANNOTATIONS (NOT JUST 'NON' CASE AND '-' CASE) ############",
        "detail": "Demo.EMS-Pipeline-pycontextnlp-negation-detector.pyConTextNLP_Annotation_Creator",
        "documentation": {}
    },
    {
        "label": "negations_pycontextnlp_individual_transcript",
        "kind": 2,
        "importPath": "Demo.EMS-Pipeline-pycontextnlp-negation-detector.pyConTextNLP_Annotation_Creator",
        "description": "Demo.EMS-Pipeline-pycontextnlp-negation-detector.pyConTextNLP_Annotation_Creator",
        "peekOfCode": "def negations_pycontextnlp_individual_transcript(clinical_text):\n    PYCONTEXTNLP_MODIFIERS = r'/' + os.getcwd() + '/data/pycontextnlp_modifiers.yml'\n    PYCONTEXTNLP_TARGETS = r'/' + os.getcwd() + '/data/pycontextnlp_targets.yml'\n    modifiers = itemData.get_items(PYCONTEXTNLP_MODIFIERS)\n    targets = itemData.get_items(PYCONTEXTNLP_TARGETS)\n    sentences = transcript_to_sentences_of_tokens(clinical_text, False)\n    list_negated_edges = []\n    list_positions = []\n    curr_combined_length = 0\n    for sentence in sentences:",
        "detail": "Demo.EMS-Pipeline-pycontextnlp-negation-detector.pyConTextNLP_Annotation_Creator",
        "documentation": {}
    },
    {
        "label": "pycontextnlp_markup_sentence",
        "kind": 2,
        "importPath": "Demo.EMS-Pipeline-pycontextnlp-negation-detector.pyConTextNLP_Annotation_Creator",
        "description": "Demo.EMS-Pipeline-pycontextnlp-negation-detector.pyConTextNLP_Annotation_Creator",
        "peekOfCode": "def pycontextnlp_markup_sentence(s, modifiers, targets, prune_inactive=True):\n    markup = pyConText.ConTextMarkup()\n    markup.setRawText(s)\n    markup.cleanText()\n    markup.markItems(modifiers, mode=\"modifier\")\n    markup.markItems(targets, mode=\"target\")\n    markup.pruneMarks()\n    markup.dropMarks('Exclusion')\n    markup.applyModifiers()\n    markup.pruneSelfModifyingRelationships()",
        "detail": "Demo.EMS-Pipeline-pycontextnlp-negation-detector.pyConTextNLP_Annotation_Creator",
        "documentation": {}
    },
    {
        "label": "transcript_to_sentences_of_tokens",
        "kind": 2,
        "importPath": "Demo.EMS-Pipeline-pycontextnlp-negation-detector.pyConTextNLP_Annotation_Creator",
        "description": "Demo.EMS-Pipeline-pycontextnlp-negation-detector.pyConTextNLP_Annotation_Creator",
        "peekOfCode": "def transcript_to_sentences_of_tokens(transcript_to_process, return_tokens=True):\n    # first split the transcript by whitespace into a list\n    list_tokens = transcript_to_process.split()\n    # if a tokens last character is ':' or '-', add a special character ^ to the end of the token before it\n    # indicating end of sentence.\n    for i in range(len(list_tokens)):\n        if (list_tokens[i][-1] == ':' or list_tokens[i][-1] == '-') \\\n                and (i != 0) \\\n                and (list_tokens[i - 1][-1] != '.') \\\n                and (list_tokens[i - 1][-1] != '!') \\",
        "detail": "Demo.EMS-Pipeline-pycontextnlp-negation-detector.pyConTextNLP_Annotation_Creator",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Demo.EMS-Pipeline-pycontextnlp-negation-detector.pyConTextNLP_Annotation_Creator",
        "description": "Demo.EMS-Pipeline-pycontextnlp-negation-detector.pyConTextNLP_Annotation_Creator",
        "peekOfCode": "def main():\n    clinical_text_df = pd.read_excel(\"data/eimara_annotations.xls\")\n    # file used for testing purposes\n    # clinical_text_df = pd.read_excel(\"data/test_opposite_concepts.xls\")\n    # pycontextnlp method\n    negations_pycontextnlp(clinical_text_df)\nif __name__ == \"__main__\":\n    main()",
        "detail": "Demo.EMS-Pipeline-pycontextnlp-negation-detector.pyConTextNLP_Annotation_Creator",
        "documentation": {}
    },
    {
        "label": "negations_pycontextnlp",
        "kind": 2,
        "importPath": "Demo.EMS-Pipeline-pycontextnlp-negation-detector.pyConTextNLP_Negation_Detector",
        "description": "Demo.EMS-Pipeline-pycontextnlp-negation-detector.pyConTextNLP_Negation_Detector",
        "peekOfCode": "def negations_pycontextnlp(clinical_text_df):\n    total_neg_concepts_detected = 0\n    total_expected_negated_concepts = 0\n    precision_sum = 0.\n    recall_sum = 0.\n    f1_sum = 0.\n    total_transcripts_passed = 0.\n    for index, row in clinical_text_df.iterrows():\n        # if the row has a NaN value in the transcripts column, skip it\n        if (pd.isna(clinical_text_df.iloc[index, 0])):",
        "detail": "Demo.EMS-Pipeline-pycontextnlp-negation-detector.pyConTextNLP_Negation_Detector",
        "documentation": {}
    },
    {
        "label": "negations_pycontextnlp_individual_transcript",
        "kind": 2,
        "importPath": "Demo.EMS-Pipeline-pycontextnlp-negation-detector.pyConTextNLP_Negation_Detector",
        "description": "Demo.EMS-Pipeline-pycontextnlp-negation-detector.pyConTextNLP_Negation_Detector",
        "peekOfCode": "def negations_pycontextnlp_individual_transcript(clinical_text):\n    PYCONTEXTNLP_MODIFIERS = r'/' + os.getcwd() + '/data/pycontextnlp_modifiers.yml'\n    PYCONTEXTNLP_TARGETS = r'/' + os.getcwd() + '/data/pycontextnlp_targets.yml'\n    modifiers = itemData.get_items(PYCONTEXTNLP_MODIFIERS)\n    targets = itemData.get_items(PYCONTEXTNLP_TARGETS)\n    sentences = transcript_to_sentences_of_tokens(clinical_text, False)\n    list_negated_edges = []\n    list_positions = []\n    curr_combined_length = 0\n    for sentence in sentences:",
        "detail": "Demo.EMS-Pipeline-pycontextnlp-negation-detector.pyConTextNLP_Negation_Detector",
        "documentation": {}
    },
    {
        "label": "pycontextnlp_markup_sentence",
        "kind": 2,
        "importPath": "Demo.EMS-Pipeline-pycontextnlp-negation-detector.pyConTextNLP_Negation_Detector",
        "description": "Demo.EMS-Pipeline-pycontextnlp-negation-detector.pyConTextNLP_Negation_Detector",
        "peekOfCode": "def pycontextnlp_markup_sentence(s, modifiers, targets, prune_inactive=True):\n    markup = pyConText.ConTextMarkup()\n    markup.setRawText(s)\n    markup.cleanText()\n    markup.markItems(modifiers, mode=\"modifier\")\n    markup.markItems(targets, mode=\"target\")\n    markup.pruneMarks()\n    markup.dropMarks('Exclusion')\n    markup.applyModifiers()\n    markup.pruneSelfModifyingRelationships()",
        "detail": "Demo.EMS-Pipeline-pycontextnlp-negation-detector.pyConTextNLP_Negation_Detector",
        "documentation": {}
    },
    {
        "label": "transcript_to_sentences_of_tokens",
        "kind": 2,
        "importPath": "Demo.EMS-Pipeline-pycontextnlp-negation-detector.pyConTextNLP_Negation_Detector",
        "description": "Demo.EMS-Pipeline-pycontextnlp-negation-detector.pyConTextNLP_Negation_Detector",
        "peekOfCode": "def transcript_to_sentences_of_tokens(transcript_to_process, return_tokens=True):\n    # first split the transcript by whitespace into a list\n    list_tokens = transcript_to_process.split()\n    # if a tokens last character is ':' or '-', add a special character ^ to the end of the token before it\n    # indicating end of sentence.\n    for i in range(len(list_tokens)):\n        if (list_tokens[i][-1] == ':' or list_tokens[i][-1] == '-') \\\n                and (i != 0) \\\n                and (list_tokens[i - 1][-1] != '.') \\\n                and (list_tokens[i - 1][-1] != '!') \\",
        "detail": "Demo.EMS-Pipeline-pycontextnlp-negation-detector.pyConTextNLP_Negation_Detector",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Demo.EMS-Pipeline-pycontextnlp-negation-detector.pyConTextNLP_Negation_Detector",
        "description": "Demo.EMS-Pipeline-pycontextnlp-negation-detector.pyConTextNLP_Negation_Detector",
        "peekOfCode": "def main():\n    clinical_text_df = pd.read_excel(\"data/eimara_annotations.xls\")\n    # file used for testing purposes\n    # clinical_text_df = pd.read_excel(\"data/test_opposite_concepts.xls\")\n    # pycontextnlp method\n    negations_pycontextnlp(clinical_text_df)\nif __name__ == \"__main__\":\n    main()",
        "detail": "Demo.EMS-Pipeline-pycontextnlp-negation-detector.pyConTextNLP_Negation_Detector",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Demo.EMS-Pipeline-pycontextnlp-negation-detector.pyConTextNLP_Usage_Example",
        "description": "Demo.EMS-Pipeline-pycontextnlp-negation-detector.pyConTextNLP_Usage_Example",
        "peekOfCode": "def main():\n    PYCONTEXTNLP_MODIFIERS = r'/' + os.getcwd() + '/data/pycontextnlp_modifiers.yml'\n    PYCONTEXTNLP_TARGETS = r'/' + os.getcwd() + '/data/pycontextnlp_targets.yml'\n    test_str = \"not n/v/d \"\n    modifiers = itemData.get_items(PYCONTEXTNLP_MODIFIERS)\n    targets = itemData.get_items(PYCONTEXTNLP_TARGETS)\n    markup = pyConText.ConTextMarkup()\n    print(test_str.lower())\n    markup.setRawText(test_str.lower())\n    # print(markup)",
        "detail": "Demo.EMS-Pipeline-pycontextnlp-negation-detector.pyConTextNLP_Usage_Example",
        "documentation": {}
    },
    {
        "label": "EMSAgent",
        "kind": 6,
        "importPath": "Demo.EMSAgent.EMSAgentSystem",
        "description": "Demo.EMSAgent.EMSAgentSystem",
        "peekOfCode": "class EMSAgent(nn.Module):\n    def __init__(self, config, date):\n        super(EMSAgent, self).__init__()\n        self.config = config\n        self.tokenizer = BertTokenizer.from_pretrained(self.config.backbone, do_lower_Case=True)\n        self.clean_model_date = date\n        self.save_model_root = os.path.join('EMSAgent/models', '{}'.format(self.clean_model_date))\n        if self.config.graph == 'hetero':\n            signs_df = pd.read_excel('./EMSAgent/config_file/All Protocols Mapping.xlsx')\n            impre_df = pd.read_excel('./EMSAgent/config_file/Impression Protocol.xlsx')",
        "detail": "Demo.EMSAgent.EMSAgentSystem",
        "documentation": {}
    },
    {
        "label": "add_new_parts",
        "kind": 2,
        "importPath": "Demo.EMSAgent.EMSAgentSystem",
        "description": "Demo.EMSAgent.EMSAgentSystem",
        "peekOfCode": "def add_new_parts(old_string, new_string):\n    # Split the strings into words\n    words1 = old_string.split()\n    words2 = new_string.split()\n    # Find new words in string2\n    new_words = [word for word in words2 if word not in words1]\n    # Update string1 with new words\n    old_string += \" \" + \" \".join(new_words)\n    return old_string\ndef EMSAgentSystem(SpeechToNLPQueue, FeedbackQueue):",
        "detail": "Demo.EMSAgent.EMSAgentSystem",
        "documentation": {}
    },
    {
        "label": "EMSAgentSystem",
        "kind": 2,
        "importPath": "Demo.EMSAgent.EMSAgentSystem",
        "description": "Demo.EMSAgent.EMSAgentSystem",
        "peekOfCode": "def EMSAgentSystem(SpeechToNLPQueue, FeedbackQueue):\n    # ProtocolSignal.signal.connect(Window.UpdateProtocolBoxes)\n    # initialize\n    seed_everything(3407)\n    from EMSAgent.default_sets import model_name\n    config = {\n        'max_len': 512,\n        'fusion': None,\n        'cls': 'fc'\n    }",
        "detail": "Demo.EMSAgent.EMSAgentSystem",
        "documentation": {}
    },
    {
        "label": "bertEmbedding",
        "kind": 6,
        "importPath": "Demo.EMSAgent.Heterogeneous_graph",
        "description": "Demo.EMSAgent.Heterogeneous_graph",
        "peekOfCode": "class bertEmbedding():\n    def __init__(self, tokenizer, model):\n        self.tokenizer = tokenizer\n        self.model = model\n        if self.tokenizer.pad_token is None:\n            self.tokenizer.pad_token = tokenizer.eos_token\n        if dataset == 'EMS':\n            self.max_len = 256\n        elif dataset == 'MIMIC3':\n            self.max_len = 32",
        "detail": "Demo.EMSAgent.Heterogeneous_graph",
        "documentation": {}
    },
    {
        "label": "HeteroGraph",
        "kind": 6,
        "importPath": "Demo.EMSAgent.Heterogeneous_graph",
        "description": "Demo.EMSAgent.Heterogeneous_graph",
        "peekOfCode": "class HeteroGraph(nn.Module):\n    def __init__(self, backbone, mode):\n        super(HeteroGraph, self).__init__()\n        if dataset == 'EMS':\n            if mode == 'group':\n                self.p_node = p_node\n                self.group_p_dict = group_p_dict  # [p2hier, group_p_dict]\n            elif mode == 'ungroup':\n                self.p_node = ungroup_p_node\n                self.group_p_dict = None",
        "detail": "Demo.EMSAgent.Heterogeneous_graph",
        "documentation": {}
    },
    {
        "label": "CHUNKModel",
        "kind": 6,
        "importPath": "Demo.EMSAgent.chunk_test",
        "description": "Demo.EMSAgent.chunk_test",
        "peekOfCode": "class CHUNKModel(nn.Module):\n    def __init__(self, config, date):\n        super(CHUNKModel, self).__init__()\n        self.config = config\n        self.p_node = p_node\n        self.tokenizer = BertTokenizer.from_pretrained(self.config.backbone, do_lower_Case=True)\n        self.clean_model_date = date\n        self.save_model_root = os.path.join('./models', '{}'.format(self.clean_model_date),\n                                            'iter:{}_lr:{}_bs:{}_epoch:{}_lossfn:{}_backbone:{}_graph:{}_cluster:{}_cls:{}_attn:{}'.format(\n                                                0, self.config.learning_rate, self.config.batch_size,",
        "detail": "Demo.EMSAgent.chunk_test",
        "documentation": {}
    },
    {
        "label": "chunkPipeline",
        "kind": 2,
        "importPath": "Demo.EMSAgent.chunk_test",
        "description": "Demo.EMSAgent.chunk_test",
        "peekOfCode": "def chunkPipeline(config, date):\n    haydon = pd.read_excel('/home/xueren/Desktop/EMS/data/EMS/haydon chunks.xlsx', sheet_name=None)\n    model = CHUNKModel(config, date)\n    all_preds = []\n    all_probs = []\n    for name, sheet in haydon.items():\n        prediction = []\n        prediction_prob = []\n        label = sheet['Protocols'][0].strip().lower()\n        for i in range(len(sheet)):",
        "detail": "Demo.EMSAgent.chunk_test",
        "documentation": {}
    },
    {
        "label": "seed_everything",
        "kind": 2,
        "importPath": "Demo.EMSAgent.default_sets",
        "description": "Demo.EMSAgent.default_sets",
        "peekOfCode": "def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ[\"WANDB__SERVICE_WAIT\"] = \"300\"\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    # Some cudnn methods can be random even after fixing the seed\n    # unless you tell it to be deterministic",
        "detail": "Demo.EMSAgent.default_sets",
        "documentation": {}
    },
    {
        "label": "os.environ[\"TOKENIZERS_PARALLELISM\"]",
        "kind": 5,
        "importPath": "Demo.EMSAgent.default_sets",
        "description": "Demo.EMSAgent.default_sets",
        "peekOfCode": "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nimport torch\nimport numpy as np\nimport json\nimport torch.multiprocessing\nimport pipeline_config\ntorch.multiprocessing.set_sharing_strategy('file_system')\nmodel_name = pipeline_config.protocol_model_type\ntask = 'multi_label'\ndataset = 'EMS' #EMS, MIMIC3",
        "detail": "Demo.EMSAgent.default_sets",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "Demo.EMSAgent.default_sets",
        "description": "Demo.EMSAgent.default_sets",
        "peekOfCode": "model_name = pipeline_config.protocol_model_type\ntask = 'multi_label'\ndataset = 'EMS' #EMS, MIMIC3\nmulti_graph = False #if use multi graph (KAMG)\ndevice = pipeline_config.protocol_model_device\nSAVE_RESULT_ROOT = os.path.dirname(os.path.realpath(__file__))\n# SAVE_RESULT_ROOT = '/gpfs/gpfs0/scratch/zar8jw'\nif dataset == 'EMS':\n    EMS_DIR = None\n    # EMS_DIR = '/sfs/qumulo/qhome/zar8jw/EMS/data/EMS/multi_label'",
        "detail": "Demo.EMSAgent.default_sets",
        "documentation": {}
    },
    {
        "label": "task",
        "kind": 5,
        "importPath": "Demo.EMSAgent.default_sets",
        "description": "Demo.EMSAgent.default_sets",
        "peekOfCode": "task = 'multi_label'\ndataset = 'EMS' #EMS, MIMIC3\nmulti_graph = False #if use multi graph (KAMG)\ndevice = pipeline_config.protocol_model_device\nSAVE_RESULT_ROOT = os.path.dirname(os.path.realpath(__file__))\n# SAVE_RESULT_ROOT = '/gpfs/gpfs0/scratch/zar8jw'\nif dataset == 'EMS':\n    EMS_DIR = None\n    # EMS_DIR = '/sfs/qumulo/qhome/zar8jw/EMS/data/EMS/multi_label'\n    p_node = [",
        "detail": "Demo.EMSAgent.default_sets",
        "documentation": {}
    },
    {
        "label": "dataset",
        "kind": 5,
        "importPath": "Demo.EMSAgent.default_sets",
        "description": "Demo.EMSAgent.default_sets",
        "peekOfCode": "dataset = 'EMS' #EMS, MIMIC3\nmulti_graph = False #if use multi graph (KAMG)\ndevice = pipeline_config.protocol_model_device\nSAVE_RESULT_ROOT = os.path.dirname(os.path.realpath(__file__))\n# SAVE_RESULT_ROOT = '/gpfs/gpfs0/scratch/zar8jw'\nif dataset == 'EMS':\n    EMS_DIR = None\n    # EMS_DIR = '/sfs/qumulo/qhome/zar8jw/EMS/data/EMS/multi_label'\n    p_node = [\n        'airway - failed (protocol 3 - 16)',",
        "detail": "Demo.EMSAgent.default_sets",
        "documentation": {}
    },
    {
        "label": "multi_graph",
        "kind": 5,
        "importPath": "Demo.EMSAgent.default_sets",
        "description": "Demo.EMSAgent.default_sets",
        "peekOfCode": "multi_graph = False #if use multi graph (KAMG)\ndevice = pipeline_config.protocol_model_device\nSAVE_RESULT_ROOT = os.path.dirname(os.path.realpath(__file__))\n# SAVE_RESULT_ROOT = '/gpfs/gpfs0/scratch/zar8jw'\nif dataset == 'EMS':\n    EMS_DIR = None\n    # EMS_DIR = '/sfs/qumulo/qhome/zar8jw/EMS/data/EMS/multi_label'\n    p_node = [\n        'airway - failed (protocol 3 - 16)',\n        'environmental - heat exposure/heat exhaustion environmental - heat stroke (protocol 5 - 2)',",
        "detail": "Demo.EMSAgent.default_sets",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "Demo.EMSAgent.default_sets",
        "description": "Demo.EMSAgent.default_sets",
        "peekOfCode": "device = pipeline_config.protocol_model_device\nSAVE_RESULT_ROOT = os.path.dirname(os.path.realpath(__file__))\n# SAVE_RESULT_ROOT = '/gpfs/gpfs0/scratch/zar8jw'\nif dataset == 'EMS':\n    EMS_DIR = None\n    # EMS_DIR = '/sfs/qumulo/qhome/zar8jw/EMS/data/EMS/multi_label'\n    p_node = [\n        'airway - failed (protocol 3 - 16)',\n        'environmental - heat exposure/heat exhaustion environmental - heat stroke (protocol 5 - 2)',\n        'environmental - hypothermia (protocol 5 - 1)',",
        "detail": "Demo.EMSAgent.default_sets",
        "documentation": {}
    },
    {
        "label": "SAVE_RESULT_ROOT",
        "kind": 5,
        "importPath": "Demo.EMSAgent.default_sets",
        "description": "Demo.EMSAgent.default_sets",
        "peekOfCode": "SAVE_RESULT_ROOT = os.path.dirname(os.path.realpath(__file__))\n# SAVE_RESULT_ROOT = '/gpfs/gpfs0/scratch/zar8jw'\nif dataset == 'EMS':\n    EMS_DIR = None\n    # EMS_DIR = '/sfs/qumulo/qhome/zar8jw/EMS/data/EMS/multi_label'\n    p_node = [\n        'airway - failed (protocol 3 - 16)',\n        'environmental - heat exposure/heat exhaustion environmental - heat stroke (protocol 5 - 2)',\n        'environmental - hypothermia (protocol 5 - 1)',\n        'general - behavioral/patient restraint (protocol 3 - 4)',",
        "detail": "Demo.EMSAgent.default_sets",
        "documentation": {}
    },
    {
        "label": "GAT",
        "kind": 6,
        "importPath": "Demo.EMSAgent.model",
        "description": "Demo.EMSAgent.model",
        "peekOfCode": "class GAT(torch.nn.Module):\n    def __init__(self, hidden_channels, out_channels, num_heads):\n        super(GAT, self).__init__()\n        self.conv1 = GATConv(-1, hidden_channels, heads=num_heads, dropout=0.3)\n        self.conv2 = GATConv(hidden_channels * num_heads, out_channels, heads=1, concat=False, dropout=0.3)\n    def forward(self, data):\n        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_weight\n        x = F.dropout(x, p=0.3, training=self.training)\n        x = F.elu(self.conv1(x, edge_index, edge_attr))\n        x = F.dropout(x, p=0.3, training=self.training)",
        "detail": "Demo.EMSAgent.model",
        "documentation": {}
    },
    {
        "label": "GCN",
        "kind": 6,
        "importPath": "Demo.EMSAgent.model",
        "description": "Demo.EMSAgent.model",
        "peekOfCode": "class GCN(torch.nn.Module):\n    def __init__(self, hidden_channels, out_channels):\n        super().__init__()\n        self.conv1 = GCNConv(-1, hidden_channels)\n        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n        self.lin = Linear(hidden_channels, out_channels)\n    def forward(self, data):\n        x, edge_index, edge_weight = data.x, data.edge_index, data.edge_attr\n        x = self.conv1(x, edge_index, edge_weight)\n        x = F.relu(x)",
        "detail": "Demo.EMSAgent.model",
        "documentation": {}
    },
    {
        "label": "HGT",
        "kind": 6,
        "importPath": "Demo.EMSAgent.model",
        "description": "Demo.EMSAgent.model",
        "peekOfCode": "class HGT(nn.Module):\n    def __init__(self, hidden_channels, out_channels, num_heads, num_layers):\n        super().__init__()\n        self.lin_dict = torch.nn.ModuleDict()\n        node_types = ['protocol', 'impression', 'medication', 'procedure']\n        for node_type in node_types:\n            self.lin_dict[node_type] = Linear(-1, hidden_channels)\n        self.convs = torch.nn.ModuleList()\n        meta_data = (['protocol', 'impression', 'medication', 'procedure'],\n                     [('protocol', 'has', 'impression'),",
        "detail": "Demo.EMSAgent.model",
        "documentation": {}
    },
    {
        "label": "EMSMultiModel",
        "kind": 6,
        "importPath": "Demo.EMSAgent.model",
        "description": "Demo.EMSAgent.model",
        "peekOfCode": "class EMSMultiModel(nn.Module):\n    def __init__(self, backbone, max_len, attn, cluster, fusion, cls, graph_type):\n        super(EMSMultiModel, self).__init__()\n        self.backbone = backbone\n        if self.backbone == 'stanford-crfm/BioMedLM':\n            self.hidden_size = 2560\n        elif self.backbone in ['UFNLP/gatortron-base', 'microsoft/biogpt']:\n            self.hidden_size = 1024\n        elif self.backbone in ['google/mobilebert-uncased', 'nlpie/clinical-mobilebert', 'nlpie/bio-mobilebert']:\n            self.hidden_size = 512",
        "detail": "Demo.EMSAgent.model",
        "documentation": {}
    },
    {
        "label": "AttrDict",
        "kind": 6,
        "importPath": "Demo.EMSAgent.utils",
        "description": "Demo.EMSAgent.utils",
        "peekOfCode": "class AttrDict(dict):\n    def __getattr__(self, attr):\n        return self[attr]\n    def __setattr__(self, attr, value):\n        self[attr] = value\ndef convert_label(labels, ages, logits=None):\n    '''\n    labels: [[ohe], [ohe], [ohe]]\n    logits: [[logit], [logit], [logit]]\n    ages: [tensor, tensor, tensor]",
        "detail": "Demo.EMSAgent.utils",
        "documentation": {}
    },
    {
        "label": "convert_label",
        "kind": 2,
        "importPath": "Demo.EMSAgent.utils",
        "description": "Demo.EMSAgent.utils",
        "peekOfCode": "def convert_label(labels, ages, logits=None):\n    '''\n    labels: [[ohe], [ohe], [ohe]]\n    logits: [[logit], [logit], [logit]]\n    ages: [tensor, tensor, tensor]\n    convert clustered labels to exact labels\n    p_node contains protocol names (the sequence order the same with labels).\n    '''\n    encodings = []\n    encoding_logits = []",
        "detail": "Demo.EMSAgent.utils",
        "documentation": {}
    },
    {
        "label": "cnt_instance_per_label",
        "kind": 2,
        "importPath": "Demo.EMSAgent.utils",
        "description": "Demo.EMSAgent.utils",
        "peekOfCode": "def cnt_instance_per_label(df):\n    label_cnt = {}\n    if dataset == 'EMS':\n        column_name = 'Ungrouped Protocols'\n    elif dataset == 'MIMIC3':\n        column_name = 'ICD9_DIAG'\n    else:\n        raise Exception('check dataset in default_sets.py')\n    for i in range(len(df)):\n        if type(df[column_name][i]) == float:",
        "detail": "Demo.EMSAgent.utils",
        "documentation": {}
    },
    {
        "label": "convertListToStr",
        "kind": 2,
        "importPath": "Demo.EMSAgent.utils",
        "description": "Demo.EMSAgent.utils",
        "peekOfCode": "def convertListToStr(l):\n    s = \"\"\n    if len(l) == 0: return s\n    for each in l:\n        s += str(each) + ';'\n    return s[:-1]\ndef checkOnehot(X):\n    for x in X:\n        if x != 0 and x != 1:\n            return False",
        "detail": "Demo.EMSAgent.utils",
        "documentation": {}
    },
    {
        "label": "checkOnehot",
        "kind": 2,
        "importPath": "Demo.EMSAgent.utils",
        "description": "Demo.EMSAgent.utils",
        "peekOfCode": "def checkOnehot(X):\n    for x in X:\n        if x != 0 and x != 1:\n            return False\n    return True\ndef sortby(p2tfidf):\n    for p, values in p2tfidf.items():\n        od = OrderedDict(sorted(values.items(), key=lambda x:x[1], reverse=True))\n        p2tfidf[p] = od\n    return p2tfidf",
        "detail": "Demo.EMSAgent.utils",
        "documentation": {}
    },
    {
        "label": "sortby",
        "kind": 2,
        "importPath": "Demo.EMSAgent.utils",
        "description": "Demo.EMSAgent.utils",
        "peekOfCode": "def sortby(p2tfidf):\n    for p, values in p2tfidf.items():\n        od = OrderedDict(sorted(values.items(), key=lambda x:x[1], reverse=True))\n        p2tfidf[p] = od\n    return p2tfidf\ndef ungroup(age, protocols):\n    ungroup_protocols = []\n    for p in protocols.split(';'):\n        p = p.strip().lower()\n        if p in reverse_group_p_dict:",
        "detail": "Demo.EMSAgent.utils",
        "documentation": {}
    },
    {
        "label": "ungroup",
        "kind": 2,
        "importPath": "Demo.EMSAgent.utils",
        "description": "Demo.EMSAgent.utils",
        "peekOfCode": "def ungroup(age, protocols):\n    ungroup_protocols = []\n    for p in protocols.split(';'):\n        p = p.strip().lower()\n        if p in reverse_group_p_dict:\n            if int(age) <= 21:\n                ungroup_protocols.append(reverse_group_p_dict[p][1])\n            else:\n                ungroup_protocols.append(reverse_group_p_dict[p][0])\n        else:",
        "detail": "Demo.EMSAgent.utils",
        "documentation": {}
    },
    {
        "label": "p2onehot",
        "kind": 2,
        "importPath": "Demo.EMSAgent.utils",
        "description": "Demo.EMSAgent.utils",
        "peekOfCode": "def p2onehot(ps, ref_list):\n    '''\n    :param ps: list of protocols, e.g.: ['medical - altered mental status (protocol 3 - 15)']\n    :param ref_list: p_node, ungroup_p_node\n    :return: onehot encoding\n    '''\n    ohe = [0] * len(ref_list)\n    for p in ps:\n        idx = ref_list.index(p)\n        ohe[idx] = 1",
        "detail": "Demo.EMSAgent.utils",
        "documentation": {}
    },
    {
        "label": "onehot2p",
        "kind": 2,
        "importPath": "Demo.EMSAgent.utils",
        "description": "Demo.EMSAgent.utils",
        "peekOfCode": "def onehot2p(onehot):\n    pred = []\n    for i in range(len(onehot)):\n        if onehot[i] == 1:\n            p_name = p_node[i]\n            pred.append(p_name)\n    return convertListToStr(pred)\ndef removePunctuation(sentence):\n    sentence = re.sub(r'[?|!|\\'|\"|;|:|#|&|-]', r' ', sentence)\n    sentence = re.sub(r'[.|,|)|(|\\|/|_|~|<|>]', r' ', sentence)",
        "detail": "Demo.EMSAgent.utils",
        "documentation": {}
    },
    {
        "label": "removePunctuation",
        "kind": 2,
        "importPath": "Demo.EMSAgent.utils",
        "description": "Demo.EMSAgent.utils",
        "peekOfCode": "def removePunctuation(sentence):\n    sentence = re.sub(r'[?|!|\\'|\"|;|:|#|&|-]', r' ', sentence)\n    sentence = re.sub(r'[.|,|)|(|\\|/|_|~|<|>]', r' ', sentence)\n    sentence = re.sub(r\"[\\([{})\\]]\", r' ', sentence)\n    sentence = re.sub(r\"[*]\", r' ', sentence)\n    sentence = re.sub(r\"[%]\", r' percentage', sentence)\n    sentence = sentence.strip()\n    sentence = sentence.replace(\"\\n\", \" \")\n    return sentence\ndef text_remove_double_space(text):",
        "detail": "Demo.EMSAgent.utils",
        "documentation": {}
    },
    {
        "label": "text_remove_double_space",
        "kind": 2,
        "importPath": "Demo.EMSAgent.utils",
        "description": "Demo.EMSAgent.utils",
        "peekOfCode": "def text_remove_double_space(text):\n    text = text.lower()\n    res = ''\n    for word in text.split():\n        res = res + word + ' '\n    return res.strip()\n'''\nThe following codes from https://github.com/MemoriesJ/KAMG/blob/6618c6633bbe40de7447d5ae7338784b5233aa6a/NeuralNLP-NeuralClassifier-KAMG/evaluate/classification_evaluate.py\n'''\ndef ranking_precision_score(y_true, y_score, k=10):",
        "detail": "Demo.EMSAgent.utils",
        "documentation": {}
    },
    {
        "label": "ranking_precision_score",
        "kind": 2,
        "importPath": "Demo.EMSAgent.utils",
        "description": "Demo.EMSAgent.utils",
        "peekOfCode": "def ranking_precision_score(y_true, y_score, k=10):\n    \"\"\"Precision at rank k\n    Parameters\n    ----------\n    y_true : array-like, shape = [n_samples]\n        Ground truth (true relevance labels).\n    y_score : array-like, shape = [n_samples]\n        Predicted scores.\n    k : int\n        Rank.",
        "detail": "Demo.EMSAgent.utils",
        "documentation": {}
    },
    {
        "label": "get_precision_at_k",
        "kind": 2,
        "importPath": "Demo.EMSAgent.utils",
        "description": "Demo.EMSAgent.utils",
        "peekOfCode": "def get_precision_at_k(y_true, y_score, k=10):\n    \"\"\"Mean precision at rank k\n    Parameters\n    ----------\n    y_true : array-like, shape = [n_samples]\n        Ground truth (true relevance labels).\n    y_score : array-like, shape = [n_samples]\n        Predicted scores.\n    k : int\n        Rank.",
        "detail": "Demo.EMSAgent.utils",
        "documentation": {}
    },
    {
        "label": "ranking_recall_score",
        "kind": 2,
        "importPath": "Demo.EMSAgent.utils",
        "description": "Demo.EMSAgent.utils",
        "peekOfCode": "def ranking_recall_score(y_true, y_score, k=10):\n    # https://ils.unc.edu/courses/2013_spring/inls509_001/lectures/10-EvaluationMetrics.pdf\n    \"\"\"Recall at rank k\n    Parameters\n    ----------\n    y_true : array-like, shape = [n_samples]\n        Ground truth (true relevance labels).\n    y_score : array-like, shape = [n_samples]\n        Predicted scores.\n    k : int",
        "detail": "Demo.EMSAgent.utils",
        "documentation": {}
    },
    {
        "label": "get_recall_at_k",
        "kind": 2,
        "importPath": "Demo.EMSAgent.utils",
        "description": "Demo.EMSAgent.utils",
        "peekOfCode": "def get_recall_at_k(y_true, y_score, k=10):\n    \"\"\"Mean recall at rank k\n    Parameters\n    ----------\n    y_true : array-like, shape = [n_samples]\n        Ground truth (true relevance labels).\n    y_score : array-like, shape = [n_samples]\n        Predicted scores.\n    k : int\n        Rank.",
        "detail": "Demo.EMSAgent.utils",
        "documentation": {}
    },
    {
        "label": "ranking_rprecision_score",
        "kind": 2,
        "importPath": "Demo.EMSAgent.utils",
        "description": "Demo.EMSAgent.utils",
        "peekOfCode": "def ranking_rprecision_score(y_true, y_score, k=10):\n    \"\"\"Precision at rank k\n    Parameters\n    ----------\n    y_true : array-like, shape = [n_samples]\n        Ground truth (true relevance labels).\n    y_score : array-like, shape = [n_samples]\n        Predicted scores.\n    k : int\n        Rank.",
        "detail": "Demo.EMSAgent.utils",
        "documentation": {}
    },
    {
        "label": "get_r_precision_at_k",
        "kind": 2,
        "importPath": "Demo.EMSAgent.utils",
        "description": "Demo.EMSAgent.utils",
        "peekOfCode": "def get_r_precision_at_k(y_true, y_score, k=10):\n    \"\"\"Mean precision at rank k\n    Parameters\n    ----------\n    y_true : array-like, shape = [n_samples]\n        Ground truth (true relevance labels).\n    y_score : array-like, shape = [n_samples]\n        Predicted scores.\n    k : int\n        Rank.",
        "detail": "Demo.EMSAgent.utils",
        "documentation": {}
    },
    {
        "label": "dcg_score",
        "kind": 2,
        "importPath": "Demo.EMSAgent.utils",
        "description": "Demo.EMSAgent.utils",
        "peekOfCode": "def dcg_score(y_true, y_score, k=10, gains=\"exponential\"):\n    \"\"\"Discounted cumulative gain (DCG) at rank k\n    Parameters\n    ----------\n    y_true : array-like, shape = [n_samples]\n        Ground truth (true relevance labels).\n    y_score : array-like, shape = [n_samples]\n        Predicted scores.\n    k : int\n        Rank.",
        "detail": "Demo.EMSAgent.utils",
        "documentation": {}
    },
    {
        "label": "ndcg_score",
        "kind": 2,
        "importPath": "Demo.EMSAgent.utils",
        "description": "Demo.EMSAgent.utils",
        "peekOfCode": "def ndcg_score(y_true, y_score, k=10, gains=\"exponential\"):\n    \"\"\"Normalized discounted cumulative gain (NDCG) at rank k\n    Parameters\n    ----------\n    y_true : array-like, shape = [n_samples]\n        Ground truth (true relevance labels).\n    y_score : array-like, shape = [n_samples]\n        Predicted scores.\n    k : int\n        Rank.",
        "detail": "Demo.EMSAgent.utils",
        "documentation": {}
    },
    {
        "label": "get_ndcg_at_k",
        "kind": 2,
        "importPath": "Demo.EMSAgent.utils",
        "description": "Demo.EMSAgent.utils",
        "peekOfCode": "def get_ndcg_at_k(y_true, y_predict_score, k, gains=\"exponential\"):\n    \"\"\"Normalized discounted cumulative gain (NDCG) at rank k\n        Parameters\n        ----------\n        y_true : array-like, shape = [n_samples]\n            Ground truth (true relevance labels).\n        y_predict_score : array-like, shape = [n_samples]\n            Predicted scores.\n        k : int\n            Rank.",
        "detail": "Demo.EMSAgent.utils",
        "documentation": {}
    },
    {
        "label": "AugmentationMethod",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.augmentations.methods.base_method",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.augmentations.methods.base_method",
        "peekOfCode": "class AugmentationMethod:\n    @tf.function\n    def augment(self, *args, **kwargs):\n        raise NotImplementedError()",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.augmentations.methods.base_method",
        "documentation": {}
    },
    {
        "label": "FreqMasking",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.augmentations.methods.specaugment",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.augmentations.methods.specaugment",
        "peekOfCode": "class FreqMasking(AugmentationMethod):\n    def __init__(self, num_masks: int = 1, mask_factor: float = 27):\n        self.num_masks = num_masks\n        self.mask_factor = mask_factor\n    @tf.function\n    def augment(self, spectrogram: tf.Tensor):\n        \"\"\"\n        Masking the frequency channels (shape[1])\n        Args:\n            spectrogram: shape (T, num_feature_bins, V)",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.augmentations.methods.specaugment",
        "documentation": {}
    },
    {
        "label": "TimeMasking",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.augmentations.methods.specaugment",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.augmentations.methods.specaugment",
        "peekOfCode": "class TimeMasking(AugmentationMethod):\n    def __init__(self, num_masks: int = 1, mask_factor: float = 100, p_upperbound: float = 1.0):\n        self.num_masks = num_masks\n        self.mask_factor = mask_factor\n        self.p_upperbound = p_upperbound\n    @tf.function\n    def augment(self, spectrogram: tf.Tensor):\n        \"\"\"\n        Masking the time channel (shape[0])\n        Args:",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.augmentations.methods.specaugment",
        "documentation": {}
    },
    {
        "label": "Augmentation",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.augmentations.augmentation",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.augmentations.augmentation",
        "peekOfCode": "class Augmentation:\n    def __init__(self, config: dict = None):\n        if not config:\n            config = {}\n#        print(\"config in augmentation: %s\" % config)\n        self.prob = float(config.pop(\"prob\", 0.5))\n        self.signal_augmentations = self.parse(config.pop(\"signal_augment\", {}))\n        self.feature_augmentations = self.parse(config.pop(\"feature_augment\", {}))\n    def _augment(self, inputs, augmentations):\n        outputs = inputs",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.augmentations.augmentation",
        "documentation": {}
    },
    {
        "label": "AUGMENTATIONS",
        "kind": 5,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.augmentations.augmentation",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.augmentations.augmentation",
        "peekOfCode": "AUGMENTATIONS = {\n    \"freq_masking\": specaugment.FreqMasking,\n    \"time_masking\": specaugment.TimeMasking,\n}\nclass Augmentation:\n    def __init__(self, config: dict = None):\n        if not config:\n            config = {}\n#        print(\"config in augmentation: %s\" % config)\n        self.prob = float(config.pop(\"prob\", 0.5))",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.augmentations.augmentation",
        "documentation": {}
    },
    {
        "label": "DecoderConfig",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.configs.config",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.configs.config",
        "peekOfCode": "class DecoderConfig:\n    def __init__(self, config: dict = None):\n        if not config:\n            config = {}\n        self.beam_width = config.pop(\"beam_width\", 0)\n        self.blank_at_zero = config.pop(\"blank_at_zero\", True)\n        self.norm_score = config.pop(\"norm_score\", True)\n        self.lm_config = config.pop(\"lm_config\", {})\n        self.vocabulary = file_util.preprocess_paths(config.pop(\"vocabulary\", None))\n        self.target_vocab_size = config.pop(\"target_vocab_size\", 1024)",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.configs.config",
        "documentation": {}
    },
    {
        "label": "DatasetConfig",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.configs.config",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.configs.config",
        "peekOfCode": "class DatasetConfig:\n    def __init__(self, config: dict = None):\n        if not config:\n            config = {}\n        self.stage = config.pop(\"stage\", None)\n        self.data_paths = file_util.preprocess_paths(config.pop(\"data_paths\", None))\n        self.tfrecords_dir = file_util.preprocess_paths(config.pop(\"tfrecords_dir\", None), isdir=True)\n        self.tfrecords_shards = config.pop(\"tfrecords_shards\", 16)\n        self.shuffle = config.pop(\"shuffle\", False)\n        self.cache = config.pop(\"cache\", False)",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.configs.config",
        "documentation": {}
    },
    {
        "label": "RunningConfig",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.configs.config",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.configs.config",
        "peekOfCode": "class RunningConfig:\n    def __init__(self, config: dict = None):\n        if not config:\n            config = {}\n        self.batch_size = config.pop(\"batch_size\", 1)\n        self.accumulation_steps = config.pop(\"accumulation_steps\", 1)\n        self.num_epochs = config.pop(\"num_epochs\", 20)\n        for k, v in config.items():\n            setattr(self, k, v)\n            if k == \"checkpoint\":",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.configs.config",
        "documentation": {}
    },
    {
        "label": "LearningConfig",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.configs.config",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.configs.config",
        "peekOfCode": "class LearningConfig:\n    def __init__(self, config: dict = None):\n        if not config:\n            config = {}\n        self.train_dataset_config = DatasetConfig(config.pop(\"train_dataset_config\", {}))\n        self.eval_dataset_config = DatasetConfig(config.pop(\"eval_dataset_config\", {}))\n        self.test_dataset_config = DatasetConfig(config.pop(\"test_dataset_config\", {}))\n        self.optimizer_config = config.pop(\"optimizer_config\", {})\n        self.running_config = RunningConfig(config.pop(\"running_config\", {}))\n        for k, v in config.items():",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.configs.config",
        "documentation": {}
    },
    {
        "label": "Config",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.configs.config",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.configs.config",
        "peekOfCode": "class Config:\n    \"\"\"User config class for training, testing or infering\"\"\"\n    def __init__(self, data: Union[str, dict]):\n        config = data if isinstance(data, dict) else file_util.load_yaml(file_util.preprocess_paths(data))\n        self.speech_config = config.pop(\"speech_config\", {})\n        self.decoder_config = config.pop(\"decoder_config\", {})\n        self.model_config = config.pop(\"model_config\", {})\n        self.learning_config = LearningConfig(config.pop(\"learning_config\", {}))\n#        print(\"Config: %s\" % config.items())\n        for k, v in config.items():",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.configs.config",
        "documentation": {}
    },
    {
        "label": "ASRDataset",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.datasets.asr_dataset",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.datasets.asr_dataset",
        "peekOfCode": "class ASRDataset(BaseDataset):\n    \"\"\"Dataset for ASR using Generator\"\"\"\n    def __init__(\n        self,\n        stage: str,\n        speech_featurizer: SpeechFeaturizer,\n        text_featurizer: TextFeaturizer,\n        data_paths: list,\n        augmentations: Augmentation = Augmentation(None),\n        cache: bool = False,",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.datasets.asr_dataset",
        "documentation": {}
    },
    {
        "label": "ASRTFRecordDataset",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.datasets.asr_dataset",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.datasets.asr_dataset",
        "peekOfCode": "class ASRTFRecordDataset(ASRDataset):\n    \"\"\"Dataset for ASR using TFRecords\"\"\"\n    def __init__(\n        self,\n        data_paths: list,\n        tfrecords_dir: str,\n        speech_featurizer: SpeechFeaturizer,\n        text_featurizer: TextFeaturizer,\n        stage: str,\n        augmentations: Augmentation = Augmentation(None),",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.datasets.asr_dataset",
        "documentation": {}
    },
    {
        "label": "ASRSliceDataset",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.datasets.asr_dataset",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.datasets.asr_dataset",
        "peekOfCode": "class ASRSliceDataset(ASRDataset):\n    \"\"\"Dataset for ASR using Slice\"\"\"\n    @staticmethod\n    def load(record: tf.Tensor):\n        def fn(path: bytes):\n            return load_and_convert_to_wav(path.decode(\"utf-8\")).numpy()\n        audio = tf.numpy_function(fn, inp=[record[0]], Tout=tf.string)\n        return record[0], audio, record[2]\n    def create(self, batch_size: int):\n        self.read_entries()",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.datasets.asr_dataset",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.datasets.asr_dataset",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.datasets.asr_dataset",
        "peekOfCode": "logger = tf.get_logger()\nclass ASRDataset(BaseDataset):\n    \"\"\"Dataset for ASR using Generator\"\"\"\n    def __init__(\n        self,\n        stage: str,\n        speech_featurizer: SpeechFeaturizer,\n        text_featurizer: TextFeaturizer,\n        data_paths: list,\n        augmentations: Augmentation = Augmentation(None),",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.datasets.asr_dataset",
        "documentation": {}
    },
    {
        "label": "BaseDataset",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.datasets.base_dataset",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.datasets.base_dataset",
        "peekOfCode": "class BaseDataset(metaclass=abc.ABCMeta):\n    \"\"\"Based dataset for all models\"\"\"\n    def __init__(\n        self,\n        data_paths: list,\n        augmentations: Augmentation = Augmentation(None),\n        cache: bool = False,\n        shuffle: bool = False,\n        buffer_size: int = BUFFER_SIZE,\n        indefinite: bool = False,",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.datasets.base_dataset",
        "documentation": {}
    },
    {
        "label": "BUFFER_SIZE",
        "kind": 5,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.datasets.base_dataset",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.datasets.base_dataset",
        "peekOfCode": "BUFFER_SIZE = 100\nTFRECORD_SHARDS = 16\nAUTOTUNE = tf.data.experimental.AUTOTUNE\nclass BaseDataset(metaclass=abc.ABCMeta):\n    \"\"\"Based dataset for all models\"\"\"\n    def __init__(\n        self,\n        data_paths: list,\n        augmentations: Augmentation = Augmentation(None),\n        cache: bool = False,",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.datasets.base_dataset",
        "documentation": {}
    },
    {
        "label": "TFRECORD_SHARDS",
        "kind": 5,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.datasets.base_dataset",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.datasets.base_dataset",
        "peekOfCode": "TFRECORD_SHARDS = 16\nAUTOTUNE = tf.data.experimental.AUTOTUNE\nclass BaseDataset(metaclass=abc.ABCMeta):\n    \"\"\"Based dataset for all models\"\"\"\n    def __init__(\n        self,\n        data_paths: list,\n        augmentations: Augmentation = Augmentation(None),\n        cache: bool = False,\n        shuffle: bool = False,",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.datasets.base_dataset",
        "documentation": {}
    },
    {
        "label": "AUTOTUNE",
        "kind": 5,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.datasets.base_dataset",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.datasets.base_dataset",
        "peekOfCode": "AUTOTUNE = tf.data.experimental.AUTOTUNE\nclass BaseDataset(metaclass=abc.ABCMeta):\n    \"\"\"Based dataset for all models\"\"\"\n    def __init__(\n        self,\n        data_paths: list,\n        augmentations: Augmentation = Augmentation(None),\n        cache: bool = False,\n        shuffle: bool = False,\n        buffer_size: int = BUFFER_SIZE,",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.datasets.base_dataset",
        "documentation": {}
    },
    {
        "label": "fft_weights",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.methods.gammatone",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.methods.gammatone",
        "peekOfCode": "def fft_weights(\n    nfft,\n    fs,\n    nfilts,\n    width,\n    fmin,\n    fmax,\n    maxlen,\n):\n    \"\"\"",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.methods.gammatone",
        "documentation": {}
    },
    {
        "label": "erb_point",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.methods.gammatone",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.methods.gammatone",
        "peekOfCode": "def erb_point(\n    low_freq,\n    high_freq,\n    fraction,\n):\n    \"\"\"\n    Calculates a single point on an ERB scale between ``low_freq`` and\n    ``high_freq``, determined by ``fraction``. When ``fraction`` is ``1``,\n    ``low_freq`` will be returned. When ``fraction`` is ``0``, ``high_freq``\n    will be returned.",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.methods.gammatone",
        "documentation": {}
    },
    {
        "label": "erb_space",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.methods.gammatone",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.methods.gammatone",
        "peekOfCode": "def erb_space(\n    low_freq=DEFAULT_LOW_FREQ,\n    high_freq=DEFAULT_HIGH_FREQ,\n    num=DEFAULT_FILTER_NUM,\n):\n    \"\"\"\n    This function computes an array of ``num`` frequencies uniformly spaced\n    between ``high_freq`` and ``low_freq`` on an ERB scale.\n    For a definition of ERB, see Moore, B. C. J., and Glasberg, B. R. (1983).\n    \"Suggested formulae for calculating auditory-filter bandwidths and",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.methods.gammatone",
        "documentation": {}
    },
    {
        "label": "make_erb_filters",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.methods.gammatone",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.methods.gammatone",
        "peekOfCode": "def make_erb_filters(\n    fs,\n    centre_freqs,\n    width=1.0,\n):\n    \"\"\"\n    This function computes the filter coefficients for a bank of\n    Gammatone filters. These filters were defined by Patterson and Holdworth for\n    simulating the cochlea.\n    The result is returned as a :class:`ERBCoeffArray`. Each row of the",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.methods.gammatone",
        "documentation": {}
    },
    {
        "label": "pi",
        "kind": 5,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.methods.gammatone",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.methods.gammatone",
        "peekOfCode": "pi = tf.constant(np.pi, dtype=tf.complex64)\nDEFAULT_FILTER_NUM = 100\nDEFAULT_LOW_FREQ = 100\nDEFAULT_HIGH_FREQ = 44100 / 4\ndef fft_weights(\n    nfft,\n    fs,\n    nfilts,\n    width,\n    fmin,",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.methods.gammatone",
        "documentation": {}
    },
    {
        "label": "DEFAULT_FILTER_NUM",
        "kind": 5,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.methods.gammatone",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.methods.gammatone",
        "peekOfCode": "DEFAULT_FILTER_NUM = 100\nDEFAULT_LOW_FREQ = 100\nDEFAULT_HIGH_FREQ = 44100 / 4\ndef fft_weights(\n    nfft,\n    fs,\n    nfilts,\n    width,\n    fmin,\n    fmax,",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.methods.gammatone",
        "documentation": {}
    },
    {
        "label": "DEFAULT_LOW_FREQ",
        "kind": 5,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.methods.gammatone",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.methods.gammatone",
        "peekOfCode": "DEFAULT_LOW_FREQ = 100\nDEFAULT_HIGH_FREQ = 44100 / 4\ndef fft_weights(\n    nfft,\n    fs,\n    nfilts,\n    width,\n    fmin,\n    fmax,\n    maxlen,",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.methods.gammatone",
        "documentation": {}
    },
    {
        "label": "DEFAULT_HIGH_FREQ",
        "kind": 5,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.methods.gammatone",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.methods.gammatone",
        "peekOfCode": "DEFAULT_HIGH_FREQ = 44100 / 4\ndef fft_weights(\n    nfft,\n    fs,\n    nfilts,\n    width,\n    fmin,\n    fmax,\n    maxlen,\n):",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.methods.gammatone",
        "documentation": {}
    },
    {
        "label": "SpeechFeaturizer",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.speech_featurizers",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.speech_featurizers",
        "peekOfCode": "class SpeechFeaturizer(metaclass=abc.ABCMeta):\n    def __init__(\n        self,\n        speech_config: dict,\n    ):\n        \"\"\"\n        We should use TFSpeechFeaturizer for training to avoid differences\n        between tf and librosa when converting to tflite in post-training stage\n        speech_config = {\n            \"sample_rate\": int,",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.speech_featurizers",
        "documentation": {}
    },
    {
        "label": "NumpySpeechFeaturizer",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.speech_featurizers",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.speech_featurizers",
        "peekOfCode": "class NumpySpeechFeaturizer(SpeechFeaturizer):\n    def __init__(self, speech_config: dict):\n        super(NumpySpeechFeaturizer, self).__init__(speech_config)\n        self.delta = speech_config.get(\"delta\", False)\n        self.delta_delta = speech_config.get(\"delta_delta\", False)\n        self.pitch = speech_config.get(\"pitch\", False)\n    @property\n    def shape(self) -> list:\n        # None for time dimension\n        channel_dim = 1",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.speech_featurizers",
        "documentation": {}
    },
    {
        "label": "TFSpeechFeaturizer",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.speech_featurizers",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.speech_featurizers",
        "peekOfCode": "class TFSpeechFeaturizer(SpeechFeaturizer):\n    @property\n    def shape(self) -> list:\n        length = self.max_length if self.max_length > 0 else None\n        return [length, self.num_feature_bins, 1]\n    def stft(\n        self,\n        signal,\n    ):\n        if self.center:",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.speech_featurizers",
        "documentation": {}
    },
    {
        "label": "load_and_convert_to_wav",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.speech_featurizers",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.speech_featurizers",
        "peekOfCode": "def load_and_convert_to_wav(\n    path: str,\n) -> tf.Tensor:\n    wave, rate = librosa.load(os.path.expanduser(path), sr=None, mono=True)\n#    print(wave.shape)\n#    print(len(wave))\n#    print(rate)\n    return tf.audio.encode_wav(tf.expand_dims(wave, axis=-1), sample_rate=rate)\ndef read_raw_audio(\n    audio: Union[str, bytes, np.ndarray],",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.speech_featurizers",
        "documentation": {}
    },
    {
        "label": "read_raw_audio",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.speech_featurizers",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.speech_featurizers",
        "peekOfCode": "def read_raw_audio(\n    audio: Union[str, bytes, np.ndarray],\n    sample_rate=16000,\n) -> np.ndarray:\n    if isinstance(audio, str):\n#        print(\"sample_rate=%s\" % sample_rate)\n        wave, _ = librosa.load(os.path.expanduser(audio), sr=sample_rate, mono=True)\n    elif isinstance(audio, bytes):\n        wave, sr = sf.read(io.BytesIO(audio))\n        if wave.ndim > 1:",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.speech_featurizers",
        "documentation": {}
    },
    {
        "label": "tf_read_raw_audio",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.speech_featurizers",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.speech_featurizers",
        "peekOfCode": "def tf_read_raw_audio(\n    audio: tf.Tensor,\n    sample_rate=16000,\n) -> tf.Tensor:\n    wave, rate = tf.audio.decode_wav(audio, desired_channels=1, desired_samples=-1)\n    if not env_util.has_devices(\"TPU\"):\n        resampled = tfio.audio.resample(wave, rate_in=tf.cast(rate, dtype=tf.int64), rate_out=sample_rate)\n        return tf.reshape(resampled, shape=[-1])  # reshape for using tf.signal\n    return tf.reshape(wave, shape=[-1])  # reshape for using tf.signal\ndef slice_signal(",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.speech_featurizers",
        "documentation": {}
    },
    {
        "label": "slice_signal",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.speech_featurizers",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.speech_featurizers",
        "peekOfCode": "def slice_signal(\n    signal,\n    window_size,\n    stride=0.5,\n) -> np.ndarray:\n    \"\"\"Return windows of the given signal by sweeping in stride fractions of window\"\"\"\n    assert signal.ndim == 1, signal.ndim\n    n_samples = signal.shape[0]\n    offset = int(window_size * stride)\n    slices = []",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.speech_featurizers",
        "documentation": {}
    },
    {
        "label": "tf_merge_slices",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.speech_featurizers",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.speech_featurizers",
        "peekOfCode": "def tf_merge_slices(\n    slices: tf.Tensor,\n) -> tf.Tensor:\n    # slices shape = [batch, window_size]\n    return tf.keras.backend.flatten(slices)  # return shape = [-1, ]\ndef merge_slices(\n    slices: np.ndarray,\n) -> np.ndarray:\n    # slices shape = [batch, window_size]\n    return np.reshape(slices, [-1])",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.speech_featurizers",
        "documentation": {}
    },
    {
        "label": "merge_slices",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.speech_featurizers",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.speech_featurizers",
        "peekOfCode": "def merge_slices(\n    slices: np.ndarray,\n) -> np.ndarray:\n    # slices shape = [batch, window_size]\n    return np.reshape(slices, [-1])\ndef normalize_audio_feature(\n    audio_feature: np.ndarray,\n    per_frame=False,\n) -> np.ndarray:\n    \"\"\"Mean and variance normalization\"\"\"",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.speech_featurizers",
        "documentation": {}
    },
    {
        "label": "normalize_audio_feature",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.speech_featurizers",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.speech_featurizers",
        "peekOfCode": "def normalize_audio_feature(\n    audio_feature: np.ndarray,\n    per_frame=False,\n) -> np.ndarray:\n    \"\"\"Mean and variance normalization\"\"\"\n    axis = 1 if per_frame else None\n    mean = np.mean(audio_feature, axis=axis)\n    std_dev = np.sqrt(np.var(audio_feature, axis=axis) + 1e-9)\n    normalized = (audio_feature - mean) / std_dev\n    return normalized",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.speech_featurizers",
        "documentation": {}
    },
    {
        "label": "tf_normalize_audio_features",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.speech_featurizers",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.speech_featurizers",
        "peekOfCode": "def tf_normalize_audio_features(\n    audio_feature: tf.Tensor,\n    per_frame=False,\n) -> tf.Tensor:\n    \"\"\"\n    TF Mean and variance features normalization\n    Args:\n        audio_feature: tf.Tensor with shape [T, F]\n    Returns:\n        normalized audio features with shape [T, F]",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.speech_featurizers",
        "documentation": {}
    },
    {
        "label": "normalize_signal",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.speech_featurizers",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.speech_featurizers",
        "peekOfCode": "def normalize_signal(\n    signal: np.ndarray,\n) -> np.ndarray:\n    \"\"\"Normailize signal to [-1, 1] range\"\"\"\n    gain = 1.0 / (np.max(np.abs(signal)) + 1e-9)\n    return signal * gain\ndef tf_normalize_signal(\n    signal: tf.Tensor,\n) -> tf.Tensor:\n    \"\"\"",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.speech_featurizers",
        "documentation": {}
    },
    {
        "label": "tf_normalize_signal",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.speech_featurizers",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.speech_featurizers",
        "peekOfCode": "def tf_normalize_signal(\n    signal: tf.Tensor,\n) -> tf.Tensor:\n    \"\"\"\n    TF Normailize signal to [-1, 1] range\n    Args:\n        signal: tf.Tensor with shape [None]\n    Returns:\n        normalized signal with shape [None]\n    \"\"\"",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.speech_featurizers",
        "documentation": {}
    },
    {
        "label": "preemphasis",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.speech_featurizers",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.speech_featurizers",
        "peekOfCode": "def preemphasis(\n    signal: np.ndarray,\n    coeff=0.97,\n) -> np.ndarray:\n    if not coeff or coeff <= 0.0:\n        return signal\n    return np.append(signal[0], signal[1:] - coeff * signal[:-1])\ndef tf_preemphasis(\n    signal: tf.Tensor,\n    coeff=0.97,",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.speech_featurizers",
        "documentation": {}
    },
    {
        "label": "tf_preemphasis",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.speech_featurizers",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.speech_featurizers",
        "peekOfCode": "def tf_preemphasis(\n    signal: tf.Tensor,\n    coeff=0.97,\n):\n    \"\"\"\n    TF Pre-emphasis\n    Args:\n        signal: tf.Tensor with shape [None]\n        coeff: Float that indicates the preemphasis coefficient\n    Returns:",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.speech_featurizers",
        "documentation": {}
    },
    {
        "label": "depreemphasis",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.speech_featurizers",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.speech_featurizers",
        "peekOfCode": "def depreemphasis(\n    signal: np.ndarray,\n    coeff=0.97,\n) -> np.ndarray:\n    if not coeff or coeff <= 0.0:\n        return signal\n    x = np.zeros(signal.shape[0], dtype=np.float32)\n    x[0] = signal[0]\n    for n in range(1, signal.shape[0], 1):\n        x[n] = coeff * x[n - 1] + signal[n]",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.speech_featurizers",
        "documentation": {}
    },
    {
        "label": "tf_depreemphasis",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.speech_featurizers",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.speech_featurizers",
        "peekOfCode": "def tf_depreemphasis(\n    signal: tf.Tensor,\n    coeff=0.97,\n) -> tf.Tensor:\n    \"\"\"\n    TF Depreemphasis\n    Args:\n        signal: tf.Tensor with shape [B, None]\n        coeff: Float that indicates the preemphasis coefficient\n    Returns:",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.speech_featurizers",
        "documentation": {}
    },
    {
        "label": "TextFeaturizer",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.text_featurizers",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.text_featurizers",
        "peekOfCode": "class TextFeaturizer(metaclass=abc.ABCMeta):\n    def __init__(\n        self,\n        decoder_config: dict,\n    ):\n        self.scorer = None\n        self.decoder_config = DecoderConfig(decoder_config)\n        self.blank = None\n        self.tokens2indices = {}\n        self.tokens = []",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.text_featurizers",
        "documentation": {}
    },
    {
        "label": "CharFeaturizer",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.text_featurizers",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.text_featurizers",
        "peekOfCode": "class CharFeaturizer(TextFeaturizer):\n    \"\"\"\n    Extract text feature based on char-level granularity.\n    By looking up the vocabulary table, each line of transcript will be\n    converted to a sequence of integer indexes.\n    \"\"\"\n    def __init__(\n        self,\n        decoder_config: dict,\n    ):",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.text_featurizers",
        "documentation": {}
    },
    {
        "label": "SubwordFeaturizer",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.text_featurizers",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.text_featurizers",
        "peekOfCode": "class SubwordFeaturizer(TextFeaturizer):\n    \"\"\"\n    Extract text feature based on char-level granularity.\n    By looking up the vocabulary table, each line of transcript will be\n    converted to a sequence of integer indexes.\n    \"\"\"\n    def __init__(\n        self,\n        decoder_config: dict,\n        subwords=None,",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.text_featurizers",
        "documentation": {}
    },
    {
        "label": "SentencePieceFeaturizer",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.text_featurizers",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.text_featurizers",
        "peekOfCode": "class SentencePieceFeaturizer(TextFeaturizer):\n    \"\"\"\n    Extract text feature based on sentence piece package.\n    \"\"\"\n    UNK_TOKEN, UNK_TOKEN_ID = \"<unk>\", 1\n    BOS_TOKEN, BOS_TOKEN_ID = \"<s>\", 2\n    EOS_TOKEN, EOS_TOKEN_ID = \"</s>\", 3\n    PAD_TOKEN, PAD_TOKEN_ID = \"<pad>\", 0  # unused, by default\n    def __init__(\n        self,",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.text_featurizers",
        "documentation": {}
    },
    {
        "label": "ENGLISH_CHARACTERS",
        "kind": 5,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.text_featurizers",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.text_featurizers",
        "peekOfCode": "ENGLISH_CHARACTERS = [\n    \" \",\n    \"a\",\n    \"b\",\n    \"c\",\n    \"d\",\n    \"e\",\n    \"f\",\n    \"g\",\n    \"h\",",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.featurizers.text_featurizers",
        "documentation": {}
    },
    {
        "label": "prepare_training_datasets",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.helpers.dataset_helpers",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.helpers.dataset_helpers",
        "peekOfCode": "def prepare_training_datasets(\n    config: Config,\n    speech_featurizer: SpeechFeaturizer,\n    text_featurizer: TextFeaturizer,\n    tfrecords: bool = False,\n    metadata: str = None,\n):\n#    print(\"prepare_training_datasets:\")\n#    for k, v in vars(config.learning_config.eval_dataset_config).items():\n#        print(\"the value of {} is {}\".format(k, v))",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.helpers.dataset_helpers",
        "documentation": {}
    },
    {
        "label": "prepare_testing_datasets",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.helpers.dataset_helpers",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.helpers.dataset_helpers",
        "peekOfCode": "def prepare_testing_datasets(\n    config: Config,\n    speech_featurizer: SpeechFeaturizer,\n    text_featurizer: TextFeaturizer,\n):\n    test_dataset = asr_dataset.ASRSliceDataset(\n        speech_featurizer=speech_featurizer,\n        text_featurizer=text_featurizer,\n        **vars(config.learning_config.test_dataset_config)\n    )",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.helpers.dataset_helpers",
        "documentation": {}
    },
    {
        "label": "prepare_training_data_loaders",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.helpers.dataset_helpers",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.helpers.dataset_helpers",
        "peekOfCode": "def prepare_training_data_loaders(\n    config: Config,\n    train_dataset: asr_dataset.ASRDataset,\n    eval_dataset: asr_dataset.ASRDataset,\n    batch_size: int = None,\n):\n    global_batch_size = batch_size or config.learning_config.running_config.batch_size\n    train_data_loader = train_dataset.create(global_batch_size)\n    eval_data_loader = eval_dataset.create(global_batch_size)\n#    print(\"train_data_loader: \", train_data_loader.cardinality().numpy())",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.helpers.dataset_helpers",
        "documentation": {}
    },
    {
        "label": "run_testing",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.helpers.exec_helpers",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.helpers.exec_helpers",
        "peekOfCode": "def run_testing(\n    model: BaseModel,\n    test_dataset: ASRSliceDataset,\n    test_data_loader: tf.data.Dataset,\n    output: str,\n):\n    with file_util.save_file(file_util.preprocess_paths(output)) as filepath:\n        overwrite = True\n        if tf.io.gfile.exists(filepath):\n            overwrite = input(f\"Overwrite existing result file {filepath} ? (y/n): \").lower() == \"y\"",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.helpers.exec_helpers",
        "documentation": {}
    },
    {
        "label": "convert_tflite",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.helpers.exec_helpers",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.helpers.exec_helpers",
        "peekOfCode": "def convert_tflite(\n    model: BaseModel,\n    output: str,\n):\n#    with tempfile.TemporaryDirectory() as temp_dir_name:\n#        save_path = os.path.join(temp_dir_name, 'saved_model')\n#        print(\"saving saved_model and loading tflite from %s\" % save_path)\n#        model.save(save_path, include_optimizer=False, save_format='tf')\n#        converter = tf.lite.TFLiteConverter.from_saved_model(save_path)\n#    converter = tf.lite.TFLiteConverter.from_saved_model(\"/home/liuyi/TensorFlowASR/examples/conformer/saved_models/\")",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.helpers.exec_helpers",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.helpers.exec_helpers",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.helpers.exec_helpers",
        "peekOfCode": "logger = tf.get_logger()\ndef run_testing(\n    model: BaseModel,\n    test_dataset: ASRSliceDataset,\n    test_data_loader: tf.data.Dataset,\n    output: str,\n):\n    with file_util.save_file(file_util.preprocess_paths(output)) as filepath:\n        overwrite = True\n        if tf.io.gfile.exists(filepath):",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.helpers.exec_helpers",
        "documentation": {}
    },
    {
        "label": "prepare_featurizers",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.helpers.featurizer_helpers",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.helpers.featurizer_helpers",
        "peekOfCode": "def prepare_featurizers(\n    config: Config,\n    subwords: bool = True,\n    sentence_piece: bool = False,\n):\n    speech_featurizer = speech_featurizers.TFSpeechFeaturizer(config.speech_config)\n    if sentence_piece:\n        logger.info(\"Loading SentencePiece model ...\")\n        text_featurizer = text_featurizers.SentencePieceFeaturizer(config.decoder_config)\n    elif subwords:",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.helpers.featurizer_helpers",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.helpers.featurizer_helpers",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.helpers.featurizer_helpers",
        "peekOfCode": "logger = tf.get_logger()\ndef prepare_featurizers(\n    config: Config,\n    subwords: bool = True,\n    sentence_piece: bool = False,\n):\n    speech_featurizer = speech_featurizers.TFSpeechFeaturizer(config.speech_config)\n    if sentence_piece:\n        logger.info(\"Loading SentencePiece model ...\")\n        text_featurizer = text_featurizers.SentencePieceFeaturizer(config.decoder_config)",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.helpers.featurizer_helpers",
        "documentation": {}
    },
    {
        "label": "CtcLoss",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.losses.ctc_loss",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.losses.ctc_loss",
        "peekOfCode": "class CtcLoss(tf.keras.losses.Loss):\n    def __init__(\n        self,\n        blank=0,\n        global_batch_size=None,\n        name=None,\n    ):\n        super(CtcLoss, self).__init__(reduction=tf.keras.losses.Reduction.NONE, name=name)\n        self.blank = blank\n        self.global_batch_size = global_batch_size",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.losses.ctc_loss",
        "documentation": {}
    },
    {
        "label": "ctc_loss",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.losses.ctc_loss",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.losses.ctc_loss",
        "peekOfCode": "def ctc_loss(\n    y_true,\n    y_pred,\n    input_length,\n    label_length,\n    blank,\n    name=None,\n):\n    return tf.nn.ctc_loss(\n        labels=tf.cast(y_true, tf.int32),",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.losses.ctc_loss",
        "documentation": {}
    },
    {
        "label": "RnntLoss",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.losses.rnnt_loss",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.losses.rnnt_loss",
        "peekOfCode": "class RnntLoss(tf.keras.losses.Loss):\n    def __init__(\n        self,\n        blank=0,\n        global_batch_size=None,\n        name=None,\n    ):\n        super(RnntLoss, self).__init__(reduction=tf.keras.losses.Reduction.NONE, name=name)\n        self.blank = blank\n        self.global_batch_size = global_batch_size",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.losses.rnnt_loss",
        "documentation": {}
    },
    {
        "label": "rnnt_loss",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.losses.rnnt_loss",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.losses.rnnt_loss",
        "peekOfCode": "def rnnt_loss(\n    logits,\n    labels,\n    label_length,\n    logit_length,\n    blank=0,\n    name=None,\n):\n    if use_warprnnt:\n        return rnnt_loss_warprnnt(",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.losses.rnnt_loss",
        "documentation": {}
    },
    {
        "label": "rnnt_loss_warprnnt",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.losses.rnnt_loss",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.losses.rnnt_loss",
        "peekOfCode": "def rnnt_loss_warprnnt(\n    logits,\n    labels,\n    label_length,\n    logit_length,\n    blank=0,\n):\n    if not env_util.has_devices([\"GPU\", \"TPU\"]):\n        logits = tf.nn.log_softmax(logits)\n    loss = warp_rnnt_loss(",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.losses.rnnt_loss",
        "documentation": {}
    },
    {
        "label": "nan_to_zero",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.losses.rnnt_loss",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.losses.rnnt_loss",
        "peekOfCode": "def nan_to_zero(\n    input_tensor,\n):\n    return tf.where(tf.math.is_nan(input_tensor), tf.zeros_like(input_tensor), input_tensor)\ndef reduce_logsumexp(\n    input_tensor,\n    axis,\n):\n    maximum = tf.reduce_max(input_tensor, axis=axis)\n    input_tensor = nan_to_zero(input_tensor - maximum)",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.losses.rnnt_loss",
        "documentation": {}
    },
    {
        "label": "reduce_logsumexp",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.losses.rnnt_loss",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.losses.rnnt_loss",
        "peekOfCode": "def reduce_logsumexp(\n    input_tensor,\n    axis,\n):\n    maximum = tf.reduce_max(input_tensor, axis=axis)\n    input_tensor = nan_to_zero(input_tensor - maximum)\n    return tf.math.log(tf.reduce_sum(tf.exp(input_tensor), axis=axis)) + maximum\ndef extract_diagonals(\n    log_probs,\n):",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.losses.rnnt_loss",
        "documentation": {}
    },
    {
        "label": "extract_diagonals",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.losses.rnnt_loss",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.losses.rnnt_loss",
        "peekOfCode": "def extract_diagonals(\n    log_probs,\n):\n    time_steps = tf.shape(log_probs)[1]  # T\n#    tf.print(\"time_steps: \", time_steps)\n    output_steps = tf.shape(log_probs)[2]  # U + 1\n#    tf.print(\"output_steps: \", output_steps)\n    reverse_log_probs = tf.reverse(log_probs, axis=[-1])\n    paddings = [[0, 0], [0, 0], [time_steps - 1, 0]]\n    padded_reverse_log_probs = tf.pad(reverse_log_probs, paddings, \"CONSTANT\", constant_values=LOG_0)",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.losses.rnnt_loss",
        "documentation": {}
    },
    {
        "label": "transition_probs",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.losses.rnnt_loss",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.losses.rnnt_loss",
        "peekOfCode": "def transition_probs(\n    one_hot_labels,\n    log_probs,\n):\n    \"\"\"\n    :return: blank_probs with shape batch_size x input_max_len x target_max_len\n             truth_probs with shape batch_size x input_max_len x (target_max_len-1)\n    \"\"\"\n    blank_probs = log_probs[:, :, :, 0]\n    truth_probs = tf.reduce_sum(tf.multiply(log_probs[:, :, :-1, :], one_hot_labels), axis=-1)",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.losses.rnnt_loss",
        "documentation": {}
    },
    {
        "label": "forward_dp",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.losses.rnnt_loss",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.losses.rnnt_loss",
        "peekOfCode": "def forward_dp(\n    bp_diags,\n    tp_diags,\n    batch_size,\n    input_max_len,\n    target_max_len,\n):\n    \"\"\"\n    :return: forward variable alpha with shape batch_size x input_max_len x target_max_len\n    \"\"\"",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.losses.rnnt_loss",
        "documentation": {}
    },
    {
        "label": "backward_dp",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.losses.rnnt_loss",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.losses.rnnt_loss",
        "peekOfCode": "def backward_dp(\n    bp_diags,\n    tp_diags,\n    batch_size,\n    input_max_len,\n    target_max_len,\n    label_length,\n    logit_length,\n    blank_sl,\n):",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.losses.rnnt_loss",
        "documentation": {}
    },
    {
        "label": "compute_rnnt_loss_and_grad_helper",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.losses.rnnt_loss",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.losses.rnnt_loss",
        "peekOfCode": "def compute_rnnt_loss_and_grad_helper(logits, labels, label_length, logit_length):\n    batch_size = tf.shape(logits)[0]\n    input_max_len = tf.shape(logits)[1]\n    target_max_len = tf.shape(logits)[2]\n    vocab_size = tf.shape(logits)[3]\n    one_hot_labels = tf.one_hot(\n        tf.tile(tf.expand_dims(labels, axis=1), multiples=[1, input_max_len, 1]),\n        depth=vocab_size,\n    )\n    log_probs = tf.nn.log_softmax(logits)",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.losses.rnnt_loss",
        "documentation": {}
    },
    {
        "label": "rnnt_loss_tf",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.losses.rnnt_loss",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.losses.rnnt_loss",
        "peekOfCode": "def rnnt_loss_tf(\n    logits,\n    labels,\n    label_length,\n    logit_length,\n    name=None,\n):\n    name = \"rnnt_loss\" if name is None else name\n    with tf.name_scope(name):\n        logits = tf.convert_to_tensor(logits, name=\"logits\")",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.losses.rnnt_loss",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.losses.rnnt_loss",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.losses.rnnt_loss",
        "peekOfCode": "logger = tf.get_logger()\nLOG_0 = float(\"-inf\")\ntry:\n    from warprnnt_tensorflow import rnnt_loss as warp_rnnt_loss\n    use_warprnnt = True\n    logger.info(\"Use RNNT loss in WarpRnnt\")\nexcept ImportError:\n    logger.info(\"Use RNNT loss in TensorFlow\")\n    use_warprnnt = False\nclass RnntLoss(tf.keras.losses.Loss):",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.losses.rnnt_loss",
        "documentation": {}
    },
    {
        "label": "LOG_0",
        "kind": 5,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.losses.rnnt_loss",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.losses.rnnt_loss",
        "peekOfCode": "LOG_0 = float(\"-inf\")\ntry:\n    from warprnnt_tensorflow import rnnt_loss as warp_rnnt_loss\n    use_warprnnt = True\n    logger.info(\"Use RNNT loss in WarpRnnt\")\nexcept ImportError:\n    logger.info(\"Use RNNT loss in TensorFlow\")\n    use_warprnnt = False\nclass RnntLoss(tf.keras.losses.Loss):\n    def __init__(",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.losses.rnnt_loss",
        "documentation": {}
    },
    {
        "label": "ErrorRate",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.metrics.error_rates",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.metrics.error_rates",
        "peekOfCode": "class ErrorRate(tf.keras.metrics.Metric):\n    \"\"\"Metric for WER or CER\"\"\"\n    def __init__(\n        self,\n        func,\n        name=\"error_rate\",\n        **kwargs,\n    ):\n        super(ErrorRate, self).__init__(name=name, **kwargs)\n        self.numerator = self.add_weight(name=f\"{name}_numerator\", initializer=\"zeros\")",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.metrics.error_rates",
        "documentation": {}
    },
    {
        "label": "GLU",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.models.activations.glu",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.models.activations.glu",
        "peekOfCode": "class GLU(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        axis=-1,\n        name=\"glu_activation\",\n        **kwargs,\n    ):\n        super(GLU, self).__init__(name=name, **kwargs)\n        self.axis = axis\n    def call(",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.models.activations.glu",
        "documentation": {}
    },
    {
        "label": "CtcModel",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.models.ctc.base_ctc",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.models.ctc.base_ctc",
        "peekOfCode": "class CtcModel(BaseModel):\n    def __init__(\n        self,\n        encoder: tf.keras.Model,\n        decoder: Union[tf.keras.Model, tf.keras.layers.Layer] = None,\n        vocabulary_size: int = None,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.encoder = encoder",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.models.ctc.base_ctc",
        "documentation": {}
    },
    {
        "label": "Reshape",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.models.ctc.deepspeech2",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.models.ctc.deepspeech2",
        "peekOfCode": "class Reshape(tf.keras.layers.Layer):\n    def call(\n        self,\n        inputs,\n    ):\n        return math_util.merge_two_last_dims(inputs)\nclass ConvBlock(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        conv_type: str = \"conv2d\",",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.models.ctc.deepspeech2",
        "documentation": {}
    },
    {
        "label": "ConvBlock",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.models.ctc.deepspeech2",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.models.ctc.deepspeech2",
        "peekOfCode": "class ConvBlock(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        conv_type: str = \"conv2d\",\n        kernels: list = [11, 41],\n        strides: list = [2, 2],\n        filters: int = 32,\n        dropout: float = 0.1,\n        **kwargs,\n    ):",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.models.ctc.deepspeech2",
        "documentation": {}
    },
    {
        "label": "ConvModule",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.models.ctc.deepspeech2",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.models.ctc.deepspeech2",
        "peekOfCode": "class ConvModule(tf.keras.Model):\n    def __init__(\n        self,\n        conv_type: str = \"conv2d\",\n        kernels: list = [[11, 41], [11, 21], [11, 21]],\n        strides: list = [[2, 2], [1, 2], [1, 2]],\n        filters: list = [32, 32, 96],\n        dropout: float = 0.1,\n        **kwargs,\n    ):",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.models.ctc.deepspeech2",
        "documentation": {}
    },
    {
        "label": "RnnBlock",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.models.ctc.deepspeech2",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.models.ctc.deepspeech2",
        "peekOfCode": "class RnnBlock(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        rnn_type: str = \"lstm\",\n        units: int = 1024,\n        bidirectional: bool = True,\n        rowconv: int = 0,\n        dropout: float = 0.1,\n        **kwargs,\n    ):",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.models.ctc.deepspeech2",
        "documentation": {}
    },
    {
        "label": "RnnModule",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.models.ctc.deepspeech2",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.models.ctc.deepspeech2",
        "peekOfCode": "class RnnModule(tf.keras.Model):\n    def __init__(\n        self,\n        nlayers: int = 5,\n        rnn_type: str = \"lstm\",\n        units: int = 1024,\n        bidirectional: bool = True,\n        rowconv: int = 0,\n        dropout: float = 0.1,\n        **kwargs,",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.models.ctc.deepspeech2",
        "documentation": {}
    },
    {
        "label": "FcBlock",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.models.ctc.deepspeech2",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.models.ctc.deepspeech2",
        "peekOfCode": "class FcBlock(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        units: int = 1024,\n        dropout: float = 0.1,\n        **kwargs,\n    ):\n        super(FcBlock, self).__init__(**kwargs)\n        self.fc = tf.keras.layers.Dense(units, name=f\"{self.name}_fc\")\n        self.bn = tf.keras.layers.BatchNormalization(name=f\"{self.name}_bn\")",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.models.ctc.deepspeech2",
        "documentation": {}
    },
    {
        "label": "FcModule",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.models.ctc.deepspeech2",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.models.ctc.deepspeech2",
        "peekOfCode": "class FcModule(tf.keras.Model):\n    def __init__(\n        self,\n        nlayers: int = 0,\n        units: int = 1024,\n        dropout: float = 0.1,\n        **kwargs,\n    ):\n        super(FcModule, self).__init__(**kwargs)\n        self.blocks = [FcBlock(units=units, dropout=dropout, name=f\"{self.name}_block_{i}\") for i in range(nlayers)]",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.models.ctc.deepspeech2",
        "documentation": {}
    },
    {
        "label": "DeepSpeech2Encoder",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.models.ctc.deepspeech2",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.models.ctc.deepspeech2",
        "peekOfCode": "class DeepSpeech2Encoder(tf.keras.Model):\n    def __init__(\n        self,\n        conv_type: str = \"conv2d\",\n        conv_kernels: list = [[11, 41], [11, 21], [11, 21]],\n        conv_strides: list = [[2, 2], [1, 2], [1, 2]],\n        conv_filters: list = [32, 32, 96],\n        conv_dropout: float = 0.1,\n        rnn_nlayers: int = 5,\n        rnn_type: str = \"lstm\",",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.models.ctc.deepspeech2",
        "documentation": {}
    },
    {
        "label": "DeepSpeech2",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.models.ctc.deepspeech2",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.models.ctc.deepspeech2",
        "peekOfCode": "class DeepSpeech2(CtcModel):\n    def __init__(\n        self,\n        vocabulary_size: int,\n        conv_type: str = \"conv2d\",\n        conv_kernels: list = [[11, 41], [11, 21], [11, 21]],\n        conv_strides: list = [[2, 2], [1, 2], [1, 2]],\n        conv_filters: list = [32, 32, 96],\n        conv_dropout: float = 0.1,\n        rnn_nlayers: int = 5,",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.models.ctc.deepspeech2",
        "documentation": {}
    },
    {
        "label": "Reshape",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.models.ctc.jasper",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.models.ctc.jasper",
        "peekOfCode": "class Reshape(tf.keras.layers.Layer):\n    def call(self, inputs):\n        return math_util.merge_two_last_dims(inputs)\nclass JasperSubBlock(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        channels: int = 256,\n        kernels: int = 11,\n        strides: int = 1,\n        dropout: float = 0.1,",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.models.ctc.jasper",
        "documentation": {}
    },
    {
        "label": "JasperSubBlock",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.models.ctc.jasper",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.models.ctc.jasper",
        "peekOfCode": "class JasperSubBlock(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        channels: int = 256,\n        kernels: int = 11,\n        strides: int = 1,\n        dropout: float = 0.1,\n        dilation: int = 1,\n        kernel_regularizer=None,\n        bias_regularizer=None,",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.models.ctc.jasper",
        "documentation": {}
    },
    {
        "label": "JasperResidual",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.models.ctc.jasper",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.models.ctc.jasper",
        "peekOfCode": "class JasperResidual(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        channels: int = 256,\n        kernel_regularizer=None,\n        bias_regularizer=None,\n        **kwargs,\n    ):\n        super(JasperResidual, self).__init__(**kwargs)\n        self.pointwise_conv1d = tf.keras.layers.Conv1D(",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.models.ctc.jasper",
        "documentation": {}
    },
    {
        "label": "JasperSubBlockResidual",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.models.ctc.jasper",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.models.ctc.jasper",
        "peekOfCode": "class JasperSubBlockResidual(JasperSubBlock):\n    def __init__(\n        self,\n        channels: int = 256,\n        kernels: int = 11,\n        strides: int = 1,\n        dropout: float = 0.1,\n        dilation: int = 1,\n        nresiduals: int = 1,\n        kernel_regularizer=None,",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.models.ctc.jasper",
        "documentation": {}
    },
    {
        "label": "JasperBlock",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.models.ctc.jasper",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.models.ctc.jasper",
        "peekOfCode": "class JasperBlock(tf.keras.Model):\n    def __init__(\n        self,\n        nsubblocks: int = 3,\n        channels: int = 256,\n        kernels: int = 11,\n        dropout: float = 0.1,\n        dense: bool = False,\n        nresiduals: int = 1,\n        kernel_regularizer=None,",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.models.ctc.jasper",
        "documentation": {}
    },
    {
        "label": "JasperEncoder",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.models.ctc.jasper",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.models.ctc.jasper",
        "peekOfCode": "class JasperEncoder(tf.keras.Model):\n    def __init__(\n        self,\n        dense: bool = False,\n        first_additional_block_channels: int = 256,\n        first_additional_block_kernels: int = 11,\n        first_additional_block_strides: int = 2,\n        first_additional_block_dilation: int = 1,\n        first_additional_block_dropout: int = 0.2,\n        nsubblocks: int = 5,",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.models.ctc.jasper",
        "documentation": {}
    },
    {
        "label": "Jasper",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.models.ctc.jasper",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.models.ctc.jasper",
        "peekOfCode": "class Jasper(CtcModel):\n    def __init__(\n        self,\n        vocabulary_size: int,\n        dense: bool = False,\n        first_additional_block_channels: int = 256,\n        first_additional_block_kernels: int = 11,\n        first_additional_block_strides: int = 2,\n        first_additional_block_dilation: int = 1,\n        first_additional_block_dropout: int = 0.2,",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.models.ctc.jasper",
        "documentation": {}
    },
    {
        "label": "FFModule",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.models.encoders.conformer",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.models.encoders.conformer",
        "peekOfCode": "class FFModule(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        input_dim,\n        dropout=0.0,\n        fc_factor=0.5,\n        kernel_regularizer=L2,\n        bias_regularizer=L2,\n        name=\"ff_module\",\n        **kwargs,",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.models.encoders.conformer",
        "documentation": {}
    },
    {
        "label": "MHSAModule",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.models.encoders.conformer",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.models.encoders.conformer",
        "peekOfCode": "class MHSAModule(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        head_size,\n        num_heads,\n        dropout=0.0,\n        mha_type=\"relmha\",\n        kernel_regularizer=L2,\n        bias_regularizer=L2,\n        name=\"mhsa_module\",",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.models.encoders.conformer",
        "documentation": {}
    },
    {
        "label": "ConvModule",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.models.encoders.conformer",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.models.encoders.conformer",
        "peekOfCode": "class ConvModule(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        input_dim,\n        kernel_size=32,\n        dropout=0.0,\n        depth_multiplier=1,\n        kernel_regularizer=L2,\n        bias_regularizer=L2,\n        name=\"conv_module\",",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.models.encoders.conformer",
        "documentation": {}
    },
    {
        "label": "ConformerBlock",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.models.encoders.conformer",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.models.encoders.conformer",
        "peekOfCode": "class ConformerBlock(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        input_dim,\n        dropout=0.0,\n        fc_factor=0.5,\n        head_size=36,\n        num_heads=4,\n        mha_type=\"relmha\",\n        kernel_size=32,",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.models.encoders.conformer",
        "documentation": {}
    },
    {
        "label": "ConformerEncoder",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.models.encoders.conformer",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.models.encoders.conformer",
        "peekOfCode": "class ConformerEncoder(tf.keras.Model):\n    def __init__(\n        self,\n        subsampling,\n        positional_encoding=\"sinusoid\",\n        dmodel=144,\n        num_blocks=16,\n        mha_type=\"relmha\",\n        head_size=36,\n        num_heads=4,",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.models.encoders.conformer",
        "documentation": {}
    },
    {
        "label": "L2",
        "kind": 5,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.models.encoders.conformer",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.models.encoders.conformer",
        "peekOfCode": "L2 = tf.keras.regularizers.l2(1e-6)\nclass FFModule(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        input_dim,\n        dropout=0.0,\n        fc_factor=0.5,\n        kernel_regularizer=L2,\n        bias_regularizer=L2,\n        name=\"ff_module\",",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.models.encoders.conformer",
        "documentation": {}
    },
    {
        "label": "Reshape",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.models.encoders.contextnet",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.models.encoders.contextnet",
        "peekOfCode": "class Reshape(tf.keras.layers.Layer):\n    def call(self, inputs):\n        return math_util.merge_two_last_dims(inputs)\nclass ConvModule(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        kernel_size: int = 3,\n        strides: int = 1,\n        filters: int = 256,\n        activation: str = \"silu\",",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.models.encoders.contextnet",
        "documentation": {}
    },
    {
        "label": "ConvModule",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.models.encoders.contextnet",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.models.encoders.contextnet",
        "peekOfCode": "class ConvModule(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        kernel_size: int = 3,\n        strides: int = 1,\n        filters: int = 256,\n        activation: str = \"silu\",\n        kernel_regularizer=None,\n        bias_regularizer=None,\n        **kwargs,",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.models.encoders.contextnet",
        "documentation": {}
    },
    {
        "label": "SEModule",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.models.encoders.contextnet",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.models.encoders.contextnet",
        "peekOfCode": "class SEModule(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        kernel_size: int = 3,\n        strides: int = 1,\n        filters: int = 256,\n        activation: str = \"silu\",\n        kernel_regularizer=None,\n        bias_regularizer=None,\n        **kwargs,",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.models.encoders.contextnet",
        "documentation": {}
    },
    {
        "label": "ConvBlock",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.models.encoders.contextnet",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.models.encoders.contextnet",
        "peekOfCode": "class ConvBlock(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        nlayers: int = 3,\n        kernel_size: int = 3,\n        filters: int = 256,\n        strides: int = 1,\n        residual: bool = True,\n        activation: str = \"silu\",\n        alpha: float = 1.0,",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.models.encoders.contextnet",
        "documentation": {}
    },
    {
        "label": "ContextNetEncoder",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.models.encoders.contextnet",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.models.encoders.contextnet",
        "peekOfCode": "class ContextNetEncoder(tf.keras.Model):\n    def __init__(\n        self,\n        blocks: List[dict] = [],\n        alpha: float = 1.0,\n        kernel_regularizer=None,\n        bias_regularizer=None,\n        **kwargs,\n    ):\n        super(ContextNetEncoder, self).__init__(**kwargs)",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.models.encoders.contextnet",
        "documentation": {}
    },
    {
        "label": "get_activation",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.models.encoders.contextnet",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.models.encoders.contextnet",
        "peekOfCode": "def get_activation(\n    activation: str = \"silu\",\n):\n    activation = activation.lower()\n    if activation in [\"silu\", \"swish\"]:\n        return tf.nn.swish\n    elif activation == \"relu\":\n        return tf.nn.relu\n    elif activation == \"linear\":\n        return tf.keras.activations.linear",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.models.encoders.contextnet",
        "documentation": {}
    },
    {
        "label": "L2",
        "kind": 5,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.models.encoders.contextnet",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.models.encoders.contextnet",
        "peekOfCode": "L2 = tf.keras.regularizers.l2(1e-6)\ndef get_activation(\n    activation: str = \"silu\",\n):\n    activation = activation.lower()\n    if activation in [\"silu\", \"swish\"]:\n        return tf.nn.swish\n    elif activation == \"relu\":\n        return tf.nn.relu\n    elif activation == \"linear\":",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.models.encoders.contextnet",
        "documentation": {}
    },
    {
        "label": "BNLSTMCell",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.models.layers.bnlstmcell",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.models.layers.bnlstmcell",
        "peekOfCode": "class BNLSTMCell(tf.keras.layers.LSTMCell):\n    def __init__(\n        self,\n        *args,\n        **kwargs,\n    ):\n        super().__init__(*args, **kwargs)\n        self.beta = self.add_weight(\n            shape=(self.units * 4,),\n            name=\"lstm_bn_beta\",",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.models.layers.bnlstmcell",
        "documentation": {}
    },
    {
        "label": "ds2_rnn_batch_norm",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.models.layers.bnlstmcell",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.models.layers.bnlstmcell",
        "peekOfCode": "def ds2_rnn_batch_norm(\n    x_i,\n    x_f,\n    x_c,\n    x_o,\n    beta=None,\n    gamma=None,\n):\n    # x is input * weight with shape [batch_size, units * 4]\n    # Merge into single array of features",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.models.layers.bnlstmcell",
        "documentation": {}
    },
    {
        "label": "Embedding",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.models.layers.embedding",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.models.layers.embedding",
        "peekOfCode": "class Embedding(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        vocab_size,\n        embed_dim,\n        contraint=None,\n        regularizer=None,\n        initializer=None,\n        **kwargs,\n    ):",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.models.layers.embedding",
        "documentation": {}
    },
    {
        "label": "MultiHeadAttention",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.models.layers.multihead_attention",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.models.layers.multihead_attention",
        "peekOfCode": "class MultiHeadAttention(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        num_heads,\n        head_size,\n        output_size: int = None,\n        dropout: float = 0.0,\n        use_projection_bias: bool = True,\n        return_attn_coef: bool = False,\n        kernel_initializer: typing.Union[str, typing.Callable] = \"glorot_uniform\",",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.models.layers.multihead_attention",
        "documentation": {}
    },
    {
        "label": "RelPositionMultiHeadAttention",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.models.layers.multihead_attention",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.models.layers.multihead_attention",
        "peekOfCode": "class RelPositionMultiHeadAttention(MultiHeadAttention):\n    def build(\n        self,\n        input_shape,\n    ):\n        num_pos_features = input_shape[-1][-1]\n        self.pos_kernel = self.add_weight(\n            name=\"pos_kernel\",\n            shape=[self.num_heads, num_pos_features, self.head_size],\n            initializer=self.kernel_initializer,",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.models.layers.multihead_attention",
        "documentation": {}
    },
    {
        "label": "PointWiseFFN",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.models.layers.point_wise_ffn",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.models.layers.point_wise_ffn",
        "peekOfCode": "class PointWiseFFN(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        size,\n        output_size,\n        activation=\"relu\",\n        dropout=0.1,\n        name=\"point_wise_ffn\",\n        **kwargs,\n    ):",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.models.layers.point_wise_ffn",
        "documentation": {}
    },
    {
        "label": "PositionalEncoding",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.models.layers.positional_encoding",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.models.layers.positional_encoding",
        "peekOfCode": "class PositionalEncoding(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        alpha: int = 1,\n        beta: int = 0,\n        name=\"positional_encoding\",\n        **kwargs,\n    ):\n        super().__init__(trainable=False, name=name, **kwargs)\n        self.alpha = alpha",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.models.layers.positional_encoding",
        "documentation": {}
    },
    {
        "label": "PositionalEncodingConcat",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.models.layers.positional_encoding",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.models.layers.positional_encoding",
        "peekOfCode": "class PositionalEncodingConcat(PositionalEncoding):\n    def build(\n        self,\n        input_shape,\n    ):\n        dmodel = input_shape[-1]\n        assert dmodel % 2 == 0, f\"Input last dim must be even: {dmodel}\"\n    @staticmethod\n    def encode(\n        max_len,",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.models.layers.positional_encoding",
        "documentation": {}
    },
    {
        "label": "RowConv1D",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.models.layers.row_conv_1d",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.models.layers.row_conv_1d",
        "peekOfCode": "class RowConv1D(tf.keras.layers.Conv1D):\n    def __init__(\n        self,\n        filters,\n        future_context,\n        **kwargs,\n    ):\n        assert future_context >= 0, \"Future context must be positive\"\n        super().__init__(filters=filters, kernel_size=(future_context * 2 + 1), **kwargs)\n        self.future_context = future_context",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.models.layers.row_conv_1d",
        "documentation": {}
    },
    {
        "label": "SequenceBatchNorm",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.models.layers.sequence_wise_bn",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.models.layers.sequence_wise_bn",
        "peekOfCode": "class SequenceBatchNorm(tf.keras.layers.Layer):\n    def __init__(self, name, time_major=False, **kwargs):\n        super(SequenceBatchNorm, self).__init__(name=name, **kwargs)\n        self.time_major = time_major\n    def build(\n        self,\n        input_shape,\n    ):\n        self.beta = self.add_weight(\n            shape=[input_shape[-1]],",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.models.layers.sequence_wise_bn",
        "documentation": {}
    },
    {
        "label": "TimeReduction",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.models.layers.subsampling",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.models.layers.subsampling",
        "peekOfCode": "class TimeReduction(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        factor: int,\n        name: str = \"TimeReduction\",\n        **kwargs,\n    ):\n        super(TimeReduction, self).__init__(name=name, **kwargs)\n        self.time_reduction_factor = factor\n    def padding(",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.models.layers.subsampling",
        "documentation": {}
    },
    {
        "label": "VggSubsampling",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.models.layers.subsampling",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.models.layers.subsampling",
        "peekOfCode": "class VggSubsampling(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        filters: tuple or list = (32, 64),\n        kernel_size: int or list or tuple = 3,\n        strides: int or list or tuple = 2,\n        kernel_regularizer=None,\n        bias_regularizer=None,\n        name=\"VggSubsampling\",\n        **kwargs,",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.models.layers.subsampling",
        "documentation": {}
    },
    {
        "label": "Conv2dSubsampling",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.models.layers.subsampling",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.models.layers.subsampling",
        "peekOfCode": "class Conv2dSubsampling(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        filters: int,\n        strides: list or tuple or int = 2,\n        kernel_size: int or list or tuple = 3,\n        kernel_regularizer=None,\n        bias_regularizer=None,\n        name=\"Conv2dSubsampling\",\n        **kwargs,",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.models.layers.subsampling",
        "documentation": {}
    },
    {
        "label": "TransducerPrediction",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.models.transducer.base_transducer",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.models.transducer.base_transducer",
        "peekOfCode": "class TransducerPrediction(tf.keras.Model):\n    def __init__(\n        self,\n        vocabulary_size: int,\n        embed_dim: int,\n        embed_dropout: float = 0,\n        num_rnns: int = 1,\n        rnn_units: int = 512,\n        rnn_type: str = \"lstm\",\n        rnn_implementation: int = 2,",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.models.transducer.base_transducer",
        "documentation": {}
    },
    {
        "label": "TransducerJointReshape",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.models.transducer.base_transducer",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.models.transducer.base_transducer",
        "peekOfCode": "class TransducerJointReshape(tf.keras.layers.Layer):\n    def __init__(self, axis: int = 1, name=\"transducer_joint_reshape\", **kwargs):\n        super().__init__(name=name, trainable=False, **kwargs)\n        self.axis = axis\n    def call(self, inputs, repeats=None, **kwargs):\n        outputs = tf.expand_dims(inputs, axis=self.axis)\n        return tf.repeat(outputs, repeats=repeats, axis=self.axis)\n    def get_config(self):\n        conf = super().get_config()\n        conf.update({\"axis\": self.axis})",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.models.transducer.base_transducer",
        "documentation": {}
    },
    {
        "label": "TransducerJoint",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.models.transducer.base_transducer",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.models.transducer.base_transducer",
        "peekOfCode": "class TransducerJoint(tf.keras.Model):\n    def __init__(\n        self,\n        vocabulary_size: int,\n        joint_dim: int = 1024,\n        activation: str = \"tanh\",\n        prejoint_linear: bool = True,\n        postjoint_linear: bool = False,\n        joint_mode: str = \"add\",\n        kernel_regularizer=None,",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.models.transducer.base_transducer",
        "documentation": {}
    },
    {
        "label": "Transducer",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.models.transducer.base_transducer",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.models.transducer.base_transducer",
        "peekOfCode": "class Transducer(BaseModel):\n    \"\"\"Transducer Model Warper\"\"\"\n    def __init__(\n        self,\n        encoder: tf.keras.Model,\n        vocabulary_size: int,\n        embed_dim: int = 512,\n        embed_dropout: float = 0,\n        num_rnns: int = 1,\n        rnn_units: int = 320,",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.models.transducer.base_transducer",
        "documentation": {}
    },
    {
        "label": "Hypothesis",
        "kind": 5,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.models.transducer.base_transducer",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.models.transducer.base_transducer",
        "peekOfCode": "Hypothesis = collections.namedtuple(\"Hypothesis\", (\"index\", \"prediction\", \"states\"))\nBeamHypothesis = collections.namedtuple(\"BeamHypothesis\", (\"score\", \"indices\", \"prediction\", \"states\"))\nclass TransducerPrediction(tf.keras.Model):\n    def __init__(\n        self,\n        vocabulary_size: int,\n        embed_dim: int,\n        embed_dropout: float = 0,\n        num_rnns: int = 1,\n        rnn_units: int = 512,",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.models.transducer.base_transducer",
        "documentation": {}
    },
    {
        "label": "BeamHypothesis",
        "kind": 5,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.models.transducer.base_transducer",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.models.transducer.base_transducer",
        "peekOfCode": "BeamHypothesis = collections.namedtuple(\"BeamHypothesis\", (\"score\", \"indices\", \"prediction\", \"states\"))\nclass TransducerPrediction(tf.keras.Model):\n    def __init__(\n        self,\n        vocabulary_size: int,\n        embed_dim: int,\n        embed_dropout: float = 0,\n        num_rnns: int = 1,\n        rnn_units: int = 512,\n        rnn_type: str = \"lstm\",",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.models.transducer.base_transducer",
        "documentation": {}
    },
    {
        "label": "Conformer",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.models.transducer.conformer",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.models.transducer.conformer",
        "peekOfCode": "class Conformer(Transducer):\n    def __init__(\n        self,\n        vocabulary_size: int,\n        encoder_subsampling: dict,\n        encoder_positional_encoding: str = \"sinusoid\",\n        encoder_dmodel: int = 144,\n        encoder_num_blocks: int = 16,\n        encoder_head_size: int = 36,\n        encoder_num_heads: int = 4,",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.models.transducer.conformer",
        "documentation": {}
    },
    {
        "label": "ContextNet",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.models.transducer.contextnet",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.models.transducer.contextnet",
        "peekOfCode": "class ContextNet(Transducer):\n    def __init__(\n        self,\n        vocabulary_size: int,\n        encoder_blocks: List[dict],\n        encoder_alpha: float = 0.5,\n        encoder_trainable: bool = True,\n        prediction_embed_dim: int = 512,\n        prediction_embed_dropout: int = 0,\n        prediction_num_rnns: int = 1,",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.models.transducer.contextnet",
        "documentation": {}
    },
    {
        "label": "Reshape",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.models.transducer.rnn_transducer",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.models.transducer.rnn_transducer",
        "peekOfCode": "class Reshape(tf.keras.layers.Layer):\n    def call(self, inputs):\n        return math_util.merge_two_last_dims(inputs)\nclass RnnTransducerBlock(tf.keras.Model):\n    def __init__(\n        self,\n        reduction_factor: int = 0,\n        dmodel: int = 640,\n        rnn_type: str = \"lstm\",\n        rnn_units: int = 2048,",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.models.transducer.rnn_transducer",
        "documentation": {}
    },
    {
        "label": "RnnTransducerBlock",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.models.transducer.rnn_transducer",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.models.transducer.rnn_transducer",
        "peekOfCode": "class RnnTransducerBlock(tf.keras.Model):\n    def __init__(\n        self,\n        reduction_factor: int = 0,\n        dmodel: int = 640,\n        rnn_type: str = \"lstm\",\n        rnn_units: int = 2048,\n        layer_norm: bool = True,\n        kernel_regularizer=None,\n        bias_regularizer=None,",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.models.transducer.rnn_transducer",
        "documentation": {}
    },
    {
        "label": "RnnTransducerEncoder",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.models.transducer.rnn_transducer",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.models.transducer.rnn_transducer",
        "peekOfCode": "class RnnTransducerEncoder(tf.keras.Model):\n    def __init__(\n        self,\n        reductions: dict = {0: 3, 1: 2},\n        dmodel: int = 640,\n        nlayers: int = 8,\n        rnn_type: str = \"lstm\",\n        rnn_units: int = 2048,\n        layer_norm: bool = True,\n        kernel_regularizer=None,",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.models.transducer.rnn_transducer",
        "documentation": {}
    },
    {
        "label": "RnnTransducer",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.models.transducer.rnn_transducer",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.models.transducer.rnn_transducer",
        "peekOfCode": "class RnnTransducer(Transducer):\n    def __init__(\n        self,\n        vocabulary_size: int,\n        encoder_reductions: dict = {0: 3, 1: 2},\n        encoder_dmodel: int = 640,\n        encoder_nlayers: int = 8,\n        encoder_rnn_type: str = \"lstm\",\n        encoder_rnn_units: int = 2048,\n        encoder_layer_norm: bool = True,",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.models.transducer.rnn_transducer",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.models.base_model",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.models.base_model",
        "peekOfCode": "class BaseModel(tf.keras.Model):\n    def save(\n        self,\n        filepath,\n        overwrite=True,\n        include_optimizer=True,\n        save_format=None,\n        signatures=None,\n        options=None,\n        save_traces=False,",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.models.base_model",
        "documentation": {}
    },
    {
        "label": "GradientAccumulation",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.optimizers.accumulation",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.optimizers.accumulation",
        "peekOfCode": "class GradientAccumulation:\n    def __init__(self, trainable_variables):\n        self.gradients = [\n            tf.Variable(\n                tf.zeros_like(g),\n                trainable=False,\n                synchronization=tf.VariableSynchronization.ON_READ,\n            ) for g in trainable_variables\n        ]\n    def reset(self):",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.optimizers.accumulation",
        "documentation": {}
    },
    {
        "label": "TransformerSchedule",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.optimizers.schedules",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.optimizers.schedules",
        "peekOfCode": "class TransformerSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(self, d_model, warmup_steps=4000, max_lr=None):\n        super(TransformerSchedule, self).__init__()\n        self.d_model = d_model\n        self.d_model = tf.cast(self.d_model, tf.float32)\n        self.max_lr = max_lr\n        self.warmup_steps = warmup_steps\n    def __call__(self, step):\n        # lr = (d_model^-0.5) * min(step^-0.5, step*(warm_up^-1.5))\n        step = tf.cast(step, dtype=tf.float32)",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.optimizers.schedules",
        "documentation": {}
    },
    {
        "label": "SANSchedule",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.optimizers.schedules",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.optimizers.schedules",
        "peekOfCode": "class SANSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(self, lamb, d_model, warmup_steps=4000):\n        super(SANSchedule, self).__init__()\n        self.lamb = tf.cast(lamb, tf.float32)\n        self.d_model = tf.cast(d_model, tf.float32)\n        self.warmup_steps = tf.cast(warmup_steps, tf.float32)\n    def __call__(self, step):\n        step = tf.cast(step, dtype=tf.float32)\n        arg1 = step / (self.warmup_steps ** 1.5)\n        arg2 = 1 / tf.math.sqrt(step)",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.optimizers.schedules",
        "documentation": {}
    },
    {
        "label": "BoundExponentialDecay",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.optimizers.schedules",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.optimizers.schedules",
        "peekOfCode": "class BoundExponentialDecay(ExponentialDecay):\n    def __init__(self, min_lr=0.0, **kwargs):\n        super().__init__(**kwargs)\n        self.min_lr = min_lr\n    def __call__(self, step):\n        with tf.name_scope(self.name or \"ExponentialDecay\") as name:\n            initial_learning_rate = tf.convert_to_tensor(self.initial_learning_rate, name=\"initial_learning_rate\")\n            dtype = initial_learning_rate.dtype\n            decay_steps = tf.cast(self.decay_steps, dtype)\n            decay_rate = tf.cast(self.decay_rate, dtype)",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.optimizers.schedules",
        "documentation": {}
    },
    {
        "label": "CyclicTransformerSchedule",
        "kind": 6,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.optimizers.schedules",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.optimizers.schedules",
        "peekOfCode": "class CyclicTransformerSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n    \"\"\"This callback implements a cyclical learning rate policy (CLR) to the square\n    root decay generally used to train transformers.\n    The method cycles the learning rate around the square root decay LR with an amplitude\n    equal to the target LR with a given period.\n    # Arguments\n        d_model: The dimension of the transformer model.\n        warmup_steps: Warm up steps where the LR increases linearly.\n            Default to 4000 steps.\n        max_lr: Maximum value of the learning rate reachable.",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.optimizers.schedules",
        "documentation": {}
    },
    {
        "label": "evaluate_results",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.utils.app_util",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.utils.app_util",
        "peekOfCode": "def evaluate_results(\n    filepath: str,\n):\n    logger.info(f\"Evaluating result from {filepath} ...\")\n    metrics = {\n        \"greedy_wer\": ErrorRate(wer, name=\"greedy_wer\", dtype=tf.float32),\n        \"greedy_cer\": ErrorRate(cer, name=\"greedy_cer\", dtype=tf.float32),\n        \"beamsearch_wer\": ErrorRate(wer, name=\"beamsearch_wer\", dtype=tf.float32),\n        \"beamsearch_cer\": ErrorRate(cer, name=\"beamsearch_cer\", dtype=tf.float32),\n    }",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.utils.app_util",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.utils.app_util",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.utils.app_util",
        "peekOfCode": "logger = tf.get_logger()\ndef evaluate_results(\n    filepath: str,\n):\n    logger.info(f\"Evaluating result from {filepath} ...\")\n    metrics = {\n        \"greedy_wer\": ErrorRate(wer, name=\"greedy_wer\", dtype=tf.float32),\n        \"greedy_cer\": ErrorRate(cer, name=\"greedy_cer\", dtype=tf.float32),\n        \"beamsearch_wer\": ErrorRate(wer, name=\"beamsearch_wer\", dtype=tf.float32),\n        \"beamsearch_cer\": ErrorRate(cer, name=\"beamsearch_cer\", dtype=tf.float32),",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.utils.app_util",
        "documentation": {}
    },
    {
        "label": "create_inputs",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.utils.data_util",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.utils.data_util",
        "peekOfCode": "def create_inputs(\n    inputs: tf.Tensor,\n    inputs_length: tf.Tensor,\n    predictions: tf.Tensor = None,\n    predictions_length: tf.Tensor = None,\n) -> dict:\n    data = {\n        \"inputs\": inputs,\n        \"inputs_length\": inputs_length,\n    }",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.utils.data_util",
        "documentation": {}
    },
    {
        "label": "create_logits",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.utils.data_util",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.utils.data_util",
        "peekOfCode": "def create_logits(\n    logits: tf.Tensor,\n    logits_length: tf.Tensor,\n) -> dict:\n    return {\"logits\": logits, \"logits_length\": logits_length}\ndef create_labels(\n    labels: tf.Tensor,\n    labels_length: tf.Tensor,\n) -> dict:\n    return {",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.utils.data_util",
        "documentation": {}
    },
    {
        "label": "create_labels",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.utils.data_util",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.utils.data_util",
        "peekOfCode": "def create_labels(\n    labels: tf.Tensor,\n    labels_length: tf.Tensor,\n) -> dict:\n    return {\n        \"labels\": labels,\n        \"labels_length\": labels_length,\n    }",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.utils.data_util",
        "documentation": {}
    },
    {
        "label": "setup_environment",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.utils.env_util",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.utils.env_util",
        "peekOfCode": "def setup_environment():\n    \"\"\"Setting tensorflow running environment\"\"\"\n    warnings.simplefilter(\"ignore\")\n    logger.setLevel(logging.INFO)\n    return logger\ndef setup_devices(devices: List[int], cpu: bool = False):\n    \"\"\"Setting visible devices\n    Args:\n        devices (list): list of visible devices' indices\n    \"\"\"",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.utils.env_util",
        "documentation": {}
    },
    {
        "label": "setup_devices",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.utils.env_util",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.utils.env_util",
        "peekOfCode": "def setup_devices(devices: List[int], cpu: bool = False):\n    \"\"\"Setting visible devices\n    Args:\n        devices (list): list of visible devices' indices\n    \"\"\"\n    if cpu:\n        cpus = tf.config.list_physical_devices(\"CPU\")\n        tf.config.set_visible_devices(cpus, \"CPU\")\n        tf.config.set_visible_devices([], \"GPU\")\n        logger.info(f\"Run on {len(cpus)} Physical CPUs\")",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.utils.env_util",
        "documentation": {}
    },
    {
        "label": "setup_tpu",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.utils.env_util",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.utils.env_util",
        "peekOfCode": "def setup_tpu(tpu_address=None):\n    if tpu_address is None:\n        resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n    else:\n        resolver = tf.distribute.cluster_resolver.TPUClusterResolver(\n            tpu=\"grpc://\" + tpu_address\n        )\n    tf.config.experimental_connect_to_cluster(resolver)\n    tf.tpu.experimental.initialize_tpu_system(resolver)\n    logger.info(f\"All TPUs: {tf.config.list_logical_devices('TPU')}\")",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.utils.env_util",
        "documentation": {}
    },
    {
        "label": "setup_strategy",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.utils.env_util",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.utils.env_util",
        "peekOfCode": "def setup_strategy(devices: List[int], tpu_address: str = None):\n    \"\"\"Setting mirrored strategy for training\n    Args:\n        devices (list): list of visible devices' indices\n        tpu_address (str): an optional custom tpu address\n    Returns:\n        tf.distribute.Strategy: TPUStrategy for training on tpus or MirroredStrategy for training on gpus\n    \"\"\"\n    try:\n        return setup_tpu(tpu_address)",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.utils.env_util",
        "documentation": {}
    },
    {
        "label": "has_devices",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.utils.env_util",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.utils.env_util",
        "peekOfCode": "def has_devices(devices: Union[List[str], str]):\n    if isinstance(devices, list):\n        return all([len(tf.config.list_logical_devices(d)) != 0 for d in devices])\n    return len(tf.config.list_logical_devices(devices)) != 0",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.utils.env_util",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.utils.env_util",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.utils.env_util",
        "peekOfCode": "logger = tf.get_logger()\ndef setup_environment():\n    \"\"\"Setting tensorflow running environment\"\"\"\n    warnings.simplefilter(\"ignore\")\n    logger.setLevel(logging.INFO)\n    return logger\ndef setup_devices(devices: List[int], cpu: bool = False):\n    \"\"\"Setting visible devices\n    Args:\n        devices (list): list of visible devices' indices",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.utils.env_util",
        "documentation": {}
    },
    {
        "label": "float_feature",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.utils.feature_util",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.utils.feature_util",
        "peekOfCode": "def float_feature(\n    list_of_floats,\n):\n    return tf.train.Feature(float_list=tf.train.FloatList(value=list_of_floats))\ndef int64_feature(\n    list_of_ints,\n):\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=list_of_ints))\ndef bytestring_feature(\n    list_of_bytestrings,",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.utils.feature_util",
        "documentation": {}
    },
    {
        "label": "int64_feature",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.utils.feature_util",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.utils.feature_util",
        "peekOfCode": "def int64_feature(\n    list_of_ints,\n):\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=list_of_ints))\ndef bytestring_feature(\n    list_of_bytestrings,\n):\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=list_of_bytestrings))",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.utils.feature_util",
        "documentation": {}
    },
    {
        "label": "bytestring_feature",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.utils.feature_util",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.utils.feature_util",
        "peekOfCode": "def bytestring_feature(\n    list_of_bytestrings,\n):\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=list_of_bytestrings))",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.utils.feature_util",
        "documentation": {}
    },
    {
        "label": "load_yaml",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.utils.file_util",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.utils.file_util",
        "peekOfCode": "def load_yaml(\n    path: str,\n) -> dict:\n    # Fix yaml numbers https://stackoverflow.com/a/30462009/11037553\n    loader = yaml.SafeLoader\n    loader.add_implicit_resolver(\n        u\"tag:yaml.org,2002:float\",\n        re.compile(\n            u\"\"\"^(?:\n         [-+]?(?:[0-9][0-9_]*)\\\\.[0-9_]*(?:[eE][-+]?[0-9]+)?",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.utils.file_util",
        "documentation": {}
    },
    {
        "label": "is_hdf5_filepath",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.utils.file_util",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.utils.file_util",
        "peekOfCode": "def is_hdf5_filepath(\n    filepath: str,\n) -> bool:\n    return (\n        filepath.endswith(\".h5\")\n        or filepath.endswith(\".keras\")\n        or filepath.endswith(\".hdf5\")\n    )\ndef is_cloud_path(\n    path: str,",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.utils.file_util",
        "documentation": {}
    },
    {
        "label": "is_cloud_path",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.utils.file_util",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.utils.file_util",
        "peekOfCode": "def is_cloud_path(\n    path: str,\n) -> bool:\n    \"\"\"Check if the path is on cloud (which requires tf.io.gfile)\n    Args:\n        path (str): Path to directory or file\n    Returns:\n        bool: True if path is on cloud, False otherwise\n    \"\"\"\n    return bool(re.match(r\"^[a-z]+://\", path))",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.utils.file_util",
        "documentation": {}
    },
    {
        "label": "preprocess_paths",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.utils.file_util",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.utils.file_util",
        "peekOfCode": "def preprocess_paths(\n    paths: Union[List[str], str],\n    isdir: bool = False,\n) -> Union[List[str], str]:\n    \"\"\"Expand the path to the root \"/\" and makedirs\n    Args:\n        paths (Union[List, str]): A path or list of paths\n    Returns:\n        Union[List, str]: A processed path or list of paths, return None if it's not path\n    \"\"\"",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.utils.file_util",
        "documentation": {}
    },
    {
        "label": "save_file",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.utils.file_util",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.utils.file_util",
        "peekOfCode": "def save_file(\n    filepath: str,\n):\n    if is_cloud_path(filepath) and is_hdf5_filepath(filepath):\n        _, ext = os.path.splitext(filepath)\n        with tempfile.NamedTemporaryFile(suffix=ext) as tmp:\n            yield tmp.name\n            tf.io.gfile.copy(tmp.name, filepath, overwrite=True)\n    else:\n        yield filepath",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.utils.file_util",
        "documentation": {}
    },
    {
        "label": "read_file",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.utils.file_util",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.utils.file_util",
        "peekOfCode": "def read_file(\n    filepath: str,\n):\n    if is_cloud_path(filepath) and is_hdf5_filepath(filepath):\n        _, ext = os.path.splitext(filepath)\n        with tempfile.NamedTemporaryFile(suffix=ext) as tmp:\n            tf.io.gfile.copy(filepath, tmp.name, overwrite=True)\n            yield tmp.name\n    else:\n        yield filepath",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.utils.file_util",
        "documentation": {}
    },
    {
        "label": "get_rnn",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.utils.layer_util",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.utils.layer_util",
        "peekOfCode": "def get_rnn(\n    rnn_type: str,\n):\n    assert rnn_type in [\"lstm\", \"gru\", \"rnn\"]\n    if rnn_type == \"lstm\":\n        return tf.keras.layers.LSTM\n    if rnn_type == \"gru\":\n        return tf.keras.layers.GRU\n    return tf.keras.layers.SimpleRNN\ndef get_conv(",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.utils.layer_util",
        "documentation": {}
    },
    {
        "label": "get_conv",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.utils.layer_util",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.utils.layer_util",
        "peekOfCode": "def get_conv(\n    conv_type: str,\n):\n    assert conv_type in [\"conv1d\", \"conv2d\"]\n    if conv_type == \"conv1d\":\n        return tf.keras.layers.Conv1D\n    return tf.keras.layers.Conv2D",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.utils.layer_util",
        "documentation": {}
    },
    {
        "label": "log10",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.utils.math_util",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.utils.math_util",
        "peekOfCode": "def log10(x):\n    numerator = tf.math.log(x)\n    denominator = tf.math.log(tf.constant(10, dtype=numerator.dtype))\n    return numerator / denominator\ndef get_num_batches(\n    nsamples,\n    batch_size,\n    drop_remainders=True,\n):\n    if nsamples is None or batch_size is None:",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.utils.math_util",
        "documentation": {}
    },
    {
        "label": "get_num_batches",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.utils.math_util",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.utils.math_util",
        "peekOfCode": "def get_num_batches(\n    nsamples,\n    batch_size,\n    drop_remainders=True,\n):\n    if nsamples is None or batch_size is None:\n        return None\n    if drop_remainders:\n        return math.floor(float(nsamples) / float(batch_size))\n    return math.ceil(float(nsamples) / float(batch_size))",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.utils.math_util",
        "documentation": {}
    },
    {
        "label": "nan_to_zero",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.utils.math_util",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.utils.math_util",
        "peekOfCode": "def nan_to_zero(\n    input_tensor: tf.Tensor,\n):\n    return tf.where(tf.math.is_nan(input_tensor), tf.zeros_like(input_tensor), input_tensor)\ndef bytes_to_string(\n    array: np.ndarray,\n    encoding: str = \"utf-8\",\n):\n    if array is None:\n        return None",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.utils.math_util",
        "documentation": {}
    },
    {
        "label": "bytes_to_string",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.utils.math_util",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.utils.math_util",
        "peekOfCode": "def bytes_to_string(\n    array: np.ndarray,\n    encoding: str = \"utf-8\",\n):\n    if array is None:\n        return None\n    return [transcript.decode(encoding) for transcript in array]\ndef get_reduced_length(\n    length,\n    reduction_factor,",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.utils.math_util",
        "documentation": {}
    },
    {
        "label": "get_reduced_length",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.utils.math_util",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.utils.math_util",
        "peekOfCode": "def get_reduced_length(\n    length,\n    reduction_factor,\n):\n    return tf.cast(\n        tf.math.ceil(tf.divide(length, tf.cast(reduction_factor, dtype=length.dtype))),\n        dtype=tf.int32,\n    )\ndef count_non_blank(\n    tensor: tf.Tensor,",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.utils.math_util",
        "documentation": {}
    },
    {
        "label": "count_non_blank",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.utils.math_util",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.utils.math_util",
        "peekOfCode": "def count_non_blank(\n    tensor: tf.Tensor,\n    blank: int or tf.Tensor = 0,\n    axis=None,\n):\n    return tf.reduce_sum(\n        tf.where(tf.not_equal(tensor, blank), x=tf.ones_like(tensor), y=tf.zeros_like(tensor)),\n        axis=axis,\n    )\ndef merge_two_last_dims(x):",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.utils.math_util",
        "documentation": {}
    },
    {
        "label": "merge_two_last_dims",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.utils.math_util",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.utils.math_util",
        "peekOfCode": "def merge_two_last_dims(x):\n    b, _, f, c = shape_util.shape_list(x)\n    return tf.reshape(x, shape=[b, -1, f * c])\ndef merge_repeated(\n    yseqs,\n    blank=0,\n):\n    result = tf.reshape(yseqs[0], [1])\n    U = shape_util.shape_list(yseqs)[0]\n    i = tf.constant(1, dtype=tf.int32)",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.utils.math_util",
        "documentation": {}
    },
    {
        "label": "merge_repeated",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.utils.math_util",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.utils.math_util",
        "peekOfCode": "def merge_repeated(\n    yseqs,\n    blank=0,\n):\n    result = tf.reshape(yseqs[0], [1])\n    U = shape_util.shape_list(yseqs)[0]\n    i = tf.constant(1, dtype=tf.int32)\n    def _cond(i, result, yseqs, U):\n        return tf.less(i, U)\n    def _body(i, result, yseqs, U):",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.utils.math_util",
        "documentation": {}
    },
    {
        "label": "find_max_length_prediction_tfarray",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.utils.math_util",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.utils.math_util",
        "peekOfCode": "def find_max_length_prediction_tfarray(\n    tfarray: tf.TensorArray,\n) -> tf.Tensor:\n    with tf.name_scope(\"find_max_length_prediction_tfarray\"):\n        index = tf.constant(0, dtype=tf.int32)\n        total = tfarray.size()\n        max_length = tf.constant(0, dtype=tf.int32)\n        def condition(index, _):\n            return tf.less(index, total)\n        def body(index, max_length):",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.utils.math_util",
        "documentation": {}
    },
    {
        "label": "pad_prediction_tfarray",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.utils.math_util",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.utils.math_util",
        "peekOfCode": "def pad_prediction_tfarray(\n    tfarray: tf.TensorArray,\n    blank: int or tf.Tensor,\n) -> tf.TensorArray:\n    with tf.name_scope(\"pad_prediction_tfarray\"):\n        index = tf.constant(0, dtype=tf.int32)\n        total = tfarray.size()\n        max_length = find_max_length_prediction_tfarray(tfarray) + 1\n        def condition(index, _):\n            return tf.less(index, total)",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.utils.math_util",
        "documentation": {}
    },
    {
        "label": "execute_wer",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.utils.metric_util",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.utils.metric_util",
        "peekOfCode": "def execute_wer(\n    decode,\n    target,\n):\n    decode = math_util.bytes_to_string(decode)\n    target = math_util.bytes_to_string(target)\n    dis = 0.0\n    length = 0.0\n    for dec, tar in zip(decode, target):\n        words = set(dec.split() + tar.split())",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.utils.metric_util",
        "documentation": {}
    },
    {
        "label": "wer",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.utils.metric_util",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.utils.metric_util",
        "peekOfCode": "def wer(\n    decode: tf.Tensor,\n    target: tf.Tensor,\n) -> Tuple[tf.Tensor, tf.Tensor]:\n    \"\"\"Word Error Rate\n    Args:\n        decode (np.ndarray): array of prediction texts\n        target (np.ndarray): array of groundtruth texts\n    Returns:\n        tuple: a tuple of tf.Tensor of (edit distances, number of words) of each text",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.utils.metric_util",
        "documentation": {}
    },
    {
        "label": "execute_cer",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.utils.metric_util",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.utils.metric_util",
        "peekOfCode": "def execute_cer(decode, target):\n    decode = math_util.bytes_to_string(decode)\n    target = math_util.bytes_to_string(target)\n    dis = 0\n    length = 0\n    for dec, tar in zip(decode, target):\n        dis += distance.edit_distance(dec, tar)\n        length += len(tar)\n    return tf.convert_to_tensor(dis, tf.float32), tf.convert_to_tensor(length, tf.float32)\ndef cer(",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.utils.metric_util",
        "documentation": {}
    },
    {
        "label": "cer",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.utils.metric_util",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.utils.metric_util",
        "peekOfCode": "def cer(\n    decode: tf.Tensor,\n    target: tf.Tensor,\n) -> Tuple[tf.Tensor, tf.Tensor]:\n    \"\"\"Character Error Rate\n    Args:\n        decode (np.ndarray): array of prediction texts\n        target (np.ndarray): array of groundtruth texts\n    Returns:\n        tuple: a tuple of tf.Tensor of (edit distances, number of characters) of each text",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.utils.metric_util",
        "documentation": {}
    },
    {
        "label": "tf_cer",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.utils.metric_util",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.utils.metric_util",
        "peekOfCode": "def tf_cer(\n    decode: tf.Tensor,\n    target: tf.Tensor,\n) -> Tuple[tf.Tensor, tf.Tensor]:\n    \"\"\"Tensorflwo Charactor Error rate\n    Args:\n        decoder (tf.Tensor): tensor shape [B]\n        target (tf.Tensor): tensor shape [B]\n    Returns:\n        tuple: a tuple of tf.Tensor of (edit distances, number of characters) of each text",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.utils.metric_util",
        "documentation": {}
    },
    {
        "label": "shape_list",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.utils.shape_util",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.utils.shape_util",
        "peekOfCode": "def shape_list(x, out_type=tf.int32):\n    \"\"\"Deal with dynamic shape in tensorflow cleanly.\"\"\"\n    static = x.shape.as_list()\n    dynamic = tf.shape(x, out_type=out_type)\n    return [dynamic[i] if s is None else s for i, s in enumerate(static)]\ndef get_shape_invariants(tensor):\n    shapes = shape_list(tensor)\n    return tf.TensorShape([i if isinstance(i, int) else None for i in shapes])\ndef get_float_spec(tensor):\n    shape = get_shape_invariants(tensor)",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.utils.shape_util",
        "documentation": {}
    },
    {
        "label": "get_shape_invariants",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.utils.shape_util",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.utils.shape_util",
        "peekOfCode": "def get_shape_invariants(tensor):\n    shapes = shape_list(tensor)\n    return tf.TensorShape([i if isinstance(i, int) else None for i in shapes])\ndef get_float_spec(tensor):\n    shape = get_shape_invariants(tensor)\n    return tf.TensorSpec(shape, dtype=tf.float32)",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.utils.shape_util",
        "documentation": {}
    },
    {
        "label": "get_float_spec",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.tensorflow_asr.utils.shape_util",
        "description": "Demo.EMSConformer.inference.tensorflow_asr.utils.shape_util",
        "peekOfCode": "def get_float_spec(tensor):\n    shape = get_shape_invariants(tensor)\n    return tf.TensorSpec(shape, dtype=tf.float32)",
        "detail": "Demo.EMSConformer.inference.tensorflow_asr.utils.shape_util",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.amran_run_tflite_model",
        "description": "Demo.EMSConformer.inference.amran_run_tflite_model",
        "peekOfCode": "def main(\n    #filename: str = \"/home/liuyi/TensorFlowASR/dataset/LibriSpeech/test-clean/5639/40744/5639-40744-0008.flac\",\n    filename: str = \"/home/liuyi/audio_data/sample100/signs_symptoms_audio_concatenated/sss81.wav\",\n    tflite: str = \"./tflite_models/pretrained_librispeech_train_ss_test_concatenated_epoch50_noOptimize.tflite\",\n#    tflite: str = \"./tflite_models/pretrained_librispeech_train_ss_test_concatenated_epoch50.tflite\",\n#    tflite: str = \"./tflite_models/subsampling-conformer.latest.tflite\",\n#    tflite: str = \"\",\n    blank: int = 0,\n    num_rnns: int = 1,\n    nstates: int = 2,",
        "detail": "Demo.EMSConformer.inference.amran_run_tflite_model",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.gen_saved_model",
        "description": "Demo.EMSConformer.inference.gen_saved_model",
        "peekOfCode": "def main(\n    config: str = DEFAULT_YAML,\n    h5: str = None,\n    sentence_piece: bool = False,\n    subwords: bool = False,\n    output_dir: str = None,\n):\n    assert h5 and output_dir\n    config = Config(config)\n    tf.random.set_seed(0)",
        "detail": "Demo.EMSConformer.inference.gen_saved_model",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "Demo.EMSConformer.inference.gen_saved_model",
        "description": "Demo.EMSConformer.inference.gen_saved_model",
        "peekOfCode": "logger = env_util.setup_environment()\nimport tensorflow as tf\nDEFAULT_YAML = os.path.join(os.path.abspath(os.path.dirname(__file__)), \"config.yml\")\nfrom tensorflow_asr.configs.config import Config\nfrom tensorflow_asr.helpers import featurizer_helpers\nfrom tensorflow_asr.models.transducer.conformer import Conformer\ndef main(\n    config: str = DEFAULT_YAML,\n    h5: str = None,\n    sentence_piece: bool = False,",
        "detail": "Demo.EMSConformer.inference.gen_saved_model",
        "documentation": {}
    },
    {
        "label": "DEFAULT_YAML",
        "kind": 5,
        "importPath": "Demo.EMSConformer.inference.gen_saved_model",
        "description": "Demo.EMSConformer.inference.gen_saved_model",
        "peekOfCode": "DEFAULT_YAML = os.path.join(os.path.abspath(os.path.dirname(__file__)), \"config.yml\")\nfrom tensorflow_asr.configs.config import Config\nfrom tensorflow_asr.helpers import featurizer_helpers\nfrom tensorflow_asr.models.transducer.conformer import Conformer\ndef main(\n    config: str = DEFAULT_YAML,\n    h5: str = None,\n    sentence_piece: bool = False,\n    subwords: bool = False,\n    output_dir: str = None,",
        "detail": "Demo.EMSConformer.inference.gen_saved_model",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.gen_tflite_model",
        "description": "Demo.EMSConformer.inference.gen_tflite_model",
        "peekOfCode": "def main(\n    config: str = DEFAULT_YAML,\n#    config: str = \"./h5_models/subword-conformer-config.yml\",\n    h5: str = None,\n    subwords: bool = True,\n    sentence_piece: bool = False,\n    output: str = None,\n):\n    assert h5 and output\n    tf.keras.backend.clear_session()",
        "detail": "Demo.EMSConformer.inference.gen_tflite_model",
        "documentation": {}
    },
    {
        "label": "os.environ['CUDA_VISIBLE_DEVICES']",
        "kind": 5,
        "importPath": "Demo.EMSConformer.inference.gen_tflite_model",
        "description": "Demo.EMSConformer.inference.gen_tflite_model",
        "peekOfCode": "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n#devices = [-1]\n#gpus = tf.config.list_physical_devices(\"GPU\")\n#visible_gpus = [-1]\n#visible_gpus = [gpus[i] for i in devices]\n#tf.config.set_visible_devices(visible_gpus, \"GPU\")\n#strategy = tf.distribute.MirroredStrategy()\nimport fire\nfrom tensorflow_asr.utils import env_util\nlogger = env_util.setup_environment()",
        "detail": "Demo.EMSConformer.inference.gen_tflite_model",
        "documentation": {}
    },
    {
        "label": "#devices",
        "kind": 5,
        "importPath": "Demo.EMSConformer.inference.gen_tflite_model",
        "description": "Demo.EMSConformer.inference.gen_tflite_model",
        "peekOfCode": "#devices = [-1]\n#gpus = tf.config.list_physical_devices(\"GPU\")\n#visible_gpus = [-1]\n#visible_gpus = [gpus[i] for i in devices]\n#tf.config.set_visible_devices(visible_gpus, \"GPU\")\n#strategy = tf.distribute.MirroredStrategy()\nimport fire\nfrom tensorflow_asr.utils import env_util\nlogger = env_util.setup_environment()\n#import tensorflow as tf",
        "detail": "Demo.EMSConformer.inference.gen_tflite_model",
        "documentation": {}
    },
    {
        "label": "#gpus",
        "kind": 5,
        "importPath": "Demo.EMSConformer.inference.gen_tflite_model",
        "description": "Demo.EMSConformer.inference.gen_tflite_model",
        "peekOfCode": "#gpus = tf.config.list_physical_devices(\"GPU\")\n#visible_gpus = [-1]\n#visible_gpus = [gpus[i] for i in devices]\n#tf.config.set_visible_devices(visible_gpus, \"GPU\")\n#strategy = tf.distribute.MirroredStrategy()\nimport fire\nfrom tensorflow_asr.utils import env_util\nlogger = env_util.setup_environment()\n#import tensorflow as tf\nfrom tensorflow_asr.configs.config import Config",
        "detail": "Demo.EMSConformer.inference.gen_tflite_model",
        "documentation": {}
    },
    {
        "label": "#visible_gpus",
        "kind": 5,
        "importPath": "Demo.EMSConformer.inference.gen_tflite_model",
        "description": "Demo.EMSConformer.inference.gen_tflite_model",
        "peekOfCode": "#visible_gpus = [-1]\n#visible_gpus = [gpus[i] for i in devices]\n#tf.config.set_visible_devices(visible_gpus, \"GPU\")\n#strategy = tf.distribute.MirroredStrategy()\nimport fire\nfrom tensorflow_asr.utils import env_util\nlogger = env_util.setup_environment()\n#import tensorflow as tf\nfrom tensorflow_asr.configs.config import Config\nfrom tensorflow_asr.helpers import exec_helpers, featurizer_helpers",
        "detail": "Demo.EMSConformer.inference.gen_tflite_model",
        "documentation": {}
    },
    {
        "label": "#visible_gpus",
        "kind": 5,
        "importPath": "Demo.EMSConformer.inference.gen_tflite_model",
        "description": "Demo.EMSConformer.inference.gen_tflite_model",
        "peekOfCode": "#visible_gpus = [gpus[i] for i in devices]\n#tf.config.set_visible_devices(visible_gpus, \"GPU\")\n#strategy = tf.distribute.MirroredStrategy()\nimport fire\nfrom tensorflow_asr.utils import env_util\nlogger = env_util.setup_environment()\n#import tensorflow as tf\nfrom tensorflow_asr.configs.config import Config\nfrom tensorflow_asr.helpers import exec_helpers, featurizer_helpers\nfrom tensorflow_asr.models.transducer.conformer import Conformer",
        "detail": "Demo.EMSConformer.inference.gen_tflite_model",
        "documentation": {}
    },
    {
        "label": "#strategy",
        "kind": 5,
        "importPath": "Demo.EMSConformer.inference.gen_tflite_model",
        "description": "Demo.EMSConformer.inference.gen_tflite_model",
        "peekOfCode": "#strategy = tf.distribute.MirroredStrategy()\nimport fire\nfrom tensorflow_asr.utils import env_util\nlogger = env_util.setup_environment()\n#import tensorflow as tf\nfrom tensorflow_asr.configs.config import Config\nfrom tensorflow_asr.helpers import exec_helpers, featurizer_helpers\nfrom tensorflow_asr.models.transducer.conformer import Conformer\nDEFAULT_YAML = os.path.join(os.path.abspath(os.path.dirname(__file__)), \"config.yml\")\nfrom datetime import datetime",
        "detail": "Demo.EMSConformer.inference.gen_tflite_model",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "Demo.EMSConformer.inference.gen_tflite_model",
        "description": "Demo.EMSConformer.inference.gen_tflite_model",
        "peekOfCode": "logger = env_util.setup_environment()\n#import tensorflow as tf\nfrom tensorflow_asr.configs.config import Config\nfrom tensorflow_asr.helpers import exec_helpers, featurizer_helpers\nfrom tensorflow_asr.models.transducer.conformer import Conformer\nDEFAULT_YAML = os.path.join(os.path.abspath(os.path.dirname(__file__)), \"config.yml\")\nfrom datetime import datetime\nimport argparse\ndef main(\n    config: str = DEFAULT_YAML,",
        "detail": "Demo.EMSConformer.inference.gen_tflite_model",
        "documentation": {}
    },
    {
        "label": "DEFAULT_YAML",
        "kind": 5,
        "importPath": "Demo.EMSConformer.inference.gen_tflite_model",
        "description": "Demo.EMSConformer.inference.gen_tflite_model",
        "peekOfCode": "DEFAULT_YAML = os.path.join(os.path.abspath(os.path.dirname(__file__)), \"config.yml\")\nfrom datetime import datetime\nimport argparse\ndef main(\n    config: str = DEFAULT_YAML,\n#    config: str = \"./h5_models/subword-conformer-config.yml\",\n    h5: str = None,\n    subwords: bool = True,\n    sentence_piece: bool = False,\n    output: str = None,",
        "detail": "Demo.EMSConformer.inference.gen_tflite_model",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.gen_tflite_model_easy",
        "description": "Demo.EMSConformer.inference.gen_tflite_model_easy",
        "peekOfCode": "def main(\n    config: str = DEFAULT_YAML,\n#    config: str = \"./h5_models/subword-conformer-config.yml\",\n    h5: str = None,\n    subwords: bool = True,\n    sentence_piece: bool = False,\n    output: str = None,\n):\n    assert h5 and output\n    tf.keras.backend.clear_session()",
        "detail": "Demo.EMSConformer.inference.gen_tflite_model_easy",
        "documentation": {}
    },
    {
        "label": "os.environ['CUDA_VISIBLE_DEVICES']",
        "kind": 5,
        "importPath": "Demo.EMSConformer.inference.gen_tflite_model_easy",
        "description": "Demo.EMSConformer.inference.gen_tflite_model_easy",
        "peekOfCode": "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n#devices = [-1]\n#gpus = tf.config.list_physical_devices(\"GPU\")\n#visible_gpus = [-1]\n#visible_gpus = [gpus[i] for i in devices]\n#tf.config.set_visible_devices(visible_gpus, \"GPU\")\n#strategy = tf.distribute.MirroredStrategy()\nimport fire\nfrom tensorflow_asr.utils import env_util\nlogger = env_util.setup_environment()",
        "detail": "Demo.EMSConformer.inference.gen_tflite_model_easy",
        "documentation": {}
    },
    {
        "label": "#devices",
        "kind": 5,
        "importPath": "Demo.EMSConformer.inference.gen_tflite_model_easy",
        "description": "Demo.EMSConformer.inference.gen_tflite_model_easy",
        "peekOfCode": "#devices = [-1]\n#gpus = tf.config.list_physical_devices(\"GPU\")\n#visible_gpus = [-1]\n#visible_gpus = [gpus[i] for i in devices]\n#tf.config.set_visible_devices(visible_gpus, \"GPU\")\n#strategy = tf.distribute.MirroredStrategy()\nimport fire\nfrom tensorflow_asr.utils import env_util\nlogger = env_util.setup_environment()\n#import tensorflow as tf",
        "detail": "Demo.EMSConformer.inference.gen_tflite_model_easy",
        "documentation": {}
    },
    {
        "label": "#gpus",
        "kind": 5,
        "importPath": "Demo.EMSConformer.inference.gen_tflite_model_easy",
        "description": "Demo.EMSConformer.inference.gen_tflite_model_easy",
        "peekOfCode": "#gpus = tf.config.list_physical_devices(\"GPU\")\n#visible_gpus = [-1]\n#visible_gpus = [gpus[i] for i in devices]\n#tf.config.set_visible_devices(visible_gpus, \"GPU\")\n#strategy = tf.distribute.MirroredStrategy()\nimport fire\nfrom tensorflow_asr.utils import env_util\nlogger = env_util.setup_environment()\n#import tensorflow as tf\nfrom tensorflow_asr.configs.config import Config",
        "detail": "Demo.EMSConformer.inference.gen_tflite_model_easy",
        "documentation": {}
    },
    {
        "label": "#visible_gpus",
        "kind": 5,
        "importPath": "Demo.EMSConformer.inference.gen_tflite_model_easy",
        "description": "Demo.EMSConformer.inference.gen_tflite_model_easy",
        "peekOfCode": "#visible_gpus = [-1]\n#visible_gpus = [gpus[i] for i in devices]\n#tf.config.set_visible_devices(visible_gpus, \"GPU\")\n#strategy = tf.distribute.MirroredStrategy()\nimport fire\nfrom tensorflow_asr.utils import env_util\nlogger = env_util.setup_environment()\n#import tensorflow as tf\nfrom tensorflow_asr.configs.config import Config\nfrom tensorflow_asr.helpers import exec_helpers, featurizer_helpers",
        "detail": "Demo.EMSConformer.inference.gen_tflite_model_easy",
        "documentation": {}
    },
    {
        "label": "#visible_gpus",
        "kind": 5,
        "importPath": "Demo.EMSConformer.inference.gen_tflite_model_easy",
        "description": "Demo.EMSConformer.inference.gen_tflite_model_easy",
        "peekOfCode": "#visible_gpus = [gpus[i] for i in devices]\n#tf.config.set_visible_devices(visible_gpus, \"GPU\")\n#strategy = tf.distribute.MirroredStrategy()\nimport fire\nfrom tensorflow_asr.utils import env_util\nlogger = env_util.setup_environment()\n#import tensorflow as tf\nfrom tensorflow_asr.configs.config import Config\nfrom tensorflow_asr.helpers import exec_helpers, featurizer_helpers\nfrom tensorflow_asr.models.transducer.conformer import Conformer",
        "detail": "Demo.EMSConformer.inference.gen_tflite_model_easy",
        "documentation": {}
    },
    {
        "label": "#strategy",
        "kind": 5,
        "importPath": "Demo.EMSConformer.inference.gen_tflite_model_easy",
        "description": "Demo.EMSConformer.inference.gen_tflite_model_easy",
        "peekOfCode": "#strategy = tf.distribute.MirroredStrategy()\nimport fire\nfrom tensorflow_asr.utils import env_util\nlogger = env_util.setup_environment()\n#import tensorflow as tf\nfrom tensorflow_asr.configs.config import Config\nfrom tensorflow_asr.helpers import exec_helpers, featurizer_helpers\nfrom tensorflow_asr.models.transducer.conformer import Conformer\nDEFAULT_YAML = os.path.join(os.path.abspath(os.path.dirname(__file__)), \"config.yml\")\nfrom datetime import datetime",
        "detail": "Demo.EMSConformer.inference.gen_tflite_model_easy",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "Demo.EMSConformer.inference.gen_tflite_model_easy",
        "description": "Demo.EMSConformer.inference.gen_tflite_model_easy",
        "peekOfCode": "logger = env_util.setup_environment()\n#import tensorflow as tf\nfrom tensorflow_asr.configs.config import Config\nfrom tensorflow_asr.helpers import exec_helpers, featurizer_helpers\nfrom tensorflow_asr.models.transducer.conformer import Conformer\nDEFAULT_YAML = os.path.join(os.path.abspath(os.path.dirname(__file__)), \"config.yml\")\nfrom datetime import datetime\nimport argparse\ndef main(\n    config: str = DEFAULT_YAML,",
        "detail": "Demo.EMSConformer.inference.gen_tflite_model_easy",
        "documentation": {}
    },
    {
        "label": "DEFAULT_YAML",
        "kind": 5,
        "importPath": "Demo.EMSConformer.inference.gen_tflite_model_easy",
        "description": "Demo.EMSConformer.inference.gen_tflite_model_easy",
        "peekOfCode": "DEFAULT_YAML = os.path.join(os.path.abspath(os.path.dirname(__file__)), \"config.yml\")\nfrom datetime import datetime\nimport argparse\ndef main(\n    config: str = DEFAULT_YAML,\n#    config: str = \"./h5_models/subword-conformer-config.yml\",\n    h5: str = None,\n    subwords: bool = True,\n    sentence_piece: bool = False,\n    output: str = None,",
        "detail": "Demo.EMSConformer.inference.gen_tflite_model_easy",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.run_saved_model",
        "description": "Demo.EMSConformer.inference.run_saved_model",
        "peekOfCode": "def main(\n    saved_model: str = None,\n    filename: str = None,\n):\n    tf.keras.backend.clear_session()\n    module = tf.saved_model.load(export_dir=saved_model)\n    signal = read_raw_audio(filename)\n    transcript = module.pred(signal)\n    print(\"Transcript: \", \"\".join([chr(u) for u in transcript]))\nif __name__ == \"__main__\":",
        "detail": "Demo.EMSConformer.inference.run_saved_model",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "Demo.EMSConformer.inference.run_saved_model",
        "description": "Demo.EMSConformer.inference.run_saved_model",
        "peekOfCode": "logger = env_util.setup_environment()\nimport tensorflow as tf\nDEFAULT_YAML = os.path.join(os.path.abspath(os.path.dirname(__file__)), \"config.yml\")\nfrom tensorflow_asr.featurizers.speech_featurizers import read_raw_audio\ndef main(\n    saved_model: str = None,\n    filename: str = None,\n):\n    tf.keras.backend.clear_session()\n    module = tf.saved_model.load(export_dir=saved_model)",
        "detail": "Demo.EMSConformer.inference.run_saved_model",
        "documentation": {}
    },
    {
        "label": "DEFAULT_YAML",
        "kind": 5,
        "importPath": "Demo.EMSConformer.inference.run_saved_model",
        "description": "Demo.EMSConformer.inference.run_saved_model",
        "peekOfCode": "DEFAULT_YAML = os.path.join(os.path.abspath(os.path.dirname(__file__)), \"config.yml\")\nfrom tensorflow_asr.featurizers.speech_featurizers import read_raw_audio\ndef main(\n    saved_model: str = None,\n    filename: str = None,\n):\n    tf.keras.backend.clear_session()\n    module = tf.saved_model.load(export_dir=saved_model)\n    signal = read_raw_audio(filename)\n    transcript = module.pred(signal)",
        "detail": "Demo.EMSConformer.inference.run_saved_model",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.run_tflite_model",
        "description": "Demo.EMSConformer.inference.run_tflite_model",
        "peekOfCode": "def main(\n#    filename: str = \"/home/liuyi/TensorFlowASR/dataset/LibriSpeech/test-clean/5639/40744/5639-40744-0008.flac\",\n    filename: str = \"/home/liuyi/audio_data/sample100/signs_symptoms_audio_concatenated/sss81.wav\",\n    tflite: str = \"./tflite_models/pretrained_librispeech_train_ss_test_concatenated_epoch50_noOptimize.tflite\",\n#    tflite: str = \"./tflite_models/pretrained_librispeech_train_ss_test_concatenated_epoch50.tflite\",\n#    tflite: str = \"./tflite_models/pretrained_librispeech_train_ss_test_concatenated_epoch50_noSELECT_TF_OPS.tflite\",\n#    tflite: str = \"./tflite_models/subsampling-conformer.latest.tflite\",\n#    tflite: str = \"\",\n    blank: int = 0,\n    num_rnns: int = 1,",
        "detail": "Demo.EMSConformer.inference.run_tflite_model",
        "documentation": {}
    },
    {
        "label": "writeListFile",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.run_tflite_model_e2e_eval",
        "description": "Demo.EMSConformer.inference.run_tflite_model_e2e_eval",
        "peekOfCode": "def writeListFile(file_path, output_list, encoding = None):\n    f = open(file_path, mode = \"w\")\n    output_str = \"\\n\".join(output_list)\n    f.write(output_str)\n    f.close()\ndef write2DListFile(file_path, output_list, line_sep = \" \"):\n    str_list = []\n    for out_line in output_list:\n        str_line = []\n        for e in out_line:",
        "detail": "Demo.EMSConformer.inference.run_tflite_model_e2e_eval",
        "documentation": {}
    },
    {
        "label": "write2DListFile",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.run_tflite_model_e2e_eval",
        "description": "Demo.EMSConformer.inference.run_tflite_model_e2e_eval",
        "peekOfCode": "def write2DListFile(file_path, output_list, line_sep = \" \"):\n    str_list = []\n    for out_line in output_list:\n        str_line = []\n        for e in out_line:\n            str_line.append(str(e))\n        str_list.append(str_line)\n    out_list = list(map(line_sep.join, str_list))\n    writeListFile(file_path, out_list)\ndef readFile(file_path, encoding = None):",
        "detail": "Demo.EMSConformer.inference.run_tflite_model_e2e_eval",
        "documentation": {}
    },
    {
        "label": "readFile",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.run_tflite_model_e2e_eval",
        "description": "Demo.EMSConformer.inference.run_tflite_model_e2e_eval",
        "peekOfCode": "def readFile(file_path, encoding = None):\n    f = open(file_path, 'r')\n    lines = f.read().splitlines()\n#    print(lines[0])\n    res = []\n    for line in lines:\n        line = line.strip()\n        res.append(line)\n    f.close()\n    return res",
        "detail": "Demo.EMSConformer.inference.run_tflite_model_e2e_eval",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.run_tflite_model_e2e_eval",
        "description": "Demo.EMSConformer.inference.run_tflite_model_e2e_eval",
        "peekOfCode": "def main(\n#    filename: str = \"/home/liuyi/TensorFlowASR/dataset/LibriSpeech/test-clean/5639/40744/5639-40744-0008.flac\",\n#    filename: str = \"/home/liuyi/audio_data/radu0.m4a\",\n    tflite: str = \"./tflite_models/pretrained_librispeech_train_ss_test_concatenated_epoch50_noOptimize.tflite\", # +++++ .tflite model\n#    tflite: str = \"./tflite_models/subsampling-conformer.latest.tflite\",\n#    tflite: str = \"\",\n    blank: int = 0,\n    num_rnns: int = 1,\n    nstates: int = 2,\n    statesize: int = 320,",
        "detail": "Demo.EMSConformer.inference.run_tflite_model_e2e_eval",
        "documentation": {}
    },
    {
        "label": "os.environ['CUDA_VISIBLE_DEVICES']",
        "kind": 5,
        "importPath": "Demo.EMSConformer.inference.run_tflite_model_e2e_eval",
        "description": "Demo.EMSConformer.inference.run_tflite_model_e2e_eval",
        "peekOfCode": "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\nimport tensorflow as tf\nfrom tqdm import tqdm\nfrom tensorflow_asr.metrics.error_rates import ErrorRate\nfrom tensorflow_asr.utils.file_util import read_file\nfrom tensorflow_asr.utils.metric_util import cer, wer\nlogger = tf.get_logger()\n#devices = [2]\n#gpus = tf.config.list_physical_devices(\"GPU\")\n#visible_gpus = [gpus[i] for i in devices]",
        "detail": "Demo.EMSConformer.inference.run_tflite_model_e2e_eval",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "Demo.EMSConformer.inference.run_tflite_model_e2e_eval",
        "description": "Demo.EMSConformer.inference.run_tflite_model_e2e_eval",
        "peekOfCode": "logger = tf.get_logger()\n#devices = [2]\n#gpus = tf.config.list_physical_devices(\"GPU\")\n#visible_gpus = [gpus[i] for i in devices]\n#tf.config.set_visible_devices(visible_gpus, \"GPU\")\n#strategy = tf.distribute.MirroredStrategy()\nimport fire\n#import tensorflow as tf\nfrom tensorflow_asr.utils.file_util import read_file\nimport json",
        "detail": "Demo.EMSConformer.inference.run_tflite_model_e2e_eval",
        "documentation": {}
    },
    {
        "label": "#devices",
        "kind": 5,
        "importPath": "Demo.EMSConformer.inference.run_tflite_model_e2e_eval",
        "description": "Demo.EMSConformer.inference.run_tflite_model_e2e_eval",
        "peekOfCode": "#devices = [2]\n#gpus = tf.config.list_physical_devices(\"GPU\")\n#visible_gpus = [gpus[i] for i in devices]\n#tf.config.set_visible_devices(visible_gpus, \"GPU\")\n#strategy = tf.distribute.MirroredStrategy()\nimport fire\n#import tensorflow as tf\nfrom tensorflow_asr.utils.file_util import read_file\nimport json\nfrom pydub import AudioSegment",
        "detail": "Demo.EMSConformer.inference.run_tflite_model_e2e_eval",
        "documentation": {}
    },
    {
        "label": "#gpus",
        "kind": 5,
        "importPath": "Demo.EMSConformer.inference.run_tflite_model_e2e_eval",
        "description": "Demo.EMSConformer.inference.run_tflite_model_e2e_eval",
        "peekOfCode": "#gpus = tf.config.list_physical_devices(\"GPU\")\n#visible_gpus = [gpus[i] for i in devices]\n#tf.config.set_visible_devices(visible_gpus, \"GPU\")\n#strategy = tf.distribute.MirroredStrategy()\nimport fire\n#import tensorflow as tf\nfrom tensorflow_asr.utils.file_util import read_file\nimport json\nfrom pydub import AudioSegment\nfrom tensorflow_asr.featurizers.speech_featurizers import read_raw_audio",
        "detail": "Demo.EMSConformer.inference.run_tflite_model_e2e_eval",
        "documentation": {}
    },
    {
        "label": "#visible_gpus",
        "kind": 5,
        "importPath": "Demo.EMSConformer.inference.run_tflite_model_e2e_eval",
        "description": "Demo.EMSConformer.inference.run_tflite_model_e2e_eval",
        "peekOfCode": "#visible_gpus = [gpus[i] for i in devices]\n#tf.config.set_visible_devices(visible_gpus, \"GPU\")\n#strategy = tf.distribute.MirroredStrategy()\nimport fire\n#import tensorflow as tf\nfrom tensorflow_asr.utils.file_util import read_file\nimport json\nfrom pydub import AudioSegment\nfrom tensorflow_asr.featurizers.speech_featurizers import read_raw_audio\ndef writeListFile(file_path, output_list, encoding = None):",
        "detail": "Demo.EMSConformer.inference.run_tflite_model_e2e_eval",
        "documentation": {}
    },
    {
        "label": "#strategy",
        "kind": 5,
        "importPath": "Demo.EMSConformer.inference.run_tflite_model_e2e_eval",
        "description": "Demo.EMSConformer.inference.run_tflite_model_e2e_eval",
        "peekOfCode": "#strategy = tf.distribute.MirroredStrategy()\nimport fire\n#import tensorflow as tf\nfrom tensorflow_asr.utils.file_util import read_file\nimport json\nfrom pydub import AudioSegment\nfrom tensorflow_asr.featurizers.speech_featurizers import read_raw_audio\ndef writeListFile(file_path, output_list, encoding = None):\n    f = open(file_path, mode = \"w\")\n    output_str = \"\\n\".join(output_list)",
        "detail": "Demo.EMSConformer.inference.run_tflite_model_e2e_eval",
        "documentation": {}
    },
    {
        "label": "writeListFile",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.run_tflite_model_in_files",
        "description": "Demo.EMSConformer.inference.run_tflite_model_in_files",
        "peekOfCode": "def writeListFile(file_path, output_list, encoding = None):\n    f = open(file_path, mode = \"w\")\n    output_str = \"\\n\".join(output_list)\n    f.write(output_str)\n    f.close()\ndef readFile(file_path, encoding = None):\n    f = open(file_path, 'r')\n    lines = f.read().splitlines()\n#    print(lines[0])\n    res = []",
        "detail": "Demo.EMSConformer.inference.run_tflite_model_in_files",
        "documentation": {}
    },
    {
        "label": "readFile",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.run_tflite_model_in_files",
        "description": "Demo.EMSConformer.inference.run_tflite_model_in_files",
        "peekOfCode": "def readFile(file_path, encoding = None):\n    f = open(file_path, 'r')\n    lines = f.read().splitlines()\n#    print(lines[0])\n    res = []\n    for line in lines:\n        line = line.strip()\n        res.append(line)\n    f.close()\n    return res",
        "detail": "Demo.EMSConformer.inference.run_tflite_model_in_files",
        "documentation": {}
    },
    {
        "label": "compare_normal_output",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.run_tflite_model_in_files",
        "description": "Demo.EMSConformer.inference.run_tflite_model_in_files",
        "peekOfCode": "def compare_normal_output(filepath: str):\n    logger.info(f\"Evaluating result from {filepath} ...\")\n    metrics = {\n        \"wer\": ErrorRate(wer, name=\"wer\", dtype=tf.float32),\n        \"cer\": ErrorRate(cer, name=\"cer\", dtype=tf.float32),\n    }\n    with read_file(filepath) as path:\n        with open(path, \"r\", encoding=\"utf-8\") as openfile:\n            lines = openfile.read().splitlines()\n    for eachline in tqdm(lines):",
        "detail": "Demo.EMSConformer.inference.run_tflite_model_in_files",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.run_tflite_model_in_files",
        "description": "Demo.EMSConformer.inference.run_tflite_model_in_files",
        "peekOfCode": "def main(\n    tflite: str,\n    audios_path: str,\n    true_text_path: str,\n    output: str,\n    blank: int = 0,\n    num_rnns: int = 1,\n    nstates: int = 2,\n    statesize: int = 320,\n):",
        "detail": "Demo.EMSConformer.inference.run_tflite_model_in_files",
        "documentation": {}
    },
    {
        "label": "os.environ['CUDA_VISIBLE_DEVICES']",
        "kind": 5,
        "importPath": "Demo.EMSConformer.inference.run_tflite_model_in_files",
        "description": "Demo.EMSConformer.inference.run_tflite_model_in_files",
        "peekOfCode": "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\nimport logging\nlogging.getLogger(\"tensorflow\").setLevel(logging.INFO)\nimport tensorflow as tf\nfrom tqdm import tqdm\nfrom tensorflow_asr.metrics.error_rates import ErrorRate\nfrom tensorflow_asr.utils.file_util import read_file\nfrom tensorflow_asr.utils.metric_util import cer, wer\nlogger = tf.get_logger()\nimport fire",
        "detail": "Demo.EMSConformer.inference.run_tflite_model_in_files",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "Demo.EMSConformer.inference.run_tflite_model_in_files",
        "description": "Demo.EMSConformer.inference.run_tflite_model_in_files",
        "peekOfCode": "logger = tf.get_logger()\nimport fire\n#import tensorflow as tf\nfrom tensorflow_asr.utils.file_util import read_file\nimport json\nfrom pydub import AudioSegment\nfrom tensorflow_asr.featurizers.speech_featurizers import read_raw_audio\nfrom datetime import datetime\nimport argparse\ndef writeListFile(file_path, output_list, encoding = None):",
        "detail": "Demo.EMSConformer.inference.run_tflite_model_in_files",
        "documentation": {}
    },
    {
        "label": "capture_audio",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.run_tflite_model_in_files_easy",
        "description": "Demo.EMSConformer.inference.run_tflite_model_in_files_easy",
        "peekOfCode": "def capture_audio(tflitemodel,input_details,output_details,blank,num_rnns,nstates,statesize, SpeechToNLPQueue, ConformerSignalQueue):\n    full_transcription = \"\"\n    while True:\n        signal = ConformerSignalQueue.get()\n        print(\"EMSConformer Signal: \",signal)\n        if(signal == \"Kill\"): \n            SpeechToNLPQueue.put(\"Kill\")\n            break\n        print('EMSConformer: Capturing audio!')\n        fs = 16000  # Sample rate",
        "detail": "Demo.EMSConformer.inference.run_tflite_model_in_files_easy",
        "documentation": {}
    },
    {
        "label": "writeListFile",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.run_tflite_model_in_files_easy",
        "description": "Demo.EMSConformer.inference.run_tflite_model_in_files_easy",
        "peekOfCode": "def writeListFile(file_path, output_list, encoding = None):\n    f = open(file_path, mode = \"w\")\n    output_str = \"\\n\".join(output_list)\n    f.write(output_str)\n    f.close()\ndef readFile(file_path, encoding = None):\n    f = open(file_path, 'r')\n    lines = f.read().splitlines()\n#    print(lines[0])\n    res = []",
        "detail": "Demo.EMSConformer.inference.run_tflite_model_in_files_easy",
        "documentation": {}
    },
    {
        "label": "readFile",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.run_tflite_model_in_files_easy",
        "description": "Demo.EMSConformer.inference.run_tflite_model_in_files_easy",
        "peekOfCode": "def readFile(file_path, encoding = None):\n    f = open(file_path, 'r')\n    lines = f.read().splitlines()\n#    print(lines[0])\n    res = []\n    for line in lines:\n        line = line.strip()\n        res.append(line)\n    f.close()\n    return res",
        "detail": "Demo.EMSConformer.inference.run_tflite_model_in_files_easy",
        "documentation": {}
    },
    {
        "label": "compare_normal_output",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.run_tflite_model_in_files_easy",
        "description": "Demo.EMSConformer.inference.run_tflite_model_in_files_easy",
        "peekOfCode": "def compare_normal_output(filepath: str):\n    logger.info(f\"Evaluating result from {filepath} ...\")\n    metrics = {\n        \"wer\": ErrorRate(wer, name=\"wer\", dtype=tf.float32),\n        \"cer\": ErrorRate(cer, name=\"cer\", dtype=tf.float32),\n    }\n    with read_file(filepath) as path:\n        with open(path, \"r\", encoding=\"utf-8\") as openfile:\n            lines = openfile.read().splitlines()\n    for eachline in tqdm(lines):",
        "detail": "Demo.EMSConformer.inference.run_tflite_model_in_files_easy",
        "documentation": {}
    },
    {
        "label": "trainscribe",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.run_tflite_model_in_files_easy",
        "description": "Demo.EMSConformer.inference.run_tflite_model_in_files_easy",
        "peekOfCode": "def trainscribe(tflitemodel,input_details,output_details, audio_buffer,blank,num_rnns,nstates,statesize):\n    signal = audio_buffer # audio buffer\n    start_t = time.perf_counter()\n    tflitemodel.resize_tensor_input(input_details[0][\"index\"], signal.shape)\n    tflitemodel.allocate_tensors()\n    tflitemodel.set_tensor(input_details[0][\"index\"], signal)\n    tflitemodel.set_tensor(input_details[1][\"index\"], tf.constant(blank, dtype=tf.int32))\n    #print('input_details[2][\"index\"] =', input_details[2][\"index\"])\n    #print('tf.zeros([num_rnns, nstates, 1, statesize] =', tf.zeros([num_rnns, nstates, 1, statesize]))\n    tflitemodel.set_tensor(input_details[2][\"index\"], tf.zeros([num_rnns, nstates, 1, statesize], dtype=tf.float32))",
        "detail": "Demo.EMSConformer.inference.run_tflite_model_in_files_easy",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Demo.EMSConformer.inference.run_tflite_model_in_files_easy",
        "description": "Demo.EMSConformer.inference.run_tflite_model_in_files_easy",
        "peekOfCode": "def main(\n    SpeechToNLPQueue: queue,\n    ConformerSignalQueue: queue,\n    tflite: str,\n    blank: int = 0,\n    num_rnns: int = 1,\n    nstates: int = 2,\n    statesize: int = 320,\n):\n    tflitemodel = tf.lite.Interpreter(model_path=tflite)    ",
        "detail": "Demo.EMSConformer.inference.run_tflite_model_in_files_easy",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "Demo.EMSConformer.inference.run_tflite_model_in_files_easy",
        "description": "Demo.EMSConformer.inference.run_tflite_model_in_files_easy",
        "peekOfCode": "logger = tf.get_logger()\n# For TensorFlow\n# gpus = tf.config.experimental.list_physical_devices('GPU')\n# print(gpus)\n# tf.config.experimental.set_memory_growth(gpus[0], True)  # Use GPU 0\ndef capture_audio(tflitemodel,input_details,output_details,blank,num_rnns,nstates,statesize, SpeechToNLPQueue, ConformerSignalQueue):\n    full_transcription = \"\"\n    while True:\n        signal = ConformerSignalQueue.get()\n        print(\"EMSConformer Signal: \",signal)",
        "detail": "Demo.EMSConformer.inference.run_tflite_model_in_files_easy",
        "documentation": {}
    },
    {
        "label": "AugmentationMethod",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.augmentations.methods.base_method",
        "description": "Demo.EMSConformer.tensorflow_asr.augmentations.methods.base_method",
        "peekOfCode": "class AugmentationMethod:\n    @tf.function\n    def augment(self, *args, **kwargs):\n        raise NotImplementedError()",
        "detail": "Demo.EMSConformer.tensorflow_asr.augmentations.methods.base_method",
        "documentation": {}
    },
    {
        "label": "FreqMasking",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.augmentations.methods.specaugment",
        "description": "Demo.EMSConformer.tensorflow_asr.augmentations.methods.specaugment",
        "peekOfCode": "class FreqMasking(AugmentationMethod):\n    def __init__(self, num_masks: int = 1, mask_factor: float = 27):\n        self.num_masks = num_masks\n        self.mask_factor = mask_factor\n    @tf.function\n    def augment(self, spectrogram: tf.Tensor):\n        \"\"\"\n        Masking the frequency channels (shape[1])\n        Args:\n            spectrogram: shape (T, num_feature_bins, V)",
        "detail": "Demo.EMSConformer.tensorflow_asr.augmentations.methods.specaugment",
        "documentation": {}
    },
    {
        "label": "TimeMasking",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.augmentations.methods.specaugment",
        "description": "Demo.EMSConformer.tensorflow_asr.augmentations.methods.specaugment",
        "peekOfCode": "class TimeMasking(AugmentationMethod):\n    def __init__(self, num_masks: int = 1, mask_factor: float = 100, p_upperbound: float = 1.0):\n        self.num_masks = num_masks\n        self.mask_factor = mask_factor\n        self.p_upperbound = p_upperbound\n    @tf.function\n    def augment(self, spectrogram: tf.Tensor):\n        \"\"\"\n        Masking the time channel (shape[0])\n        Args:",
        "detail": "Demo.EMSConformer.tensorflow_asr.augmentations.methods.specaugment",
        "documentation": {}
    },
    {
        "label": "Augmentation",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.augmentations.augmentation",
        "description": "Demo.EMSConformer.tensorflow_asr.augmentations.augmentation",
        "peekOfCode": "class Augmentation:\n    def __init__(self, config: dict = None):\n        if not config:\n            config = {}\n#        print(\"config in augmentation: %s\" % config)\n        self.prob = float(config.pop(\"prob\", 0.5))\n        self.signal_augmentations = self.parse(config.pop(\"signal_augment\", {}))\n        self.feature_augmentations = self.parse(config.pop(\"feature_augment\", {}))\n    def _augment(self, inputs, augmentations):\n        outputs = inputs",
        "detail": "Demo.EMSConformer.tensorflow_asr.augmentations.augmentation",
        "documentation": {}
    },
    {
        "label": "AUGMENTATIONS",
        "kind": 5,
        "importPath": "Demo.EMSConformer.tensorflow_asr.augmentations.augmentation",
        "description": "Demo.EMSConformer.tensorflow_asr.augmentations.augmentation",
        "peekOfCode": "AUGMENTATIONS = {\n    \"freq_masking\": specaugment.FreqMasking,\n    \"time_masking\": specaugment.TimeMasking,\n}\nclass Augmentation:\n    def __init__(self, config: dict = None):\n        if not config:\n            config = {}\n#        print(\"config in augmentation: %s\" % config)\n        self.prob = float(config.pop(\"prob\", 0.5))",
        "detail": "Demo.EMSConformer.tensorflow_asr.augmentations.augmentation",
        "documentation": {}
    },
    {
        "label": "DecoderConfig",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.configs.config",
        "description": "Demo.EMSConformer.tensorflow_asr.configs.config",
        "peekOfCode": "class DecoderConfig:\n    def __init__(self, config: dict = None):\n        if not config:\n            config = {}\n        self.beam_width = config.pop(\"beam_width\", 0)\n        self.blank_at_zero = config.pop(\"blank_at_zero\", True)\n        self.norm_score = config.pop(\"norm_score\", True)\n        self.lm_config = config.pop(\"lm_config\", {})\n        self.vocabulary = file_util.preprocess_paths(config.pop(\"vocabulary\", None))\n        self.target_vocab_size = config.pop(\"target_vocab_size\", 1024)",
        "detail": "Demo.EMSConformer.tensorflow_asr.configs.config",
        "documentation": {}
    },
    {
        "label": "DatasetConfig",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.configs.config",
        "description": "Demo.EMSConformer.tensorflow_asr.configs.config",
        "peekOfCode": "class DatasetConfig:\n    def __init__(self, config: dict = None):\n        if not config:\n            config = {}\n        self.stage = config.pop(\"stage\", None)\n        self.data_paths = file_util.preprocess_paths(config.pop(\"data_paths\", None))\n        self.tfrecords_dir = file_util.preprocess_paths(config.pop(\"tfrecords_dir\", None), isdir=True)\n        self.tfrecords_shards = config.pop(\"tfrecords_shards\", 16)\n        self.shuffle = config.pop(\"shuffle\", False)\n        self.cache = config.pop(\"cache\", False)",
        "detail": "Demo.EMSConformer.tensorflow_asr.configs.config",
        "documentation": {}
    },
    {
        "label": "RunningConfig",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.configs.config",
        "description": "Demo.EMSConformer.tensorflow_asr.configs.config",
        "peekOfCode": "class RunningConfig:\n    def __init__(self, config: dict = None):\n        if not config:\n            config = {}\n        self.batch_size = config.pop(\"batch_size\", 1)\n        self.accumulation_steps = config.pop(\"accumulation_steps\", 1)\n        self.num_epochs = config.pop(\"num_epochs\", 20)\n        for k, v in config.items():\n            setattr(self, k, v)\n            if k == \"checkpoint\":",
        "detail": "Demo.EMSConformer.tensorflow_asr.configs.config",
        "documentation": {}
    },
    {
        "label": "LearningConfig",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.configs.config",
        "description": "Demo.EMSConformer.tensorflow_asr.configs.config",
        "peekOfCode": "class LearningConfig:\n    def __init__(self, config: dict = None):\n        if not config:\n            config = {}\n        self.train_dataset_config = DatasetConfig(config.pop(\"train_dataset_config\", {}))\n        self.eval_dataset_config = DatasetConfig(config.pop(\"eval_dataset_config\", {}))\n        self.test_dataset_config = DatasetConfig(config.pop(\"test_dataset_config\", {}))\n        self.optimizer_config = config.pop(\"optimizer_config\", {})\n        self.running_config = RunningConfig(config.pop(\"running_config\", {}))\n        for k, v in config.items():",
        "detail": "Demo.EMSConformer.tensorflow_asr.configs.config",
        "documentation": {}
    },
    {
        "label": "Config",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.configs.config",
        "description": "Demo.EMSConformer.tensorflow_asr.configs.config",
        "peekOfCode": "class Config:\n    \"\"\"User config class for training, testing or infering\"\"\"\n    def __init__(self, data: Union[str, dict]):\n        config = data if isinstance(data, dict) else file_util.load_yaml(file_util.preprocess_paths(data))\n        self.speech_config = config.pop(\"speech_config\", {})\n        self.decoder_config = config.pop(\"decoder_config\", {})\n        self.model_config = config.pop(\"model_config\", {})\n        self.learning_config = LearningConfig(config.pop(\"learning_config\", {}))\n#        print(\"Config: %s\" % config.items())\n        for k, v in config.items():",
        "detail": "Demo.EMSConformer.tensorflow_asr.configs.config",
        "documentation": {}
    },
    {
        "label": "ASRDataset",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.datasets.asr_dataset",
        "description": "Demo.EMSConformer.tensorflow_asr.datasets.asr_dataset",
        "peekOfCode": "class ASRDataset(BaseDataset):\n    \"\"\"Dataset for ASR using Generator\"\"\"\n    def __init__(\n        self,\n        stage: str,\n        speech_featurizer: SpeechFeaturizer,\n        text_featurizer: TextFeaturizer,\n        data_paths: list,\n        augmentations: Augmentation = Augmentation(None),\n        cache: bool = False,",
        "detail": "Demo.EMSConformer.tensorflow_asr.datasets.asr_dataset",
        "documentation": {}
    },
    {
        "label": "ASRTFRecordDataset",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.datasets.asr_dataset",
        "description": "Demo.EMSConformer.tensorflow_asr.datasets.asr_dataset",
        "peekOfCode": "class ASRTFRecordDataset(ASRDataset):\n    \"\"\"Dataset for ASR using TFRecords\"\"\"\n    def __init__(\n        self,\n        data_paths: list,\n        tfrecords_dir: str,\n        speech_featurizer: SpeechFeaturizer,\n        text_featurizer: TextFeaturizer,\n        stage: str,\n        augmentations: Augmentation = Augmentation(None),",
        "detail": "Demo.EMSConformer.tensorflow_asr.datasets.asr_dataset",
        "documentation": {}
    },
    {
        "label": "ASRSliceDataset",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.datasets.asr_dataset",
        "description": "Demo.EMSConformer.tensorflow_asr.datasets.asr_dataset",
        "peekOfCode": "class ASRSliceDataset(ASRDataset):\n    \"\"\"Dataset for ASR using Slice\"\"\"\n    @staticmethod\n    def load(record: tf.Tensor):\n        def fn(path: bytes):\n            return load_and_convert_to_wav(path.decode(\"utf-8\")).numpy()\n        audio = tf.numpy_function(fn, inp=[record[0]], Tout=tf.string)\n        return record[0], audio, record[2]\n    def create(self, batch_size: int):\n        self.read_entries()",
        "detail": "Demo.EMSConformer.tensorflow_asr.datasets.asr_dataset",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "Demo.EMSConformer.tensorflow_asr.datasets.asr_dataset",
        "description": "Demo.EMSConformer.tensorflow_asr.datasets.asr_dataset",
        "peekOfCode": "logger = tf.get_logger()\nclass ASRDataset(BaseDataset):\n    \"\"\"Dataset for ASR using Generator\"\"\"\n    def __init__(\n        self,\n        stage: str,\n        speech_featurizer: SpeechFeaturizer,\n        text_featurizer: TextFeaturizer,\n        data_paths: list,\n        augmentations: Augmentation = Augmentation(None),",
        "detail": "Demo.EMSConformer.tensorflow_asr.datasets.asr_dataset",
        "documentation": {}
    },
    {
        "label": "BaseDataset",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.datasets.base_dataset",
        "description": "Demo.EMSConformer.tensorflow_asr.datasets.base_dataset",
        "peekOfCode": "class BaseDataset(metaclass=abc.ABCMeta):\n    \"\"\"Based dataset for all models\"\"\"\n    def __init__(\n        self,\n        data_paths: list,\n        augmentations: Augmentation = Augmentation(None),\n        cache: bool = False,\n        shuffle: bool = False,\n        buffer_size: int = BUFFER_SIZE,\n        indefinite: bool = False,",
        "detail": "Demo.EMSConformer.tensorflow_asr.datasets.base_dataset",
        "documentation": {}
    },
    {
        "label": "BUFFER_SIZE",
        "kind": 5,
        "importPath": "Demo.EMSConformer.tensorflow_asr.datasets.base_dataset",
        "description": "Demo.EMSConformer.tensorflow_asr.datasets.base_dataset",
        "peekOfCode": "BUFFER_SIZE = 100\nTFRECORD_SHARDS = 16\nAUTOTUNE = tf.data.experimental.AUTOTUNE\nclass BaseDataset(metaclass=abc.ABCMeta):\n    \"\"\"Based dataset for all models\"\"\"\n    def __init__(\n        self,\n        data_paths: list,\n        augmentations: Augmentation = Augmentation(None),\n        cache: bool = False,",
        "detail": "Demo.EMSConformer.tensorflow_asr.datasets.base_dataset",
        "documentation": {}
    },
    {
        "label": "TFRECORD_SHARDS",
        "kind": 5,
        "importPath": "Demo.EMSConformer.tensorflow_asr.datasets.base_dataset",
        "description": "Demo.EMSConformer.tensorflow_asr.datasets.base_dataset",
        "peekOfCode": "TFRECORD_SHARDS = 16\nAUTOTUNE = tf.data.experimental.AUTOTUNE\nclass BaseDataset(metaclass=abc.ABCMeta):\n    \"\"\"Based dataset for all models\"\"\"\n    def __init__(\n        self,\n        data_paths: list,\n        augmentations: Augmentation = Augmentation(None),\n        cache: bool = False,\n        shuffle: bool = False,",
        "detail": "Demo.EMSConformer.tensorflow_asr.datasets.base_dataset",
        "documentation": {}
    },
    {
        "label": "AUTOTUNE",
        "kind": 5,
        "importPath": "Demo.EMSConformer.tensorflow_asr.datasets.base_dataset",
        "description": "Demo.EMSConformer.tensorflow_asr.datasets.base_dataset",
        "peekOfCode": "AUTOTUNE = tf.data.experimental.AUTOTUNE\nclass BaseDataset(metaclass=abc.ABCMeta):\n    \"\"\"Based dataset for all models\"\"\"\n    def __init__(\n        self,\n        data_paths: list,\n        augmentations: Augmentation = Augmentation(None),\n        cache: bool = False,\n        shuffle: bool = False,\n        buffer_size: int = BUFFER_SIZE,",
        "detail": "Demo.EMSConformer.tensorflow_asr.datasets.base_dataset",
        "documentation": {}
    },
    {
        "label": "fft_weights",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.featurizers.methods.gammatone",
        "description": "Demo.EMSConformer.tensorflow_asr.featurizers.methods.gammatone",
        "peekOfCode": "def fft_weights(\n    nfft,\n    fs,\n    nfilts,\n    width,\n    fmin,\n    fmax,\n    maxlen,\n):\n    \"\"\"",
        "detail": "Demo.EMSConformer.tensorflow_asr.featurizers.methods.gammatone",
        "documentation": {}
    },
    {
        "label": "erb_point",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.featurizers.methods.gammatone",
        "description": "Demo.EMSConformer.tensorflow_asr.featurizers.methods.gammatone",
        "peekOfCode": "def erb_point(\n    low_freq,\n    high_freq,\n    fraction,\n):\n    \"\"\"\n    Calculates a single point on an ERB scale between ``low_freq`` and\n    ``high_freq``, determined by ``fraction``. When ``fraction`` is ``1``,\n    ``low_freq`` will be returned. When ``fraction`` is ``0``, ``high_freq``\n    will be returned.",
        "detail": "Demo.EMSConformer.tensorflow_asr.featurizers.methods.gammatone",
        "documentation": {}
    },
    {
        "label": "erb_space",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.featurizers.methods.gammatone",
        "description": "Demo.EMSConformer.tensorflow_asr.featurizers.methods.gammatone",
        "peekOfCode": "def erb_space(\n    low_freq=DEFAULT_LOW_FREQ,\n    high_freq=DEFAULT_HIGH_FREQ,\n    num=DEFAULT_FILTER_NUM,\n):\n    \"\"\"\n    This function computes an array of ``num`` frequencies uniformly spaced\n    between ``high_freq`` and ``low_freq`` on an ERB scale.\n    For a definition of ERB, see Moore, B. C. J., and Glasberg, B. R. (1983).\n    \"Suggested formulae for calculating auditory-filter bandwidths and",
        "detail": "Demo.EMSConformer.tensorflow_asr.featurizers.methods.gammatone",
        "documentation": {}
    },
    {
        "label": "make_erb_filters",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.featurizers.methods.gammatone",
        "description": "Demo.EMSConformer.tensorflow_asr.featurizers.methods.gammatone",
        "peekOfCode": "def make_erb_filters(\n    fs,\n    centre_freqs,\n    width=1.0,\n):\n    \"\"\"\n    This function computes the filter coefficients for a bank of\n    Gammatone filters. These filters were defined by Patterson and Holdworth for\n    simulating the cochlea.\n    The result is returned as a :class:`ERBCoeffArray`. Each row of the",
        "detail": "Demo.EMSConformer.tensorflow_asr.featurizers.methods.gammatone",
        "documentation": {}
    },
    {
        "label": "pi",
        "kind": 5,
        "importPath": "Demo.EMSConformer.tensorflow_asr.featurizers.methods.gammatone",
        "description": "Demo.EMSConformer.tensorflow_asr.featurizers.methods.gammatone",
        "peekOfCode": "pi = tf.constant(np.pi, dtype=tf.complex64)\nDEFAULT_FILTER_NUM = 100\nDEFAULT_LOW_FREQ = 100\nDEFAULT_HIGH_FREQ = 44100 / 4\ndef fft_weights(\n    nfft,\n    fs,\n    nfilts,\n    width,\n    fmin,",
        "detail": "Demo.EMSConformer.tensorflow_asr.featurizers.methods.gammatone",
        "documentation": {}
    },
    {
        "label": "DEFAULT_FILTER_NUM",
        "kind": 5,
        "importPath": "Demo.EMSConformer.tensorflow_asr.featurizers.methods.gammatone",
        "description": "Demo.EMSConformer.tensorflow_asr.featurizers.methods.gammatone",
        "peekOfCode": "DEFAULT_FILTER_NUM = 100\nDEFAULT_LOW_FREQ = 100\nDEFAULT_HIGH_FREQ = 44100 / 4\ndef fft_weights(\n    nfft,\n    fs,\n    nfilts,\n    width,\n    fmin,\n    fmax,",
        "detail": "Demo.EMSConformer.tensorflow_asr.featurizers.methods.gammatone",
        "documentation": {}
    },
    {
        "label": "DEFAULT_LOW_FREQ",
        "kind": 5,
        "importPath": "Demo.EMSConformer.tensorflow_asr.featurizers.methods.gammatone",
        "description": "Demo.EMSConformer.tensorflow_asr.featurizers.methods.gammatone",
        "peekOfCode": "DEFAULT_LOW_FREQ = 100\nDEFAULT_HIGH_FREQ = 44100 / 4\ndef fft_weights(\n    nfft,\n    fs,\n    nfilts,\n    width,\n    fmin,\n    fmax,\n    maxlen,",
        "detail": "Demo.EMSConformer.tensorflow_asr.featurizers.methods.gammatone",
        "documentation": {}
    },
    {
        "label": "DEFAULT_HIGH_FREQ",
        "kind": 5,
        "importPath": "Demo.EMSConformer.tensorflow_asr.featurizers.methods.gammatone",
        "description": "Demo.EMSConformer.tensorflow_asr.featurizers.methods.gammatone",
        "peekOfCode": "DEFAULT_HIGH_FREQ = 44100 / 4\ndef fft_weights(\n    nfft,\n    fs,\n    nfilts,\n    width,\n    fmin,\n    fmax,\n    maxlen,\n):",
        "detail": "Demo.EMSConformer.tensorflow_asr.featurizers.methods.gammatone",
        "documentation": {}
    },
    {
        "label": "SpeechFeaturizer",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.featurizers.speech_featurizers",
        "description": "Demo.EMSConformer.tensorflow_asr.featurizers.speech_featurizers",
        "peekOfCode": "class SpeechFeaturizer(metaclass=abc.ABCMeta):\n    def __init__(\n        self,\n        speech_config: dict,\n    ):\n        \"\"\"\n        We should use TFSpeechFeaturizer for training to avoid differences\n        between tf and librosa when converting to tflite in post-training stage\n        speech_config = {\n            \"sample_rate\": int,",
        "detail": "Demo.EMSConformer.tensorflow_asr.featurizers.speech_featurizers",
        "documentation": {}
    },
    {
        "label": "NumpySpeechFeaturizer",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.featurizers.speech_featurizers",
        "description": "Demo.EMSConformer.tensorflow_asr.featurizers.speech_featurizers",
        "peekOfCode": "class NumpySpeechFeaturizer(SpeechFeaturizer):\n    def __init__(self, speech_config: dict):\n        super(NumpySpeechFeaturizer, self).__init__(speech_config)\n        self.delta = speech_config.get(\"delta\", False)\n        self.delta_delta = speech_config.get(\"delta_delta\", False)\n        self.pitch = speech_config.get(\"pitch\", False)\n    @property\n    def shape(self) -> list:\n        # None for time dimension\n        channel_dim = 1",
        "detail": "Demo.EMSConformer.tensorflow_asr.featurizers.speech_featurizers",
        "documentation": {}
    },
    {
        "label": "TFSpeechFeaturizer",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.featurizers.speech_featurizers",
        "description": "Demo.EMSConformer.tensorflow_asr.featurizers.speech_featurizers",
        "peekOfCode": "class TFSpeechFeaturizer(SpeechFeaturizer):\n    @property\n    def shape(self) -> list:\n        length = self.max_length if self.max_length > 0 else None\n        return [length, self.num_feature_bins, 1]\n    def stft(\n        self,\n        signal,\n    ):\n        if self.center:",
        "detail": "Demo.EMSConformer.tensorflow_asr.featurizers.speech_featurizers",
        "documentation": {}
    },
    {
        "label": "load_and_convert_to_wav",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.featurizers.speech_featurizers",
        "description": "Demo.EMSConformer.tensorflow_asr.featurizers.speech_featurizers",
        "peekOfCode": "def load_and_convert_to_wav(\n    path: str,\n) -> tf.Tensor:\n    wave, rate = librosa.load(os.path.expanduser(path), sr=None, mono=True)\n#    print(wave.shape)\n#    print(len(wave))\n#    print(rate)\n    return tf.audio.encode_wav(tf.expand_dims(wave, axis=-1), sample_rate=rate)\ndef read_raw_audio(\n    audio: Union[str, bytes, np.ndarray],",
        "detail": "Demo.EMSConformer.tensorflow_asr.featurizers.speech_featurizers",
        "documentation": {}
    },
    {
        "label": "read_raw_audio",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.featurizers.speech_featurizers",
        "description": "Demo.EMSConformer.tensorflow_asr.featurizers.speech_featurizers",
        "peekOfCode": "def read_raw_audio(\n    audio: Union[str, bytes, np.ndarray],\n    sample_rate=16000,\n) -> np.ndarray:\n    if isinstance(audio, str):\n#        print(\"sample_rate=%s\" % sample_rate)\n        wave, _ = librosa.load(os.path.expanduser(audio), sr=sample_rate, mono=True)\n    elif isinstance(audio, bytes):\n        wave, sr = sf.read(io.BytesIO(audio))\n        if wave.ndim > 1:",
        "detail": "Demo.EMSConformer.tensorflow_asr.featurizers.speech_featurizers",
        "documentation": {}
    },
    {
        "label": "tf_read_raw_audio",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.featurizers.speech_featurizers",
        "description": "Demo.EMSConformer.tensorflow_asr.featurizers.speech_featurizers",
        "peekOfCode": "def tf_read_raw_audio(\n    audio: tf.Tensor,\n    sample_rate=16000,\n) -> tf.Tensor:\n    wave, rate = tf.audio.decode_wav(audio, desired_channels=1, desired_samples=-1)\n    if not env_util.has_devices(\"TPU\"):\n        resampled = tfio.audio.resample(wave, rate_in=tf.cast(rate, dtype=tf.int64), rate_out=sample_rate)\n        return tf.reshape(resampled, shape=[-1])  # reshape for using tf.signal\n    return tf.reshape(wave, shape=[-1])  # reshape for using tf.signal\ndef slice_signal(",
        "detail": "Demo.EMSConformer.tensorflow_asr.featurizers.speech_featurizers",
        "documentation": {}
    },
    {
        "label": "slice_signal",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.featurizers.speech_featurizers",
        "description": "Demo.EMSConformer.tensorflow_asr.featurizers.speech_featurizers",
        "peekOfCode": "def slice_signal(\n    signal,\n    window_size,\n    stride=0.5,\n) -> np.ndarray:\n    \"\"\"Return windows of the given signal by sweeping in stride fractions of window\"\"\"\n    assert signal.ndim == 1, signal.ndim\n    n_samples = signal.shape[0]\n    offset = int(window_size * stride)\n    slices = []",
        "detail": "Demo.EMSConformer.tensorflow_asr.featurizers.speech_featurizers",
        "documentation": {}
    },
    {
        "label": "tf_merge_slices",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.featurizers.speech_featurizers",
        "description": "Demo.EMSConformer.tensorflow_asr.featurizers.speech_featurizers",
        "peekOfCode": "def tf_merge_slices(\n    slices: tf.Tensor,\n) -> tf.Tensor:\n    # slices shape = [batch, window_size]\n    return tf.keras.backend.flatten(slices)  # return shape = [-1, ]\ndef merge_slices(\n    slices: np.ndarray,\n) -> np.ndarray:\n    # slices shape = [batch, window_size]\n    return np.reshape(slices, [-1])",
        "detail": "Demo.EMSConformer.tensorflow_asr.featurizers.speech_featurizers",
        "documentation": {}
    },
    {
        "label": "merge_slices",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.featurizers.speech_featurizers",
        "description": "Demo.EMSConformer.tensorflow_asr.featurizers.speech_featurizers",
        "peekOfCode": "def merge_slices(\n    slices: np.ndarray,\n) -> np.ndarray:\n    # slices shape = [batch, window_size]\n    return np.reshape(slices, [-1])\ndef normalize_audio_feature(\n    audio_feature: np.ndarray,\n    per_frame=False,\n) -> np.ndarray:\n    \"\"\"Mean and variance normalization\"\"\"",
        "detail": "Demo.EMSConformer.tensorflow_asr.featurizers.speech_featurizers",
        "documentation": {}
    },
    {
        "label": "normalize_audio_feature",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.featurizers.speech_featurizers",
        "description": "Demo.EMSConformer.tensorflow_asr.featurizers.speech_featurizers",
        "peekOfCode": "def normalize_audio_feature(\n    audio_feature: np.ndarray,\n    per_frame=False,\n) -> np.ndarray:\n    \"\"\"Mean and variance normalization\"\"\"\n    axis = 1 if per_frame else None\n    mean = np.mean(audio_feature, axis=axis)\n    std_dev = np.sqrt(np.var(audio_feature, axis=axis) + 1e-9)\n    normalized = (audio_feature - mean) / std_dev\n    return normalized",
        "detail": "Demo.EMSConformer.tensorflow_asr.featurizers.speech_featurizers",
        "documentation": {}
    },
    {
        "label": "tf_normalize_audio_features",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.featurizers.speech_featurizers",
        "description": "Demo.EMSConformer.tensorflow_asr.featurizers.speech_featurizers",
        "peekOfCode": "def tf_normalize_audio_features(\n    audio_feature: tf.Tensor,\n    per_frame=False,\n) -> tf.Tensor:\n    \"\"\"\n    TF Mean and variance features normalization\n    Args:\n        audio_feature: tf.Tensor with shape [T, F]\n    Returns:\n        normalized audio features with shape [T, F]",
        "detail": "Demo.EMSConformer.tensorflow_asr.featurizers.speech_featurizers",
        "documentation": {}
    },
    {
        "label": "normalize_signal",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.featurizers.speech_featurizers",
        "description": "Demo.EMSConformer.tensorflow_asr.featurizers.speech_featurizers",
        "peekOfCode": "def normalize_signal(\n    signal: np.ndarray,\n) -> np.ndarray:\n    \"\"\"Normailize signal to [-1, 1] range\"\"\"\n    gain = 1.0 / (np.max(np.abs(signal)) + 1e-9)\n    return signal * gain\ndef tf_normalize_signal(\n    signal: tf.Tensor,\n) -> tf.Tensor:\n    \"\"\"",
        "detail": "Demo.EMSConformer.tensorflow_asr.featurizers.speech_featurizers",
        "documentation": {}
    },
    {
        "label": "tf_normalize_signal",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.featurizers.speech_featurizers",
        "description": "Demo.EMSConformer.tensorflow_asr.featurizers.speech_featurizers",
        "peekOfCode": "def tf_normalize_signal(\n    signal: tf.Tensor,\n) -> tf.Tensor:\n    \"\"\"\n    TF Normailize signal to [-1, 1] range\n    Args:\n        signal: tf.Tensor with shape [None]\n    Returns:\n        normalized signal with shape [None]\n    \"\"\"",
        "detail": "Demo.EMSConformer.tensorflow_asr.featurizers.speech_featurizers",
        "documentation": {}
    },
    {
        "label": "preemphasis",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.featurizers.speech_featurizers",
        "description": "Demo.EMSConformer.tensorflow_asr.featurizers.speech_featurizers",
        "peekOfCode": "def preemphasis(\n    signal: np.ndarray,\n    coeff=0.97,\n) -> np.ndarray:\n    if not coeff or coeff <= 0.0:\n        return signal\n    return np.append(signal[0], signal[1:] - coeff * signal[:-1])\ndef tf_preemphasis(\n    signal: tf.Tensor,\n    coeff=0.97,",
        "detail": "Demo.EMSConformer.tensorflow_asr.featurizers.speech_featurizers",
        "documentation": {}
    },
    {
        "label": "tf_preemphasis",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.featurizers.speech_featurizers",
        "description": "Demo.EMSConformer.tensorflow_asr.featurizers.speech_featurizers",
        "peekOfCode": "def tf_preemphasis(\n    signal: tf.Tensor,\n    coeff=0.97,\n):\n    \"\"\"\n    TF Pre-emphasis\n    Args:\n        signal: tf.Tensor with shape [None]\n        coeff: Float that indicates the preemphasis coefficient\n    Returns:",
        "detail": "Demo.EMSConformer.tensorflow_asr.featurizers.speech_featurizers",
        "documentation": {}
    },
    {
        "label": "depreemphasis",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.featurizers.speech_featurizers",
        "description": "Demo.EMSConformer.tensorflow_asr.featurizers.speech_featurizers",
        "peekOfCode": "def depreemphasis(\n    signal: np.ndarray,\n    coeff=0.97,\n) -> np.ndarray:\n    if not coeff or coeff <= 0.0:\n        return signal\n    x = np.zeros(signal.shape[0], dtype=np.float32)\n    x[0] = signal[0]\n    for n in range(1, signal.shape[0], 1):\n        x[n] = coeff * x[n - 1] + signal[n]",
        "detail": "Demo.EMSConformer.tensorflow_asr.featurizers.speech_featurizers",
        "documentation": {}
    },
    {
        "label": "tf_depreemphasis",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.featurizers.speech_featurizers",
        "description": "Demo.EMSConformer.tensorflow_asr.featurizers.speech_featurizers",
        "peekOfCode": "def tf_depreemphasis(\n    signal: tf.Tensor,\n    coeff=0.97,\n) -> tf.Tensor:\n    \"\"\"\n    TF Depreemphasis\n    Args:\n        signal: tf.Tensor with shape [B, None]\n        coeff: Float that indicates the preemphasis coefficient\n    Returns:",
        "detail": "Demo.EMSConformer.tensorflow_asr.featurizers.speech_featurizers",
        "documentation": {}
    },
    {
        "label": "TextFeaturizer",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.featurizers.text_featurizers",
        "description": "Demo.EMSConformer.tensorflow_asr.featurizers.text_featurizers",
        "peekOfCode": "class TextFeaturizer(metaclass=abc.ABCMeta):\n    def __init__(\n        self,\n        decoder_config: dict,\n    ):\n        self.scorer = None\n        self.decoder_config = DecoderConfig(decoder_config)\n        self.blank = None\n        self.tokens2indices = {}\n        self.tokens = []",
        "detail": "Demo.EMSConformer.tensorflow_asr.featurizers.text_featurizers",
        "documentation": {}
    },
    {
        "label": "CharFeaturizer",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.featurizers.text_featurizers",
        "description": "Demo.EMSConformer.tensorflow_asr.featurizers.text_featurizers",
        "peekOfCode": "class CharFeaturizer(TextFeaturizer):\n    \"\"\"\n    Extract text feature based on char-level granularity.\n    By looking up the vocabulary table, each line of transcript will be\n    converted to a sequence of integer indexes.\n    \"\"\"\n    def __init__(\n        self,\n        decoder_config: dict,\n    ):",
        "detail": "Demo.EMSConformer.tensorflow_asr.featurizers.text_featurizers",
        "documentation": {}
    },
    {
        "label": "SubwordFeaturizer",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.featurizers.text_featurizers",
        "description": "Demo.EMSConformer.tensorflow_asr.featurizers.text_featurizers",
        "peekOfCode": "class SubwordFeaturizer(TextFeaturizer):\n    \"\"\"\n    Extract text feature based on char-level granularity.\n    By looking up the vocabulary table, each line of transcript will be\n    converted to a sequence of integer indexes.\n    \"\"\"\n    def __init__(\n        self,\n        decoder_config: dict,\n        subwords=None,",
        "detail": "Demo.EMSConformer.tensorflow_asr.featurizers.text_featurizers",
        "documentation": {}
    },
    {
        "label": "SentencePieceFeaturizer",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.featurizers.text_featurizers",
        "description": "Demo.EMSConformer.tensorflow_asr.featurizers.text_featurizers",
        "peekOfCode": "class SentencePieceFeaturizer(TextFeaturizer):\n    \"\"\"\n    Extract text feature based on sentence piece package.\n    \"\"\"\n    UNK_TOKEN, UNK_TOKEN_ID = \"<unk>\", 1\n    BOS_TOKEN, BOS_TOKEN_ID = \"<s>\", 2\n    EOS_TOKEN, EOS_TOKEN_ID = \"</s>\", 3\n    PAD_TOKEN, PAD_TOKEN_ID = \"<pad>\", 0  # unused, by default\n    def __init__(\n        self,",
        "detail": "Demo.EMSConformer.tensorflow_asr.featurizers.text_featurizers",
        "documentation": {}
    },
    {
        "label": "ENGLISH_CHARACTERS",
        "kind": 5,
        "importPath": "Demo.EMSConformer.tensorflow_asr.featurizers.text_featurizers",
        "description": "Demo.EMSConformer.tensorflow_asr.featurizers.text_featurizers",
        "peekOfCode": "ENGLISH_CHARACTERS = [\n    \" \",\n    \"a\",\n    \"b\",\n    \"c\",\n    \"d\",\n    \"e\",\n    \"f\",\n    \"g\",\n    \"h\",",
        "detail": "Demo.EMSConformer.tensorflow_asr.featurizers.text_featurizers",
        "documentation": {}
    },
    {
        "label": "prepare_training_datasets",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.helpers.dataset_helpers",
        "description": "Demo.EMSConformer.tensorflow_asr.helpers.dataset_helpers",
        "peekOfCode": "def prepare_training_datasets(\n    config: Config,\n    speech_featurizer: SpeechFeaturizer,\n    text_featurizer: TextFeaturizer,\n    tfrecords: bool = False,\n    metadata: str = None,\n):\n#    print(\"prepare_training_datasets:\")\n#    for k, v in vars(config.learning_config.eval_dataset_config).items():\n#        print(\"the value of {} is {}\".format(k, v))",
        "detail": "Demo.EMSConformer.tensorflow_asr.helpers.dataset_helpers",
        "documentation": {}
    },
    {
        "label": "prepare_testing_datasets",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.helpers.dataset_helpers",
        "description": "Demo.EMSConformer.tensorflow_asr.helpers.dataset_helpers",
        "peekOfCode": "def prepare_testing_datasets(\n    config: Config,\n    speech_featurizer: SpeechFeaturizer,\n    text_featurizer: TextFeaturizer,\n):\n    test_dataset = asr_dataset.ASRSliceDataset(\n        speech_featurizer=speech_featurizer,\n        text_featurizer=text_featurizer,\n        **vars(config.learning_config.test_dataset_config)\n    )",
        "detail": "Demo.EMSConformer.tensorflow_asr.helpers.dataset_helpers",
        "documentation": {}
    },
    {
        "label": "prepare_training_data_loaders",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.helpers.dataset_helpers",
        "description": "Demo.EMSConformer.tensorflow_asr.helpers.dataset_helpers",
        "peekOfCode": "def prepare_training_data_loaders(\n    config: Config,\n    train_dataset: asr_dataset.ASRDataset,\n    eval_dataset: asr_dataset.ASRDataset,\n    batch_size: int = None,\n):\n    global_batch_size = batch_size or config.learning_config.running_config.batch_size\n    train_data_loader = train_dataset.create(global_batch_size)\n    eval_data_loader = eval_dataset.create(global_batch_size)\n#    print(\"train_data_loader: \", train_data_loader.cardinality().numpy())",
        "detail": "Demo.EMSConformer.tensorflow_asr.helpers.dataset_helpers",
        "documentation": {}
    },
    {
        "label": "run_testing",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.helpers.exec_helpers",
        "description": "Demo.EMSConformer.tensorflow_asr.helpers.exec_helpers",
        "peekOfCode": "def run_testing(\n    model: BaseModel,\n    test_dataset: ASRSliceDataset,\n    test_data_loader: tf.data.Dataset,\n    output: str,\n):\n    with file_util.save_file(file_util.preprocess_paths(output)) as filepath:\n        overwrite = True\n        if tf.io.gfile.exists(filepath):\n            overwrite = input(f\"Overwrite existing result file {filepath} ? (y/n): \").lower() == \"y\"",
        "detail": "Demo.EMSConformer.tensorflow_asr.helpers.exec_helpers",
        "documentation": {}
    },
    {
        "label": "convert_tflite",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.helpers.exec_helpers",
        "description": "Demo.EMSConformer.tensorflow_asr.helpers.exec_helpers",
        "peekOfCode": "def convert_tflite(\n    model: BaseModel,\n    output: str,\n):\n#    with tempfile.TemporaryDirectory() as temp_dir_name:\n#        save_path = os.path.join(temp_dir_name, 'saved_model')\n#        print(\"saving saved_model and loading tflite from %s\" % save_path)\n#        model.save(save_path, include_optimizer=False, save_format='tf')\n#        converter = tf.lite.TFLiteConverter.from_saved_model(save_path)\n#    converter = tf.lite.TFLiteConverter.from_saved_model(\"/home/liuyi/TensorFlowASR/examples/conformer/saved_models/\")",
        "detail": "Demo.EMSConformer.tensorflow_asr.helpers.exec_helpers",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "Demo.EMSConformer.tensorflow_asr.helpers.exec_helpers",
        "description": "Demo.EMSConformer.tensorflow_asr.helpers.exec_helpers",
        "peekOfCode": "logger = tf.get_logger()\ndef run_testing(\n    model: BaseModel,\n    test_dataset: ASRSliceDataset,\n    test_data_loader: tf.data.Dataset,\n    output: str,\n):\n    with file_util.save_file(file_util.preprocess_paths(output)) as filepath:\n        overwrite = True\n        if tf.io.gfile.exists(filepath):",
        "detail": "Demo.EMSConformer.tensorflow_asr.helpers.exec_helpers",
        "documentation": {}
    },
    {
        "label": "prepare_featurizers",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.helpers.featurizer_helpers",
        "description": "Demo.EMSConformer.tensorflow_asr.helpers.featurizer_helpers",
        "peekOfCode": "def prepare_featurizers(\n    config: Config,\n    subwords: bool = True,\n    sentence_piece: bool = False,\n):\n    speech_featurizer = speech_featurizers.TFSpeechFeaturizer(config.speech_config)\n    if sentence_piece:\n        logger.info(\"Loading SentencePiece model ...\")\n        text_featurizer = text_featurizers.SentencePieceFeaturizer(config.decoder_config)\n    elif subwords:",
        "detail": "Demo.EMSConformer.tensorflow_asr.helpers.featurizer_helpers",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "Demo.EMSConformer.tensorflow_asr.helpers.featurizer_helpers",
        "description": "Demo.EMSConformer.tensorflow_asr.helpers.featurizer_helpers",
        "peekOfCode": "logger = tf.get_logger()\ndef prepare_featurizers(\n    config: Config,\n    subwords: bool = True,\n    sentence_piece: bool = False,\n):\n    speech_featurizer = speech_featurizers.TFSpeechFeaturizer(config.speech_config)\n    if sentence_piece:\n        logger.info(\"Loading SentencePiece model ...\")\n        text_featurizer = text_featurizers.SentencePieceFeaturizer(config.decoder_config)",
        "detail": "Demo.EMSConformer.tensorflow_asr.helpers.featurizer_helpers",
        "documentation": {}
    },
    {
        "label": "CtcLoss",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.losses.ctc_loss",
        "description": "Demo.EMSConformer.tensorflow_asr.losses.ctc_loss",
        "peekOfCode": "class CtcLoss(tf.keras.losses.Loss):\n    def __init__(\n        self,\n        blank=0,\n        global_batch_size=None,\n        name=None,\n    ):\n        super(CtcLoss, self).__init__(reduction=tf.keras.losses.Reduction.NONE, name=name)\n        self.blank = blank\n        self.global_batch_size = global_batch_size",
        "detail": "Demo.EMSConformer.tensorflow_asr.losses.ctc_loss",
        "documentation": {}
    },
    {
        "label": "ctc_loss",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.losses.ctc_loss",
        "description": "Demo.EMSConformer.tensorflow_asr.losses.ctc_loss",
        "peekOfCode": "def ctc_loss(\n    y_true,\n    y_pred,\n    input_length,\n    label_length,\n    blank,\n    name=None,\n):\n    return tf.nn.ctc_loss(\n        labels=tf.cast(y_true, tf.int32),",
        "detail": "Demo.EMSConformer.tensorflow_asr.losses.ctc_loss",
        "documentation": {}
    },
    {
        "label": "RnntLoss",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.losses.rnnt_loss",
        "description": "Demo.EMSConformer.tensorflow_asr.losses.rnnt_loss",
        "peekOfCode": "class RnntLoss(tf.keras.losses.Loss):\n    def __init__(\n        self,\n        blank=0,\n        global_batch_size=None,\n        name=None,\n    ):\n        super(RnntLoss, self).__init__(reduction=tf.keras.losses.Reduction.NONE, name=name)\n        self.blank = blank\n        self.global_batch_size = global_batch_size",
        "detail": "Demo.EMSConformer.tensorflow_asr.losses.rnnt_loss",
        "documentation": {}
    },
    {
        "label": "rnnt_loss",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.losses.rnnt_loss",
        "description": "Demo.EMSConformer.tensorflow_asr.losses.rnnt_loss",
        "peekOfCode": "def rnnt_loss(\n    logits,\n    labels,\n    label_length,\n    logit_length,\n    blank=0,\n    name=None,\n):\n    if use_warprnnt:\n        return rnnt_loss_warprnnt(",
        "detail": "Demo.EMSConformer.tensorflow_asr.losses.rnnt_loss",
        "documentation": {}
    },
    {
        "label": "rnnt_loss_warprnnt",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.losses.rnnt_loss",
        "description": "Demo.EMSConformer.tensorflow_asr.losses.rnnt_loss",
        "peekOfCode": "def rnnt_loss_warprnnt(\n    logits,\n    labels,\n    label_length,\n    logit_length,\n    blank=0,\n):\n    if not env_util.has_devices([\"GPU\", \"TPU\"]):\n        logits = tf.nn.log_softmax(logits)\n    loss = warp_rnnt_loss(",
        "detail": "Demo.EMSConformer.tensorflow_asr.losses.rnnt_loss",
        "documentation": {}
    },
    {
        "label": "nan_to_zero",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.losses.rnnt_loss",
        "description": "Demo.EMSConformer.tensorflow_asr.losses.rnnt_loss",
        "peekOfCode": "def nan_to_zero(\n    input_tensor,\n):\n    return tf.where(tf.math.is_nan(input_tensor), tf.zeros_like(input_tensor), input_tensor)\ndef reduce_logsumexp(\n    input_tensor,\n    axis,\n):\n    maximum = tf.reduce_max(input_tensor, axis=axis)\n    input_tensor = nan_to_zero(input_tensor - maximum)",
        "detail": "Demo.EMSConformer.tensorflow_asr.losses.rnnt_loss",
        "documentation": {}
    },
    {
        "label": "reduce_logsumexp",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.losses.rnnt_loss",
        "description": "Demo.EMSConformer.tensorflow_asr.losses.rnnt_loss",
        "peekOfCode": "def reduce_logsumexp(\n    input_tensor,\n    axis,\n):\n    maximum = tf.reduce_max(input_tensor, axis=axis)\n    input_tensor = nan_to_zero(input_tensor - maximum)\n    return tf.math.log(tf.reduce_sum(tf.exp(input_tensor), axis=axis)) + maximum\ndef extract_diagonals(\n    log_probs,\n):",
        "detail": "Demo.EMSConformer.tensorflow_asr.losses.rnnt_loss",
        "documentation": {}
    },
    {
        "label": "extract_diagonals",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.losses.rnnt_loss",
        "description": "Demo.EMSConformer.tensorflow_asr.losses.rnnt_loss",
        "peekOfCode": "def extract_diagonals(\n    log_probs,\n):\n    time_steps = tf.shape(log_probs)[1]  # T\n#    tf.print(\"time_steps: \", time_steps)\n    output_steps = tf.shape(log_probs)[2]  # U + 1\n#    tf.print(\"output_steps: \", output_steps)\n    reverse_log_probs = tf.reverse(log_probs, axis=[-1])\n    paddings = [[0, 0], [0, 0], [time_steps - 1, 0]]\n    padded_reverse_log_probs = tf.pad(reverse_log_probs, paddings, \"CONSTANT\", constant_values=LOG_0)",
        "detail": "Demo.EMSConformer.tensorflow_asr.losses.rnnt_loss",
        "documentation": {}
    },
    {
        "label": "transition_probs",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.losses.rnnt_loss",
        "description": "Demo.EMSConformer.tensorflow_asr.losses.rnnt_loss",
        "peekOfCode": "def transition_probs(\n    one_hot_labels,\n    log_probs,\n):\n    \"\"\"\n    :return: blank_probs with shape batch_size x input_max_len x target_max_len\n             truth_probs with shape batch_size x input_max_len x (target_max_len-1)\n    \"\"\"\n    blank_probs = log_probs[:, :, :, 0]\n    truth_probs = tf.reduce_sum(tf.multiply(log_probs[:, :, :-1, :], one_hot_labels), axis=-1)",
        "detail": "Demo.EMSConformer.tensorflow_asr.losses.rnnt_loss",
        "documentation": {}
    },
    {
        "label": "forward_dp",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.losses.rnnt_loss",
        "description": "Demo.EMSConformer.tensorflow_asr.losses.rnnt_loss",
        "peekOfCode": "def forward_dp(\n    bp_diags,\n    tp_diags,\n    batch_size,\n    input_max_len,\n    target_max_len,\n):\n    \"\"\"\n    :return: forward variable alpha with shape batch_size x input_max_len x target_max_len\n    \"\"\"",
        "detail": "Demo.EMSConformer.tensorflow_asr.losses.rnnt_loss",
        "documentation": {}
    },
    {
        "label": "backward_dp",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.losses.rnnt_loss",
        "description": "Demo.EMSConformer.tensorflow_asr.losses.rnnt_loss",
        "peekOfCode": "def backward_dp(\n    bp_diags,\n    tp_diags,\n    batch_size,\n    input_max_len,\n    target_max_len,\n    label_length,\n    logit_length,\n    blank_sl,\n):",
        "detail": "Demo.EMSConformer.tensorflow_asr.losses.rnnt_loss",
        "documentation": {}
    },
    {
        "label": "compute_rnnt_loss_and_grad_helper",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.losses.rnnt_loss",
        "description": "Demo.EMSConformer.tensorflow_asr.losses.rnnt_loss",
        "peekOfCode": "def compute_rnnt_loss_and_grad_helper(logits, labels, label_length, logit_length):\n    batch_size = tf.shape(logits)[0]\n    input_max_len = tf.shape(logits)[1]\n    target_max_len = tf.shape(logits)[2]\n    vocab_size = tf.shape(logits)[3]\n    one_hot_labels = tf.one_hot(\n        tf.tile(tf.expand_dims(labels, axis=1), multiples=[1, input_max_len, 1]),\n        depth=vocab_size,\n    )\n    log_probs = tf.nn.log_softmax(logits)",
        "detail": "Demo.EMSConformer.tensorflow_asr.losses.rnnt_loss",
        "documentation": {}
    },
    {
        "label": "rnnt_loss_tf",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.losses.rnnt_loss",
        "description": "Demo.EMSConformer.tensorflow_asr.losses.rnnt_loss",
        "peekOfCode": "def rnnt_loss_tf(\n    logits,\n    labels,\n    label_length,\n    logit_length,\n    name=None,\n):\n    name = \"rnnt_loss\" if name is None else name\n    with tf.name_scope(name):\n        logits = tf.convert_to_tensor(logits, name=\"logits\")",
        "detail": "Demo.EMSConformer.tensorflow_asr.losses.rnnt_loss",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "Demo.EMSConformer.tensorflow_asr.losses.rnnt_loss",
        "description": "Demo.EMSConformer.tensorflow_asr.losses.rnnt_loss",
        "peekOfCode": "logger = tf.get_logger()\nLOG_0 = float(\"-inf\")\ntry:\n    from warprnnt_tensorflow import rnnt_loss as warp_rnnt_loss\n    use_warprnnt = True\n    logger.info(\"Use RNNT loss in WarpRnnt\")\nexcept ImportError:\n    logger.info(\"Use RNNT loss in TensorFlow\")\n    use_warprnnt = False\nclass RnntLoss(tf.keras.losses.Loss):",
        "detail": "Demo.EMSConformer.tensorflow_asr.losses.rnnt_loss",
        "documentation": {}
    },
    {
        "label": "LOG_0",
        "kind": 5,
        "importPath": "Demo.EMSConformer.tensorflow_asr.losses.rnnt_loss",
        "description": "Demo.EMSConformer.tensorflow_asr.losses.rnnt_loss",
        "peekOfCode": "LOG_0 = float(\"-inf\")\ntry:\n    from warprnnt_tensorflow import rnnt_loss as warp_rnnt_loss\n    use_warprnnt = True\n    logger.info(\"Use RNNT loss in WarpRnnt\")\nexcept ImportError:\n    logger.info(\"Use RNNT loss in TensorFlow\")\n    use_warprnnt = False\nclass RnntLoss(tf.keras.losses.Loss):\n    def __init__(",
        "detail": "Demo.EMSConformer.tensorflow_asr.losses.rnnt_loss",
        "documentation": {}
    },
    {
        "label": "ErrorRate",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.metrics.error_rates",
        "description": "Demo.EMSConformer.tensorflow_asr.metrics.error_rates",
        "peekOfCode": "class ErrorRate(tf.keras.metrics.Metric):\n    \"\"\"Metric for WER or CER\"\"\"\n    def __init__(\n        self,\n        func,\n        name=\"error_rate\",\n        **kwargs,\n    ):\n        super(ErrorRate, self).__init__(name=name, **kwargs)\n        self.numerator = self.add_weight(name=f\"{name}_numerator\", initializer=\"zeros\")",
        "detail": "Demo.EMSConformer.tensorflow_asr.metrics.error_rates",
        "documentation": {}
    },
    {
        "label": "GLU",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.models.activations.glu",
        "description": "Demo.EMSConformer.tensorflow_asr.models.activations.glu",
        "peekOfCode": "class GLU(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        axis=-1,\n        name=\"glu_activation\",\n        **kwargs,\n    ):\n        super(GLU, self).__init__(name=name, **kwargs)\n        self.axis = axis\n    def call(",
        "detail": "Demo.EMSConformer.tensorflow_asr.models.activations.glu",
        "documentation": {}
    },
    {
        "label": "CtcModel",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.models.ctc.base_ctc",
        "description": "Demo.EMSConformer.tensorflow_asr.models.ctc.base_ctc",
        "peekOfCode": "class CtcModel(BaseModel):\n    def __init__(\n        self,\n        encoder: tf.keras.Model,\n        decoder: Union[tf.keras.Model, tf.keras.layers.Layer] = None,\n        vocabulary_size: int = None,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.encoder = encoder",
        "detail": "Demo.EMSConformer.tensorflow_asr.models.ctc.base_ctc",
        "documentation": {}
    },
    {
        "label": "Reshape",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.models.ctc.deepspeech2",
        "description": "Demo.EMSConformer.tensorflow_asr.models.ctc.deepspeech2",
        "peekOfCode": "class Reshape(tf.keras.layers.Layer):\n    def call(\n        self,\n        inputs,\n    ):\n        return math_util.merge_two_last_dims(inputs)\nclass ConvBlock(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        conv_type: str = \"conv2d\",",
        "detail": "Demo.EMSConformer.tensorflow_asr.models.ctc.deepspeech2",
        "documentation": {}
    },
    {
        "label": "ConvBlock",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.models.ctc.deepspeech2",
        "description": "Demo.EMSConformer.tensorflow_asr.models.ctc.deepspeech2",
        "peekOfCode": "class ConvBlock(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        conv_type: str = \"conv2d\",\n        kernels: list = [11, 41],\n        strides: list = [2, 2],\n        filters: int = 32,\n        dropout: float = 0.1,\n        **kwargs,\n    ):",
        "detail": "Demo.EMSConformer.tensorflow_asr.models.ctc.deepspeech2",
        "documentation": {}
    },
    {
        "label": "ConvModule",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.models.ctc.deepspeech2",
        "description": "Demo.EMSConformer.tensorflow_asr.models.ctc.deepspeech2",
        "peekOfCode": "class ConvModule(tf.keras.Model):\n    def __init__(\n        self,\n        conv_type: str = \"conv2d\",\n        kernels: list = [[11, 41], [11, 21], [11, 21]],\n        strides: list = [[2, 2], [1, 2], [1, 2]],\n        filters: list = [32, 32, 96],\n        dropout: float = 0.1,\n        **kwargs,\n    ):",
        "detail": "Demo.EMSConformer.tensorflow_asr.models.ctc.deepspeech2",
        "documentation": {}
    },
    {
        "label": "RnnBlock",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.models.ctc.deepspeech2",
        "description": "Demo.EMSConformer.tensorflow_asr.models.ctc.deepspeech2",
        "peekOfCode": "class RnnBlock(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        rnn_type: str = \"lstm\",\n        units: int = 1024,\n        bidirectional: bool = True,\n        rowconv: int = 0,\n        dropout: float = 0.1,\n        **kwargs,\n    ):",
        "detail": "Demo.EMSConformer.tensorflow_asr.models.ctc.deepspeech2",
        "documentation": {}
    },
    {
        "label": "RnnModule",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.models.ctc.deepspeech2",
        "description": "Demo.EMSConformer.tensorflow_asr.models.ctc.deepspeech2",
        "peekOfCode": "class RnnModule(tf.keras.Model):\n    def __init__(\n        self,\n        nlayers: int = 5,\n        rnn_type: str = \"lstm\",\n        units: int = 1024,\n        bidirectional: bool = True,\n        rowconv: int = 0,\n        dropout: float = 0.1,\n        **kwargs,",
        "detail": "Demo.EMSConformer.tensorflow_asr.models.ctc.deepspeech2",
        "documentation": {}
    },
    {
        "label": "FcBlock",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.models.ctc.deepspeech2",
        "description": "Demo.EMSConformer.tensorflow_asr.models.ctc.deepspeech2",
        "peekOfCode": "class FcBlock(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        units: int = 1024,\n        dropout: float = 0.1,\n        **kwargs,\n    ):\n        super(FcBlock, self).__init__(**kwargs)\n        self.fc = tf.keras.layers.Dense(units, name=f\"{self.name}_fc\")\n        self.bn = tf.keras.layers.BatchNormalization(name=f\"{self.name}_bn\")",
        "detail": "Demo.EMSConformer.tensorflow_asr.models.ctc.deepspeech2",
        "documentation": {}
    },
    {
        "label": "FcModule",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.models.ctc.deepspeech2",
        "description": "Demo.EMSConformer.tensorflow_asr.models.ctc.deepspeech2",
        "peekOfCode": "class FcModule(tf.keras.Model):\n    def __init__(\n        self,\n        nlayers: int = 0,\n        units: int = 1024,\n        dropout: float = 0.1,\n        **kwargs,\n    ):\n        super(FcModule, self).__init__(**kwargs)\n        self.blocks = [FcBlock(units=units, dropout=dropout, name=f\"{self.name}_block_{i}\") for i in range(nlayers)]",
        "detail": "Demo.EMSConformer.tensorflow_asr.models.ctc.deepspeech2",
        "documentation": {}
    },
    {
        "label": "DeepSpeech2Encoder",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.models.ctc.deepspeech2",
        "description": "Demo.EMSConformer.tensorflow_asr.models.ctc.deepspeech2",
        "peekOfCode": "class DeepSpeech2Encoder(tf.keras.Model):\n    def __init__(\n        self,\n        conv_type: str = \"conv2d\",\n        conv_kernels: list = [[11, 41], [11, 21], [11, 21]],\n        conv_strides: list = [[2, 2], [1, 2], [1, 2]],\n        conv_filters: list = [32, 32, 96],\n        conv_dropout: float = 0.1,\n        rnn_nlayers: int = 5,\n        rnn_type: str = \"lstm\",",
        "detail": "Demo.EMSConformer.tensorflow_asr.models.ctc.deepspeech2",
        "documentation": {}
    },
    {
        "label": "DeepSpeech2",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.models.ctc.deepspeech2",
        "description": "Demo.EMSConformer.tensorflow_asr.models.ctc.deepspeech2",
        "peekOfCode": "class DeepSpeech2(CtcModel):\n    def __init__(\n        self,\n        vocabulary_size: int,\n        conv_type: str = \"conv2d\",\n        conv_kernels: list = [[11, 41], [11, 21], [11, 21]],\n        conv_strides: list = [[2, 2], [1, 2], [1, 2]],\n        conv_filters: list = [32, 32, 96],\n        conv_dropout: float = 0.1,\n        rnn_nlayers: int = 5,",
        "detail": "Demo.EMSConformer.tensorflow_asr.models.ctc.deepspeech2",
        "documentation": {}
    },
    {
        "label": "Reshape",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.models.ctc.jasper",
        "description": "Demo.EMSConformer.tensorflow_asr.models.ctc.jasper",
        "peekOfCode": "class Reshape(tf.keras.layers.Layer):\n    def call(self, inputs):\n        return math_util.merge_two_last_dims(inputs)\nclass JasperSubBlock(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        channels: int = 256,\n        kernels: int = 11,\n        strides: int = 1,\n        dropout: float = 0.1,",
        "detail": "Demo.EMSConformer.tensorflow_asr.models.ctc.jasper",
        "documentation": {}
    },
    {
        "label": "JasperSubBlock",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.models.ctc.jasper",
        "description": "Demo.EMSConformer.tensorflow_asr.models.ctc.jasper",
        "peekOfCode": "class JasperSubBlock(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        channels: int = 256,\n        kernels: int = 11,\n        strides: int = 1,\n        dropout: float = 0.1,\n        dilation: int = 1,\n        kernel_regularizer=None,\n        bias_regularizer=None,",
        "detail": "Demo.EMSConformer.tensorflow_asr.models.ctc.jasper",
        "documentation": {}
    },
    {
        "label": "JasperResidual",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.models.ctc.jasper",
        "description": "Demo.EMSConformer.tensorflow_asr.models.ctc.jasper",
        "peekOfCode": "class JasperResidual(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        channels: int = 256,\n        kernel_regularizer=None,\n        bias_regularizer=None,\n        **kwargs,\n    ):\n        super(JasperResidual, self).__init__(**kwargs)\n        self.pointwise_conv1d = tf.keras.layers.Conv1D(",
        "detail": "Demo.EMSConformer.tensorflow_asr.models.ctc.jasper",
        "documentation": {}
    },
    {
        "label": "JasperSubBlockResidual",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.models.ctc.jasper",
        "description": "Demo.EMSConformer.tensorflow_asr.models.ctc.jasper",
        "peekOfCode": "class JasperSubBlockResidual(JasperSubBlock):\n    def __init__(\n        self,\n        channels: int = 256,\n        kernels: int = 11,\n        strides: int = 1,\n        dropout: float = 0.1,\n        dilation: int = 1,\n        nresiduals: int = 1,\n        kernel_regularizer=None,",
        "detail": "Demo.EMSConformer.tensorflow_asr.models.ctc.jasper",
        "documentation": {}
    },
    {
        "label": "JasperBlock",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.models.ctc.jasper",
        "description": "Demo.EMSConformer.tensorflow_asr.models.ctc.jasper",
        "peekOfCode": "class JasperBlock(tf.keras.Model):\n    def __init__(\n        self,\n        nsubblocks: int = 3,\n        channels: int = 256,\n        kernels: int = 11,\n        dropout: float = 0.1,\n        dense: bool = False,\n        nresiduals: int = 1,\n        kernel_regularizer=None,",
        "detail": "Demo.EMSConformer.tensorflow_asr.models.ctc.jasper",
        "documentation": {}
    },
    {
        "label": "JasperEncoder",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.models.ctc.jasper",
        "description": "Demo.EMSConformer.tensorflow_asr.models.ctc.jasper",
        "peekOfCode": "class JasperEncoder(tf.keras.Model):\n    def __init__(\n        self,\n        dense: bool = False,\n        first_additional_block_channels: int = 256,\n        first_additional_block_kernels: int = 11,\n        first_additional_block_strides: int = 2,\n        first_additional_block_dilation: int = 1,\n        first_additional_block_dropout: int = 0.2,\n        nsubblocks: int = 5,",
        "detail": "Demo.EMSConformer.tensorflow_asr.models.ctc.jasper",
        "documentation": {}
    },
    {
        "label": "Jasper",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.models.ctc.jasper",
        "description": "Demo.EMSConformer.tensorflow_asr.models.ctc.jasper",
        "peekOfCode": "class Jasper(CtcModel):\n    def __init__(\n        self,\n        vocabulary_size: int,\n        dense: bool = False,\n        first_additional_block_channels: int = 256,\n        first_additional_block_kernels: int = 11,\n        first_additional_block_strides: int = 2,\n        first_additional_block_dilation: int = 1,\n        first_additional_block_dropout: int = 0.2,",
        "detail": "Demo.EMSConformer.tensorflow_asr.models.ctc.jasper",
        "documentation": {}
    },
    {
        "label": "FFModule",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.models.encoders.conformer",
        "description": "Demo.EMSConformer.tensorflow_asr.models.encoders.conformer",
        "peekOfCode": "class FFModule(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        input_dim,\n        dropout=0.0,\n        fc_factor=0.5,\n        kernel_regularizer=L2,\n        bias_regularizer=L2,\n        name=\"ff_module\",\n        **kwargs,",
        "detail": "Demo.EMSConformer.tensorflow_asr.models.encoders.conformer",
        "documentation": {}
    },
    {
        "label": "MHSAModule",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.models.encoders.conformer",
        "description": "Demo.EMSConformer.tensorflow_asr.models.encoders.conformer",
        "peekOfCode": "class MHSAModule(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        head_size,\n        num_heads,\n        dropout=0.0,\n        mha_type=\"relmha\",\n        kernel_regularizer=L2,\n        bias_regularizer=L2,\n        name=\"mhsa_module\",",
        "detail": "Demo.EMSConformer.tensorflow_asr.models.encoders.conformer",
        "documentation": {}
    },
    {
        "label": "ConvModule",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.models.encoders.conformer",
        "description": "Demo.EMSConformer.tensorflow_asr.models.encoders.conformer",
        "peekOfCode": "class ConvModule(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        input_dim,\n        kernel_size=32,\n        dropout=0.0,\n        depth_multiplier=1,\n        kernel_regularizer=L2,\n        bias_regularizer=L2,\n        name=\"conv_module\",",
        "detail": "Demo.EMSConformer.tensorflow_asr.models.encoders.conformer",
        "documentation": {}
    },
    {
        "label": "ConformerBlock",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.models.encoders.conformer",
        "description": "Demo.EMSConformer.tensorflow_asr.models.encoders.conformer",
        "peekOfCode": "class ConformerBlock(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        input_dim,\n        dropout=0.0,\n        fc_factor=0.5,\n        head_size=36,\n        num_heads=4,\n        mha_type=\"relmha\",\n        kernel_size=32,",
        "detail": "Demo.EMSConformer.tensorflow_asr.models.encoders.conformer",
        "documentation": {}
    },
    {
        "label": "ConformerEncoder",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.models.encoders.conformer",
        "description": "Demo.EMSConformer.tensorflow_asr.models.encoders.conformer",
        "peekOfCode": "class ConformerEncoder(tf.keras.Model):\n    def __init__(\n        self,\n        subsampling,\n        positional_encoding=\"sinusoid\",\n        dmodel=144,\n        num_blocks=16,\n        mha_type=\"relmha\",\n        head_size=36,\n        num_heads=4,",
        "detail": "Demo.EMSConformer.tensorflow_asr.models.encoders.conformer",
        "documentation": {}
    },
    {
        "label": "L2",
        "kind": 5,
        "importPath": "Demo.EMSConformer.tensorflow_asr.models.encoders.conformer",
        "description": "Demo.EMSConformer.tensorflow_asr.models.encoders.conformer",
        "peekOfCode": "L2 = tf.keras.regularizers.l2(1e-6)\nclass FFModule(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        input_dim,\n        dropout=0.0,\n        fc_factor=0.5,\n        kernel_regularizer=L2,\n        bias_regularizer=L2,\n        name=\"ff_module\",",
        "detail": "Demo.EMSConformer.tensorflow_asr.models.encoders.conformer",
        "documentation": {}
    },
    {
        "label": "Reshape",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.models.encoders.contextnet",
        "description": "Demo.EMSConformer.tensorflow_asr.models.encoders.contextnet",
        "peekOfCode": "class Reshape(tf.keras.layers.Layer):\n    def call(self, inputs):\n        return math_util.merge_two_last_dims(inputs)\nclass ConvModule(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        kernel_size: int = 3,\n        strides: int = 1,\n        filters: int = 256,\n        activation: str = \"silu\",",
        "detail": "Demo.EMSConformer.tensorflow_asr.models.encoders.contextnet",
        "documentation": {}
    },
    {
        "label": "ConvModule",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.models.encoders.contextnet",
        "description": "Demo.EMSConformer.tensorflow_asr.models.encoders.contextnet",
        "peekOfCode": "class ConvModule(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        kernel_size: int = 3,\n        strides: int = 1,\n        filters: int = 256,\n        activation: str = \"silu\",\n        kernel_regularizer=None,\n        bias_regularizer=None,\n        **kwargs,",
        "detail": "Demo.EMSConformer.tensorflow_asr.models.encoders.contextnet",
        "documentation": {}
    },
    {
        "label": "SEModule",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.models.encoders.contextnet",
        "description": "Demo.EMSConformer.tensorflow_asr.models.encoders.contextnet",
        "peekOfCode": "class SEModule(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        kernel_size: int = 3,\n        strides: int = 1,\n        filters: int = 256,\n        activation: str = \"silu\",\n        kernel_regularizer=None,\n        bias_regularizer=None,\n        **kwargs,",
        "detail": "Demo.EMSConformer.tensorflow_asr.models.encoders.contextnet",
        "documentation": {}
    },
    {
        "label": "ConvBlock",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.models.encoders.contextnet",
        "description": "Demo.EMSConformer.tensorflow_asr.models.encoders.contextnet",
        "peekOfCode": "class ConvBlock(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        nlayers: int = 3,\n        kernel_size: int = 3,\n        filters: int = 256,\n        strides: int = 1,\n        residual: bool = True,\n        activation: str = \"silu\",\n        alpha: float = 1.0,",
        "detail": "Demo.EMSConformer.tensorflow_asr.models.encoders.contextnet",
        "documentation": {}
    },
    {
        "label": "ContextNetEncoder",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.models.encoders.contextnet",
        "description": "Demo.EMSConformer.tensorflow_asr.models.encoders.contextnet",
        "peekOfCode": "class ContextNetEncoder(tf.keras.Model):\n    def __init__(\n        self,\n        blocks: List[dict] = [],\n        alpha: float = 1.0,\n        kernel_regularizer=None,\n        bias_regularizer=None,\n        **kwargs,\n    ):\n        super(ContextNetEncoder, self).__init__(**kwargs)",
        "detail": "Demo.EMSConformer.tensorflow_asr.models.encoders.contextnet",
        "documentation": {}
    },
    {
        "label": "get_activation",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.models.encoders.contextnet",
        "description": "Demo.EMSConformer.tensorflow_asr.models.encoders.contextnet",
        "peekOfCode": "def get_activation(\n    activation: str = \"silu\",\n):\n    activation = activation.lower()\n    if activation in [\"silu\", \"swish\"]:\n        return tf.nn.swish\n    elif activation == \"relu\":\n        return tf.nn.relu\n    elif activation == \"linear\":\n        return tf.keras.activations.linear",
        "detail": "Demo.EMSConformer.tensorflow_asr.models.encoders.contextnet",
        "documentation": {}
    },
    {
        "label": "L2",
        "kind": 5,
        "importPath": "Demo.EMSConformer.tensorflow_asr.models.encoders.contextnet",
        "description": "Demo.EMSConformer.tensorflow_asr.models.encoders.contextnet",
        "peekOfCode": "L2 = tf.keras.regularizers.l2(1e-6)\ndef get_activation(\n    activation: str = \"silu\",\n):\n    activation = activation.lower()\n    if activation in [\"silu\", \"swish\"]:\n        return tf.nn.swish\n    elif activation == \"relu\":\n        return tf.nn.relu\n    elif activation == \"linear\":",
        "detail": "Demo.EMSConformer.tensorflow_asr.models.encoders.contextnet",
        "documentation": {}
    },
    {
        "label": "BNLSTMCell",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.models.layers.bnlstmcell",
        "description": "Demo.EMSConformer.tensorflow_asr.models.layers.bnlstmcell",
        "peekOfCode": "class BNLSTMCell(tf.keras.layers.LSTMCell):\n    def __init__(\n        self,\n        *args,\n        **kwargs,\n    ):\n        super().__init__(*args, **kwargs)\n        self.beta = self.add_weight(\n            shape=(self.units * 4,),\n            name=\"lstm_bn_beta\",",
        "detail": "Demo.EMSConformer.tensorflow_asr.models.layers.bnlstmcell",
        "documentation": {}
    },
    {
        "label": "ds2_rnn_batch_norm",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.models.layers.bnlstmcell",
        "description": "Demo.EMSConformer.tensorflow_asr.models.layers.bnlstmcell",
        "peekOfCode": "def ds2_rnn_batch_norm(\n    x_i,\n    x_f,\n    x_c,\n    x_o,\n    beta=None,\n    gamma=None,\n):\n    # x is input * weight with shape [batch_size, units * 4]\n    # Merge into single array of features",
        "detail": "Demo.EMSConformer.tensorflow_asr.models.layers.bnlstmcell",
        "documentation": {}
    },
    {
        "label": "Embedding",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.models.layers.embedding",
        "description": "Demo.EMSConformer.tensorflow_asr.models.layers.embedding",
        "peekOfCode": "class Embedding(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        vocab_size,\n        embed_dim,\n        contraint=None,\n        regularizer=None,\n        initializer=None,\n        **kwargs,\n    ):",
        "detail": "Demo.EMSConformer.tensorflow_asr.models.layers.embedding",
        "documentation": {}
    },
    {
        "label": "MultiHeadAttention",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.models.layers.multihead_attention",
        "description": "Demo.EMSConformer.tensorflow_asr.models.layers.multihead_attention",
        "peekOfCode": "class MultiHeadAttention(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        num_heads,\n        head_size,\n        output_size: int = None,\n        dropout: float = 0.0,\n        use_projection_bias: bool = True,\n        return_attn_coef: bool = False,\n        kernel_initializer: typing.Union[str, typing.Callable] = \"glorot_uniform\",",
        "detail": "Demo.EMSConformer.tensorflow_asr.models.layers.multihead_attention",
        "documentation": {}
    },
    {
        "label": "RelPositionMultiHeadAttention",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.models.layers.multihead_attention",
        "description": "Demo.EMSConformer.tensorflow_asr.models.layers.multihead_attention",
        "peekOfCode": "class RelPositionMultiHeadAttention(MultiHeadAttention):\n    def build(\n        self,\n        input_shape,\n    ):\n        num_pos_features = input_shape[-1][-1]\n        self.pos_kernel = self.add_weight(\n            name=\"pos_kernel\",\n            shape=[self.num_heads, num_pos_features, self.head_size],\n            initializer=self.kernel_initializer,",
        "detail": "Demo.EMSConformer.tensorflow_asr.models.layers.multihead_attention",
        "documentation": {}
    },
    {
        "label": "PointWiseFFN",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.models.layers.point_wise_ffn",
        "description": "Demo.EMSConformer.tensorflow_asr.models.layers.point_wise_ffn",
        "peekOfCode": "class PointWiseFFN(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        size,\n        output_size,\n        activation=\"relu\",\n        dropout=0.1,\n        name=\"point_wise_ffn\",\n        **kwargs,\n    ):",
        "detail": "Demo.EMSConformer.tensorflow_asr.models.layers.point_wise_ffn",
        "documentation": {}
    },
    {
        "label": "PositionalEncoding",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.models.layers.positional_encoding",
        "description": "Demo.EMSConformer.tensorflow_asr.models.layers.positional_encoding",
        "peekOfCode": "class PositionalEncoding(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        alpha: int = 1,\n        beta: int = 0,\n        name=\"positional_encoding\",\n        **kwargs,\n    ):\n        super().__init__(trainable=False, name=name, **kwargs)\n        self.alpha = alpha",
        "detail": "Demo.EMSConformer.tensorflow_asr.models.layers.positional_encoding",
        "documentation": {}
    },
    {
        "label": "PositionalEncodingConcat",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.models.layers.positional_encoding",
        "description": "Demo.EMSConformer.tensorflow_asr.models.layers.positional_encoding",
        "peekOfCode": "class PositionalEncodingConcat(PositionalEncoding):\n    def build(\n        self,\n        input_shape,\n    ):\n        dmodel = input_shape[-1]\n        assert dmodel % 2 == 0, f\"Input last dim must be even: {dmodel}\"\n    @staticmethod\n    def encode(\n        max_len,",
        "detail": "Demo.EMSConformer.tensorflow_asr.models.layers.positional_encoding",
        "documentation": {}
    },
    {
        "label": "RowConv1D",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.models.layers.row_conv_1d",
        "description": "Demo.EMSConformer.tensorflow_asr.models.layers.row_conv_1d",
        "peekOfCode": "class RowConv1D(tf.keras.layers.Conv1D):\n    def __init__(\n        self,\n        filters,\n        future_context,\n        **kwargs,\n    ):\n        assert future_context >= 0, \"Future context must be positive\"\n        super().__init__(filters=filters, kernel_size=(future_context * 2 + 1), **kwargs)\n        self.future_context = future_context",
        "detail": "Demo.EMSConformer.tensorflow_asr.models.layers.row_conv_1d",
        "documentation": {}
    },
    {
        "label": "SequenceBatchNorm",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.models.layers.sequence_wise_bn",
        "description": "Demo.EMSConformer.tensorflow_asr.models.layers.sequence_wise_bn",
        "peekOfCode": "class SequenceBatchNorm(tf.keras.layers.Layer):\n    def __init__(self, name, time_major=False, **kwargs):\n        super(SequenceBatchNorm, self).__init__(name=name, **kwargs)\n        self.time_major = time_major\n    def build(\n        self,\n        input_shape,\n    ):\n        self.beta = self.add_weight(\n            shape=[input_shape[-1]],",
        "detail": "Demo.EMSConformer.tensorflow_asr.models.layers.sequence_wise_bn",
        "documentation": {}
    },
    {
        "label": "TimeReduction",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.models.layers.subsampling",
        "description": "Demo.EMSConformer.tensorflow_asr.models.layers.subsampling",
        "peekOfCode": "class TimeReduction(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        factor: int,\n        name: str = \"TimeReduction\",\n        **kwargs,\n    ):\n        super(TimeReduction, self).__init__(name=name, **kwargs)\n        self.time_reduction_factor = factor\n    def padding(",
        "detail": "Demo.EMSConformer.tensorflow_asr.models.layers.subsampling",
        "documentation": {}
    },
    {
        "label": "VggSubsampling",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.models.layers.subsampling",
        "description": "Demo.EMSConformer.tensorflow_asr.models.layers.subsampling",
        "peekOfCode": "class VggSubsampling(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        filters: tuple or list = (32, 64),\n        kernel_size: int or list or tuple = 3,\n        strides: int or list or tuple = 2,\n        kernel_regularizer=None,\n        bias_regularizer=None,\n        name=\"VggSubsampling\",\n        **kwargs,",
        "detail": "Demo.EMSConformer.tensorflow_asr.models.layers.subsampling",
        "documentation": {}
    },
    {
        "label": "Conv2dSubsampling",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.models.layers.subsampling",
        "description": "Demo.EMSConformer.tensorflow_asr.models.layers.subsampling",
        "peekOfCode": "class Conv2dSubsampling(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        filters: int,\n        strides: list or tuple or int = 2,\n        kernel_size: int or list or tuple = 3,\n        kernel_regularizer=None,\n        bias_regularizer=None,\n        name=\"Conv2dSubsampling\",\n        **kwargs,",
        "detail": "Demo.EMSConformer.tensorflow_asr.models.layers.subsampling",
        "documentation": {}
    },
    {
        "label": "TransducerPrediction",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.models.transducer.base_transducer",
        "description": "Demo.EMSConformer.tensorflow_asr.models.transducer.base_transducer",
        "peekOfCode": "class TransducerPrediction(tf.keras.Model):\n    def __init__(\n        self,\n        vocabulary_size: int,\n        embed_dim: int,\n        embed_dropout: float = 0,\n        num_rnns: int = 1,\n        rnn_units: int = 512,\n        rnn_type: str = \"lstm\",\n        rnn_implementation: int = 2,",
        "detail": "Demo.EMSConformer.tensorflow_asr.models.transducer.base_transducer",
        "documentation": {}
    },
    {
        "label": "TransducerJointReshape",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.models.transducer.base_transducer",
        "description": "Demo.EMSConformer.tensorflow_asr.models.transducer.base_transducer",
        "peekOfCode": "class TransducerJointReshape(tf.keras.layers.Layer):\n    def __init__(self, axis: int = 1, name=\"transducer_joint_reshape\", **kwargs):\n        super().__init__(name=name, trainable=False, **kwargs)\n        self.axis = axis\n    def call(self, inputs, repeats=None, **kwargs):\n        outputs = tf.expand_dims(inputs, axis=self.axis)\n        return tf.repeat(outputs, repeats=repeats, axis=self.axis)\n    def get_config(self):\n        conf = super().get_config()\n        conf.update({\"axis\": self.axis})",
        "detail": "Demo.EMSConformer.tensorflow_asr.models.transducer.base_transducer",
        "documentation": {}
    },
    {
        "label": "TransducerJoint",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.models.transducer.base_transducer",
        "description": "Demo.EMSConformer.tensorflow_asr.models.transducer.base_transducer",
        "peekOfCode": "class TransducerJoint(tf.keras.Model):\n    def __init__(\n        self,\n        vocabulary_size: int,\n        joint_dim: int = 1024,\n        activation: str = \"tanh\",\n        prejoint_linear: bool = True,\n        postjoint_linear: bool = False,\n        joint_mode: str = \"add\",\n        kernel_regularizer=None,",
        "detail": "Demo.EMSConformer.tensorflow_asr.models.transducer.base_transducer",
        "documentation": {}
    },
    {
        "label": "Transducer",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.models.transducer.base_transducer",
        "description": "Demo.EMSConformer.tensorflow_asr.models.transducer.base_transducer",
        "peekOfCode": "class Transducer(BaseModel):\n    \"\"\"Transducer Model Warper\"\"\"\n    def __init__(\n        self,\n        encoder: tf.keras.Model,\n        vocabulary_size: int,\n        embed_dim: int = 512,\n        embed_dropout: float = 0,\n        num_rnns: int = 1,\n        rnn_units: int = 320,",
        "detail": "Demo.EMSConformer.tensorflow_asr.models.transducer.base_transducer",
        "documentation": {}
    },
    {
        "label": "Hypothesis",
        "kind": 5,
        "importPath": "Demo.EMSConformer.tensorflow_asr.models.transducer.base_transducer",
        "description": "Demo.EMSConformer.tensorflow_asr.models.transducer.base_transducer",
        "peekOfCode": "Hypothesis = collections.namedtuple(\"Hypothesis\", (\"index\", \"prediction\", \"states\"))\nBeamHypothesis = collections.namedtuple(\"BeamHypothesis\", (\"score\", \"indices\", \"prediction\", \"states\"))\nclass TransducerPrediction(tf.keras.Model):\n    def __init__(\n        self,\n        vocabulary_size: int,\n        embed_dim: int,\n        embed_dropout: float = 0,\n        num_rnns: int = 1,\n        rnn_units: int = 512,",
        "detail": "Demo.EMSConformer.tensorflow_asr.models.transducer.base_transducer",
        "documentation": {}
    },
    {
        "label": "BeamHypothesis",
        "kind": 5,
        "importPath": "Demo.EMSConformer.tensorflow_asr.models.transducer.base_transducer",
        "description": "Demo.EMSConformer.tensorflow_asr.models.transducer.base_transducer",
        "peekOfCode": "BeamHypothesis = collections.namedtuple(\"BeamHypothesis\", (\"score\", \"indices\", \"prediction\", \"states\"))\nclass TransducerPrediction(tf.keras.Model):\n    def __init__(\n        self,\n        vocabulary_size: int,\n        embed_dim: int,\n        embed_dropout: float = 0,\n        num_rnns: int = 1,\n        rnn_units: int = 512,\n        rnn_type: str = \"lstm\",",
        "detail": "Demo.EMSConformer.tensorflow_asr.models.transducer.base_transducer",
        "documentation": {}
    },
    {
        "label": "Conformer",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.models.transducer.conformer",
        "description": "Demo.EMSConformer.tensorflow_asr.models.transducer.conformer",
        "peekOfCode": "class Conformer(Transducer):\n    def __init__(\n        self,\n        vocabulary_size: int,\n        encoder_subsampling: dict,\n        encoder_positional_encoding: str = \"sinusoid\",\n        encoder_dmodel: int = 144,\n        encoder_num_blocks: int = 16,\n        encoder_head_size: int = 36,\n        encoder_num_heads: int = 4,",
        "detail": "Demo.EMSConformer.tensorflow_asr.models.transducer.conformer",
        "documentation": {}
    },
    {
        "label": "ContextNet",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.models.transducer.contextnet",
        "description": "Demo.EMSConformer.tensorflow_asr.models.transducer.contextnet",
        "peekOfCode": "class ContextNet(Transducer):\n    def __init__(\n        self,\n        vocabulary_size: int,\n        encoder_blocks: List[dict],\n        encoder_alpha: float = 0.5,\n        encoder_trainable: bool = True,\n        prediction_embed_dim: int = 512,\n        prediction_embed_dropout: int = 0,\n        prediction_num_rnns: int = 1,",
        "detail": "Demo.EMSConformer.tensorflow_asr.models.transducer.contextnet",
        "documentation": {}
    },
    {
        "label": "Reshape",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.models.transducer.rnn_transducer",
        "description": "Demo.EMSConformer.tensorflow_asr.models.transducer.rnn_transducer",
        "peekOfCode": "class Reshape(tf.keras.layers.Layer):\n    def call(self, inputs):\n        return math_util.merge_two_last_dims(inputs)\nclass RnnTransducerBlock(tf.keras.Model):\n    def __init__(\n        self,\n        reduction_factor: int = 0,\n        dmodel: int = 640,\n        rnn_type: str = \"lstm\",\n        rnn_units: int = 2048,",
        "detail": "Demo.EMSConformer.tensorflow_asr.models.transducer.rnn_transducer",
        "documentation": {}
    },
    {
        "label": "RnnTransducerBlock",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.models.transducer.rnn_transducer",
        "description": "Demo.EMSConformer.tensorflow_asr.models.transducer.rnn_transducer",
        "peekOfCode": "class RnnTransducerBlock(tf.keras.Model):\n    def __init__(\n        self,\n        reduction_factor: int = 0,\n        dmodel: int = 640,\n        rnn_type: str = \"lstm\",\n        rnn_units: int = 2048,\n        layer_norm: bool = True,\n        kernel_regularizer=None,\n        bias_regularizer=None,",
        "detail": "Demo.EMSConformer.tensorflow_asr.models.transducer.rnn_transducer",
        "documentation": {}
    },
    {
        "label": "RnnTransducerEncoder",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.models.transducer.rnn_transducer",
        "description": "Demo.EMSConformer.tensorflow_asr.models.transducer.rnn_transducer",
        "peekOfCode": "class RnnTransducerEncoder(tf.keras.Model):\n    def __init__(\n        self,\n        reductions: dict = {0: 3, 1: 2},\n        dmodel: int = 640,\n        nlayers: int = 8,\n        rnn_type: str = \"lstm\",\n        rnn_units: int = 2048,\n        layer_norm: bool = True,\n        kernel_regularizer=None,",
        "detail": "Demo.EMSConformer.tensorflow_asr.models.transducer.rnn_transducer",
        "documentation": {}
    },
    {
        "label": "RnnTransducer",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.models.transducer.rnn_transducer",
        "description": "Demo.EMSConformer.tensorflow_asr.models.transducer.rnn_transducer",
        "peekOfCode": "class RnnTransducer(Transducer):\n    def __init__(\n        self,\n        vocabulary_size: int,\n        encoder_reductions: dict = {0: 3, 1: 2},\n        encoder_dmodel: int = 640,\n        encoder_nlayers: int = 8,\n        encoder_rnn_type: str = \"lstm\",\n        encoder_rnn_units: int = 2048,\n        encoder_layer_norm: bool = True,",
        "detail": "Demo.EMSConformer.tensorflow_asr.models.transducer.rnn_transducer",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.models.base_model",
        "description": "Demo.EMSConformer.tensorflow_asr.models.base_model",
        "peekOfCode": "class BaseModel(tf.keras.Model):\n    def save(\n        self,\n        filepath,\n        overwrite=True,\n        include_optimizer=True,\n        save_format=None,\n        signatures=None,\n        options=None,\n        save_traces=False,",
        "detail": "Demo.EMSConformer.tensorflow_asr.models.base_model",
        "documentation": {}
    },
    {
        "label": "GradientAccumulation",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.optimizers.accumulation",
        "description": "Demo.EMSConformer.tensorflow_asr.optimizers.accumulation",
        "peekOfCode": "class GradientAccumulation:\n    def __init__(self, trainable_variables):\n        self.gradients = [\n            tf.Variable(\n                tf.zeros_like(g),\n                trainable=False,\n                synchronization=tf.VariableSynchronization.ON_READ,\n            ) for g in trainable_variables\n        ]\n    def reset(self):",
        "detail": "Demo.EMSConformer.tensorflow_asr.optimizers.accumulation",
        "documentation": {}
    },
    {
        "label": "TransformerSchedule",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.optimizers.schedules",
        "description": "Demo.EMSConformer.tensorflow_asr.optimizers.schedules",
        "peekOfCode": "class TransformerSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(self, d_model, warmup_steps=4000, max_lr=None):\n        super(TransformerSchedule, self).__init__()\n        self.d_model = d_model\n        self.d_model = tf.cast(self.d_model, tf.float32)\n        self.max_lr = max_lr\n        self.warmup_steps = warmup_steps\n    def __call__(self, step):\n        # lr = (d_model^-0.5) * min(step^-0.5, step*(warm_up^-1.5))\n        step = tf.cast(step, dtype=tf.float32)",
        "detail": "Demo.EMSConformer.tensorflow_asr.optimizers.schedules",
        "documentation": {}
    },
    {
        "label": "SANSchedule",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.optimizers.schedules",
        "description": "Demo.EMSConformer.tensorflow_asr.optimizers.schedules",
        "peekOfCode": "class SANSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(self, lamb, d_model, warmup_steps=4000):\n        super(SANSchedule, self).__init__()\n        self.lamb = tf.cast(lamb, tf.float32)\n        self.d_model = tf.cast(d_model, tf.float32)\n        self.warmup_steps = tf.cast(warmup_steps, tf.float32)\n    def __call__(self, step):\n        step = tf.cast(step, dtype=tf.float32)\n        arg1 = step / (self.warmup_steps ** 1.5)\n        arg2 = 1 / tf.math.sqrt(step)",
        "detail": "Demo.EMSConformer.tensorflow_asr.optimizers.schedules",
        "documentation": {}
    },
    {
        "label": "BoundExponentialDecay",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.optimizers.schedules",
        "description": "Demo.EMSConformer.tensorflow_asr.optimizers.schedules",
        "peekOfCode": "class BoundExponentialDecay(ExponentialDecay):\n    def __init__(self, min_lr=0.0, **kwargs):\n        super().__init__(**kwargs)\n        self.min_lr = min_lr\n    def __call__(self, step):\n        with tf.name_scope(self.name or \"ExponentialDecay\") as name:\n            initial_learning_rate = tf.convert_to_tensor(self.initial_learning_rate, name=\"initial_learning_rate\")\n            dtype = initial_learning_rate.dtype\n            decay_steps = tf.cast(self.decay_steps, dtype)\n            decay_rate = tf.cast(self.decay_rate, dtype)",
        "detail": "Demo.EMSConformer.tensorflow_asr.optimizers.schedules",
        "documentation": {}
    },
    {
        "label": "CyclicTransformerSchedule",
        "kind": 6,
        "importPath": "Demo.EMSConformer.tensorflow_asr.optimizers.schedules",
        "description": "Demo.EMSConformer.tensorflow_asr.optimizers.schedules",
        "peekOfCode": "class CyclicTransformerSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n    \"\"\"This callback implements a cyclical learning rate policy (CLR) to the square\n    root decay generally used to train transformers.\n    The method cycles the learning rate around the square root decay LR with an amplitude\n    equal to the target LR with a given period.\n    # Arguments\n        d_model: The dimension of the transformer model.\n        warmup_steps: Warm up steps where the LR increases linearly.\n            Default to 4000 steps.\n        max_lr: Maximum value of the learning rate reachable.",
        "detail": "Demo.EMSConformer.tensorflow_asr.optimizers.schedules",
        "documentation": {}
    },
    {
        "label": "evaluate_results",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.utils.app_util",
        "description": "Demo.EMSConformer.tensorflow_asr.utils.app_util",
        "peekOfCode": "def evaluate_results(\n    filepath: str,\n):\n    logger.info(f\"Evaluating result from {filepath} ...\")\n    metrics = {\n        \"greedy_wer\": ErrorRate(wer, name=\"greedy_wer\", dtype=tf.float32),\n        \"greedy_cer\": ErrorRate(cer, name=\"greedy_cer\", dtype=tf.float32),\n        \"beamsearch_wer\": ErrorRate(wer, name=\"beamsearch_wer\", dtype=tf.float32),\n        \"beamsearch_cer\": ErrorRate(cer, name=\"beamsearch_cer\", dtype=tf.float32),\n    }",
        "detail": "Demo.EMSConformer.tensorflow_asr.utils.app_util",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "Demo.EMSConformer.tensorflow_asr.utils.app_util",
        "description": "Demo.EMSConformer.tensorflow_asr.utils.app_util",
        "peekOfCode": "logger = tf.get_logger()\ndef evaluate_results(\n    filepath: str,\n):\n    logger.info(f\"Evaluating result from {filepath} ...\")\n    metrics = {\n        \"greedy_wer\": ErrorRate(wer, name=\"greedy_wer\", dtype=tf.float32),\n        \"greedy_cer\": ErrorRate(cer, name=\"greedy_cer\", dtype=tf.float32),\n        \"beamsearch_wer\": ErrorRate(wer, name=\"beamsearch_wer\", dtype=tf.float32),\n        \"beamsearch_cer\": ErrorRate(cer, name=\"beamsearch_cer\", dtype=tf.float32),",
        "detail": "Demo.EMSConformer.tensorflow_asr.utils.app_util",
        "documentation": {}
    },
    {
        "label": "create_inputs",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.utils.data_util",
        "description": "Demo.EMSConformer.tensorflow_asr.utils.data_util",
        "peekOfCode": "def create_inputs(\n    inputs: tf.Tensor,\n    inputs_length: tf.Tensor,\n    predictions: tf.Tensor = None,\n    predictions_length: tf.Tensor = None,\n) -> dict:\n    data = {\n        \"inputs\": inputs,\n        \"inputs_length\": inputs_length,\n    }",
        "detail": "Demo.EMSConformer.tensorflow_asr.utils.data_util",
        "documentation": {}
    },
    {
        "label": "create_logits",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.utils.data_util",
        "description": "Demo.EMSConformer.tensorflow_asr.utils.data_util",
        "peekOfCode": "def create_logits(\n    logits: tf.Tensor,\n    logits_length: tf.Tensor,\n) -> dict:\n    return {\"logits\": logits, \"logits_length\": logits_length}\ndef create_labels(\n    labels: tf.Tensor,\n    labels_length: tf.Tensor,\n) -> dict:\n    return {",
        "detail": "Demo.EMSConformer.tensorflow_asr.utils.data_util",
        "documentation": {}
    },
    {
        "label": "create_labels",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.utils.data_util",
        "description": "Demo.EMSConformer.tensorflow_asr.utils.data_util",
        "peekOfCode": "def create_labels(\n    labels: tf.Tensor,\n    labels_length: tf.Tensor,\n) -> dict:\n    return {\n        \"labels\": labels,\n        \"labels_length\": labels_length,\n    }",
        "detail": "Demo.EMSConformer.tensorflow_asr.utils.data_util",
        "documentation": {}
    },
    {
        "label": "setup_environment",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.utils.env_util",
        "description": "Demo.EMSConformer.tensorflow_asr.utils.env_util",
        "peekOfCode": "def setup_environment():\n    \"\"\"Setting tensorflow running environment\"\"\"\n    warnings.simplefilter(\"ignore\")\n    logger.setLevel(logging.INFO)\n    return logger\ndef setup_devices(devices: List[int], cpu: bool = False):\n    \"\"\"Setting visible devices\n    Args:\n        devices (list): list of visible devices' indices\n    \"\"\"",
        "detail": "Demo.EMSConformer.tensorflow_asr.utils.env_util",
        "documentation": {}
    },
    {
        "label": "setup_devices",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.utils.env_util",
        "description": "Demo.EMSConformer.tensorflow_asr.utils.env_util",
        "peekOfCode": "def setup_devices(devices: List[int], cpu: bool = False):\n    \"\"\"Setting visible devices\n    Args:\n        devices (list): list of visible devices' indices\n    \"\"\"\n    if cpu:\n        cpus = tf.config.list_physical_devices(\"CPU\")\n        tf.config.set_visible_devices(cpus, \"CPU\")\n        tf.config.set_visible_devices([], \"GPU\")\n        logger.info(f\"Run on {len(cpus)} Physical CPUs\")",
        "detail": "Demo.EMSConformer.tensorflow_asr.utils.env_util",
        "documentation": {}
    },
    {
        "label": "setup_tpu",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.utils.env_util",
        "description": "Demo.EMSConformer.tensorflow_asr.utils.env_util",
        "peekOfCode": "def setup_tpu(tpu_address=None):\n    if tpu_address is None:\n        resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n    else:\n        resolver = tf.distribute.cluster_resolver.TPUClusterResolver(\n            tpu=\"grpc://\" + tpu_address\n        )\n    tf.config.experimental_connect_to_cluster(resolver)\n    tf.tpu.experimental.initialize_tpu_system(resolver)\n    logger.info(f\"All TPUs: {tf.config.list_logical_devices('TPU')}\")",
        "detail": "Demo.EMSConformer.tensorflow_asr.utils.env_util",
        "documentation": {}
    },
    {
        "label": "setup_strategy",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.utils.env_util",
        "description": "Demo.EMSConformer.tensorflow_asr.utils.env_util",
        "peekOfCode": "def setup_strategy(devices: List[int], tpu_address: str = None):\n    \"\"\"Setting mirrored strategy for training\n    Args:\n        devices (list): list of visible devices' indices\n        tpu_address (str): an optional custom tpu address\n    Returns:\n        tf.distribute.Strategy: TPUStrategy for training on tpus or MirroredStrategy for training on gpus\n    \"\"\"\n    try:\n        return setup_tpu(tpu_address)",
        "detail": "Demo.EMSConformer.tensorflow_asr.utils.env_util",
        "documentation": {}
    },
    {
        "label": "has_devices",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.utils.env_util",
        "description": "Demo.EMSConformer.tensorflow_asr.utils.env_util",
        "peekOfCode": "def has_devices(devices: Union[List[str], str]):\n    if isinstance(devices, list):\n        return all([len(tf.config.list_logical_devices(d)) != 0 for d in devices])\n    return len(tf.config.list_logical_devices(devices)) != 0",
        "detail": "Demo.EMSConformer.tensorflow_asr.utils.env_util",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "Demo.EMSConformer.tensorflow_asr.utils.env_util",
        "description": "Demo.EMSConformer.tensorflow_asr.utils.env_util",
        "peekOfCode": "logger = tf.get_logger()\ndef setup_environment():\n    \"\"\"Setting tensorflow running environment\"\"\"\n    warnings.simplefilter(\"ignore\")\n    logger.setLevel(logging.INFO)\n    return logger\ndef setup_devices(devices: List[int], cpu: bool = False):\n    \"\"\"Setting visible devices\n    Args:\n        devices (list): list of visible devices' indices",
        "detail": "Demo.EMSConformer.tensorflow_asr.utils.env_util",
        "documentation": {}
    },
    {
        "label": "float_feature",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.utils.feature_util",
        "description": "Demo.EMSConformer.tensorflow_asr.utils.feature_util",
        "peekOfCode": "def float_feature(\n    list_of_floats,\n):\n    return tf.train.Feature(float_list=tf.train.FloatList(value=list_of_floats))\ndef int64_feature(\n    list_of_ints,\n):\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=list_of_ints))\ndef bytestring_feature(\n    list_of_bytestrings,",
        "detail": "Demo.EMSConformer.tensorflow_asr.utils.feature_util",
        "documentation": {}
    },
    {
        "label": "int64_feature",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.utils.feature_util",
        "description": "Demo.EMSConformer.tensorflow_asr.utils.feature_util",
        "peekOfCode": "def int64_feature(\n    list_of_ints,\n):\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=list_of_ints))\ndef bytestring_feature(\n    list_of_bytestrings,\n):\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=list_of_bytestrings))",
        "detail": "Demo.EMSConformer.tensorflow_asr.utils.feature_util",
        "documentation": {}
    },
    {
        "label": "bytestring_feature",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.utils.feature_util",
        "description": "Demo.EMSConformer.tensorflow_asr.utils.feature_util",
        "peekOfCode": "def bytestring_feature(\n    list_of_bytestrings,\n):\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=list_of_bytestrings))",
        "detail": "Demo.EMSConformer.tensorflow_asr.utils.feature_util",
        "documentation": {}
    },
    {
        "label": "load_yaml",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.utils.file_util",
        "description": "Demo.EMSConformer.tensorflow_asr.utils.file_util",
        "peekOfCode": "def load_yaml(\n    path: str,\n) -> dict:\n    # Fix yaml numbers https://stackoverflow.com/a/30462009/11037553\n    loader = yaml.SafeLoader\n    loader.add_implicit_resolver(\n        u\"tag:yaml.org,2002:float\",\n        re.compile(\n            u\"\"\"^(?:\n         [-+]?(?:[0-9][0-9_]*)\\\\.[0-9_]*(?:[eE][-+]?[0-9]+)?",
        "detail": "Demo.EMSConformer.tensorflow_asr.utils.file_util",
        "documentation": {}
    },
    {
        "label": "is_hdf5_filepath",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.utils.file_util",
        "description": "Demo.EMSConformer.tensorflow_asr.utils.file_util",
        "peekOfCode": "def is_hdf5_filepath(\n    filepath: str,\n) -> bool:\n    return (\n        filepath.endswith(\".h5\")\n        or filepath.endswith(\".keras\")\n        or filepath.endswith(\".hdf5\")\n    )\ndef is_cloud_path(\n    path: str,",
        "detail": "Demo.EMSConformer.tensorflow_asr.utils.file_util",
        "documentation": {}
    },
    {
        "label": "is_cloud_path",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.utils.file_util",
        "description": "Demo.EMSConformer.tensorflow_asr.utils.file_util",
        "peekOfCode": "def is_cloud_path(\n    path: str,\n) -> bool:\n    \"\"\"Check if the path is on cloud (which requires tf.io.gfile)\n    Args:\n        path (str): Path to directory or file\n    Returns:\n        bool: True if path is on cloud, False otherwise\n    \"\"\"\n    return bool(re.match(r\"^[a-z]+://\", path))",
        "detail": "Demo.EMSConformer.tensorflow_asr.utils.file_util",
        "documentation": {}
    },
    {
        "label": "preprocess_paths",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.utils.file_util",
        "description": "Demo.EMSConformer.tensorflow_asr.utils.file_util",
        "peekOfCode": "def preprocess_paths(\n    paths: Union[List[str], str],\n    isdir: bool = False,\n) -> Union[List[str], str]:\n    \"\"\"Expand the path to the root \"/\" and makedirs\n    Args:\n        paths (Union[List, str]): A path or list of paths\n    Returns:\n        Union[List, str]: A processed path or list of paths, return None if it's not path\n    \"\"\"",
        "detail": "Demo.EMSConformer.tensorflow_asr.utils.file_util",
        "documentation": {}
    },
    {
        "label": "save_file",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.utils.file_util",
        "description": "Demo.EMSConformer.tensorflow_asr.utils.file_util",
        "peekOfCode": "def save_file(\n    filepath: str,\n):\n    if is_cloud_path(filepath) and is_hdf5_filepath(filepath):\n        _, ext = os.path.splitext(filepath)\n        with tempfile.NamedTemporaryFile(suffix=ext) as tmp:\n            yield tmp.name\n            tf.io.gfile.copy(tmp.name, filepath, overwrite=True)\n    else:\n        yield filepath",
        "detail": "Demo.EMSConformer.tensorflow_asr.utils.file_util",
        "documentation": {}
    },
    {
        "label": "read_file",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.utils.file_util",
        "description": "Demo.EMSConformer.tensorflow_asr.utils.file_util",
        "peekOfCode": "def read_file(\n    filepath: str,\n):\n    if is_cloud_path(filepath) and is_hdf5_filepath(filepath):\n        _, ext = os.path.splitext(filepath)\n        with tempfile.NamedTemporaryFile(suffix=ext) as tmp:\n            tf.io.gfile.copy(filepath, tmp.name, overwrite=True)\n            yield tmp.name\n    else:\n        yield filepath",
        "detail": "Demo.EMSConformer.tensorflow_asr.utils.file_util",
        "documentation": {}
    },
    {
        "label": "get_rnn",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.utils.layer_util",
        "description": "Demo.EMSConformer.tensorflow_asr.utils.layer_util",
        "peekOfCode": "def get_rnn(\n    rnn_type: str,\n):\n    assert rnn_type in [\"lstm\", \"gru\", \"rnn\"]\n    if rnn_type == \"lstm\":\n        return tf.keras.layers.LSTM\n    if rnn_type == \"gru\":\n        return tf.keras.layers.GRU\n    return tf.keras.layers.SimpleRNN\ndef get_conv(",
        "detail": "Demo.EMSConformer.tensorflow_asr.utils.layer_util",
        "documentation": {}
    },
    {
        "label": "get_conv",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.utils.layer_util",
        "description": "Demo.EMSConformer.tensorflow_asr.utils.layer_util",
        "peekOfCode": "def get_conv(\n    conv_type: str,\n):\n    assert conv_type in [\"conv1d\", \"conv2d\"]\n    if conv_type == \"conv1d\":\n        return tf.keras.layers.Conv1D\n    return tf.keras.layers.Conv2D",
        "detail": "Demo.EMSConformer.tensorflow_asr.utils.layer_util",
        "documentation": {}
    },
    {
        "label": "log10",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.utils.math_util",
        "description": "Demo.EMSConformer.tensorflow_asr.utils.math_util",
        "peekOfCode": "def log10(x):\n    numerator = tf.math.log(x)\n    denominator = tf.math.log(tf.constant(10, dtype=numerator.dtype))\n    return numerator / denominator\ndef get_num_batches(\n    nsamples,\n    batch_size,\n    drop_remainders=True,\n):\n    if nsamples is None or batch_size is None:",
        "detail": "Demo.EMSConformer.tensorflow_asr.utils.math_util",
        "documentation": {}
    },
    {
        "label": "get_num_batches",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.utils.math_util",
        "description": "Demo.EMSConformer.tensorflow_asr.utils.math_util",
        "peekOfCode": "def get_num_batches(\n    nsamples,\n    batch_size,\n    drop_remainders=True,\n):\n    if nsamples is None or batch_size is None:\n        return None\n    if drop_remainders:\n        return math.floor(float(nsamples) / float(batch_size))\n    return math.ceil(float(nsamples) / float(batch_size))",
        "detail": "Demo.EMSConformer.tensorflow_asr.utils.math_util",
        "documentation": {}
    },
    {
        "label": "nan_to_zero",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.utils.math_util",
        "description": "Demo.EMSConformer.tensorflow_asr.utils.math_util",
        "peekOfCode": "def nan_to_zero(\n    input_tensor: tf.Tensor,\n):\n    return tf.where(tf.math.is_nan(input_tensor), tf.zeros_like(input_tensor), input_tensor)\ndef bytes_to_string(\n    array: np.ndarray,\n    encoding: str = \"utf-8\",\n):\n    if array is None:\n        return None",
        "detail": "Demo.EMSConformer.tensorflow_asr.utils.math_util",
        "documentation": {}
    },
    {
        "label": "bytes_to_string",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.utils.math_util",
        "description": "Demo.EMSConformer.tensorflow_asr.utils.math_util",
        "peekOfCode": "def bytes_to_string(\n    array: np.ndarray,\n    encoding: str = \"utf-8\",\n):\n    if array is None:\n        return None\n    return [transcript.decode(encoding) for transcript in array]\ndef get_reduced_length(\n    length,\n    reduction_factor,",
        "detail": "Demo.EMSConformer.tensorflow_asr.utils.math_util",
        "documentation": {}
    },
    {
        "label": "get_reduced_length",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.utils.math_util",
        "description": "Demo.EMSConformer.tensorflow_asr.utils.math_util",
        "peekOfCode": "def get_reduced_length(\n    length,\n    reduction_factor,\n):\n    return tf.cast(\n        tf.math.ceil(tf.divide(length, tf.cast(reduction_factor, dtype=length.dtype))),\n        dtype=tf.int32,\n    )\ndef count_non_blank(\n    tensor: tf.Tensor,",
        "detail": "Demo.EMSConformer.tensorflow_asr.utils.math_util",
        "documentation": {}
    },
    {
        "label": "count_non_blank",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.utils.math_util",
        "description": "Demo.EMSConformer.tensorflow_asr.utils.math_util",
        "peekOfCode": "def count_non_blank(\n    tensor: tf.Tensor,\n    blank: int or tf.Tensor = 0,\n    axis=None,\n):\n    return tf.reduce_sum(\n        tf.where(tf.not_equal(tensor, blank), x=tf.ones_like(tensor), y=tf.zeros_like(tensor)),\n        axis=axis,\n    )\ndef merge_two_last_dims(x):",
        "detail": "Demo.EMSConformer.tensorflow_asr.utils.math_util",
        "documentation": {}
    },
    {
        "label": "merge_two_last_dims",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.utils.math_util",
        "description": "Demo.EMSConformer.tensorflow_asr.utils.math_util",
        "peekOfCode": "def merge_two_last_dims(x):\n    b, _, f, c = shape_util.shape_list(x)\n    return tf.reshape(x, shape=[b, -1, f * c])\ndef merge_repeated(\n    yseqs,\n    blank=0,\n):\n    result = tf.reshape(yseqs[0], [1])\n    U = shape_util.shape_list(yseqs)[0]\n    i = tf.constant(1, dtype=tf.int32)",
        "detail": "Demo.EMSConformer.tensorflow_asr.utils.math_util",
        "documentation": {}
    },
    {
        "label": "merge_repeated",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.utils.math_util",
        "description": "Demo.EMSConformer.tensorflow_asr.utils.math_util",
        "peekOfCode": "def merge_repeated(\n    yseqs,\n    blank=0,\n):\n    result = tf.reshape(yseqs[0], [1])\n    U = shape_util.shape_list(yseqs)[0]\n    i = tf.constant(1, dtype=tf.int32)\n    def _cond(i, result, yseqs, U):\n        return tf.less(i, U)\n    def _body(i, result, yseqs, U):",
        "detail": "Demo.EMSConformer.tensorflow_asr.utils.math_util",
        "documentation": {}
    },
    {
        "label": "find_max_length_prediction_tfarray",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.utils.math_util",
        "description": "Demo.EMSConformer.tensorflow_asr.utils.math_util",
        "peekOfCode": "def find_max_length_prediction_tfarray(\n    tfarray: tf.TensorArray,\n) -> tf.Tensor:\n    with tf.name_scope(\"find_max_length_prediction_tfarray\"):\n        index = tf.constant(0, dtype=tf.int32)\n        total = tfarray.size()\n        max_length = tf.constant(0, dtype=tf.int32)\n        def condition(index, _):\n            return tf.less(index, total)\n        def body(index, max_length):",
        "detail": "Demo.EMSConformer.tensorflow_asr.utils.math_util",
        "documentation": {}
    },
    {
        "label": "pad_prediction_tfarray",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.utils.math_util",
        "description": "Demo.EMSConformer.tensorflow_asr.utils.math_util",
        "peekOfCode": "def pad_prediction_tfarray(\n    tfarray: tf.TensorArray,\n    blank: int or tf.Tensor,\n) -> tf.TensorArray:\n    with tf.name_scope(\"pad_prediction_tfarray\"):\n        index = tf.constant(0, dtype=tf.int32)\n        total = tfarray.size()\n        max_length = find_max_length_prediction_tfarray(tfarray) + 1\n        def condition(index, _):\n            return tf.less(index, total)",
        "detail": "Demo.EMSConformer.tensorflow_asr.utils.math_util",
        "documentation": {}
    },
    {
        "label": "execute_wer",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.utils.metric_util",
        "description": "Demo.EMSConformer.tensorflow_asr.utils.metric_util",
        "peekOfCode": "def execute_wer(\n    decode,\n    target,\n):\n    decode = math_util.bytes_to_string(decode)\n    target = math_util.bytes_to_string(target)\n    dis = 0.0\n    length = 0.0\n    for dec, tar in zip(decode, target):\n        words = set(dec.split() + tar.split())",
        "detail": "Demo.EMSConformer.tensorflow_asr.utils.metric_util",
        "documentation": {}
    },
    {
        "label": "wer",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.utils.metric_util",
        "description": "Demo.EMSConformer.tensorflow_asr.utils.metric_util",
        "peekOfCode": "def wer(\n    decode: tf.Tensor,\n    target: tf.Tensor,\n) -> Tuple[tf.Tensor, tf.Tensor]:\n    \"\"\"Word Error Rate\n    Args:\n        decode (np.ndarray): array of prediction texts\n        target (np.ndarray): array of groundtruth texts\n    Returns:\n        tuple: a tuple of tf.Tensor of (edit distances, number of words) of each text",
        "detail": "Demo.EMSConformer.tensorflow_asr.utils.metric_util",
        "documentation": {}
    },
    {
        "label": "execute_cer",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.utils.metric_util",
        "description": "Demo.EMSConformer.tensorflow_asr.utils.metric_util",
        "peekOfCode": "def execute_cer(decode, target):\n    decode = math_util.bytes_to_string(decode)\n    target = math_util.bytes_to_string(target)\n    dis = 0\n    length = 0\n    for dec, tar in zip(decode, target):\n        dis += distance.edit_distance(dec, tar)\n        length += len(tar)\n    return tf.convert_to_tensor(dis, tf.float32), tf.convert_to_tensor(length, tf.float32)\ndef cer(",
        "detail": "Demo.EMSConformer.tensorflow_asr.utils.metric_util",
        "documentation": {}
    },
    {
        "label": "cer",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.utils.metric_util",
        "description": "Demo.EMSConformer.tensorflow_asr.utils.metric_util",
        "peekOfCode": "def cer(\n    decode: tf.Tensor,\n    target: tf.Tensor,\n) -> Tuple[tf.Tensor, tf.Tensor]:\n    \"\"\"Character Error Rate\n    Args:\n        decode (np.ndarray): array of prediction texts\n        target (np.ndarray): array of groundtruth texts\n    Returns:\n        tuple: a tuple of tf.Tensor of (edit distances, number of characters) of each text",
        "detail": "Demo.EMSConformer.tensorflow_asr.utils.metric_util",
        "documentation": {}
    },
    {
        "label": "tf_cer",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.utils.metric_util",
        "description": "Demo.EMSConformer.tensorflow_asr.utils.metric_util",
        "peekOfCode": "def tf_cer(\n    decode: tf.Tensor,\n    target: tf.Tensor,\n) -> Tuple[tf.Tensor, tf.Tensor]:\n    \"\"\"Tensorflwo Charactor Error rate\n    Args:\n        decoder (tf.Tensor): tensor shape [B]\n        target (tf.Tensor): tensor shape [B]\n    Returns:\n        tuple: a tuple of tf.Tensor of (edit distances, number of characters) of each text",
        "detail": "Demo.EMSConformer.tensorflow_asr.utils.metric_util",
        "documentation": {}
    },
    {
        "label": "shape_list",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.utils.shape_util",
        "description": "Demo.EMSConformer.tensorflow_asr.utils.shape_util",
        "peekOfCode": "def shape_list(x, out_type=tf.int32):\n    \"\"\"Deal with dynamic shape in tensorflow cleanly.\"\"\"\n    static = x.shape.as_list()\n    dynamic = tf.shape(x, out_type=out_type)\n    return [dynamic[i] if s is None else s for i, s in enumerate(static)]\ndef get_shape_invariants(tensor):\n    shapes = shape_list(tensor)\n    return tf.TensorShape([i if isinstance(i, int) else None for i in shapes])\ndef get_float_spec(tensor):\n    shape = get_shape_invariants(tensor)",
        "detail": "Demo.EMSConformer.tensorflow_asr.utils.shape_util",
        "documentation": {}
    },
    {
        "label": "get_shape_invariants",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.utils.shape_util",
        "description": "Demo.EMSConformer.tensorflow_asr.utils.shape_util",
        "peekOfCode": "def get_shape_invariants(tensor):\n    shapes = shape_list(tensor)\n    return tf.TensorShape([i if isinstance(i, int) else None for i in shapes])\ndef get_float_spec(tensor):\n    shape = get_shape_invariants(tensor)\n    return tf.TensorSpec(shape, dtype=tf.float32)",
        "detail": "Demo.EMSConformer.tensorflow_asr.utils.shape_util",
        "documentation": {}
    },
    {
        "label": "get_float_spec",
        "kind": 2,
        "importPath": "Demo.EMSConformer.tensorflow_asr.utils.shape_util",
        "description": "Demo.EMSConformer.tensorflow_asr.utils.shape_util",
        "peekOfCode": "def get_float_spec(tensor):\n    shape = get_shape_invariants(tensor)\n    return tf.TensorSpec(shape, dtype=tf.float32)",
        "detail": "Demo.EMSConformer.tensorflow_asr.utils.shape_util",
        "documentation": {}
    },
    {
        "label": "compare_test_output",
        "kind": 2,
        "importPath": "Demo.EMSConformer.evaluate_asr",
        "description": "Demo.EMSConformer.evaluate_asr",
        "peekOfCode": "def compare_test_output(filepath: str):\n    logger.info(f\"Evaluating result from {filepath} ...\")\n    metrics = {\n        \"greedy_wer\": ErrorRate(wer, name=\"greedy_wer\", dtype=tf.float32),\n        \"greedy_cer\": ErrorRate(cer, name=\"greedy_cer\", dtype=tf.float32),\n        \"beamsearch_wer\": ErrorRate(wer, name=\"beamsearch_wer\", dtype=tf.float32),\n        \"beamsearch_cer\": ErrorRate(cer, name=\"beamsearch_cer\", dtype=tf.float32),\n    }\n    with read_file(filepath) as path:\n        with open(path, \"r\", encoding=\"utf-8\") as openfile:",
        "detail": "Demo.EMSConformer.evaluate_asr",
        "documentation": {}
    },
    {
        "label": "compare_normal_output",
        "kind": 2,
        "importPath": "Demo.EMSConformer.evaluate_asr",
        "description": "Demo.EMSConformer.evaluate_asr",
        "peekOfCode": "def compare_normal_output(filepath: str):\n    logger.info(f\"Evaluating result from {filepath} ...\")\n    metrics = {\n        \"wer\": ErrorRate(wer, name=\"wer\", dtype=tf.float32),\n        \"cer\": ErrorRate(cer, name=\"cer\", dtype=tf.float32),\n    }\n    with read_file(filepath) as path:\n        with open(path, \"r\", encoding=\"utf-8\") as openfile:\n            lines = openfile.read().splitlines()\n    for eachline in tqdm(lines):",
        "detail": "Demo.EMSConformer.evaluate_asr",
        "documentation": {}
    },
    {
        "label": "devices",
        "kind": 5,
        "importPath": "Demo.EMSConformer.evaluate_asr",
        "description": "Demo.EMSConformer.evaluate_asr",
        "peekOfCode": "devices = [0]\ngpus = tf.config.list_physical_devices(\"GPU\")\nvisible_gpus = [gpus[i] for i in devices]\ntf.config.set_visible_devices(visible_gpus, \"GPU\")\nfrom tensorflow_asr.metrics.error_rates import ErrorRate\nfrom tensorflow_asr.utils.file_util import read_file\nfrom tensorflow_asr.utils.metric_util import cer, wer\nlogger = tf.get_logger()\n\"\"\"\ncompare_test_output is used to evaluate wer and cer given a text file from tensorflow_asr code. The tensorflow_asr",
        "detail": "Demo.EMSConformer.evaluate_asr",
        "documentation": {}
    },
    {
        "label": "gpus",
        "kind": 5,
        "importPath": "Demo.EMSConformer.evaluate_asr",
        "description": "Demo.EMSConformer.evaluate_asr",
        "peekOfCode": "gpus = tf.config.list_physical_devices(\"GPU\")\nvisible_gpus = [gpus[i] for i in devices]\ntf.config.set_visible_devices(visible_gpus, \"GPU\")\nfrom tensorflow_asr.metrics.error_rates import ErrorRate\nfrom tensorflow_asr.utils.file_util import read_file\nfrom tensorflow_asr.utils.metric_util import cer, wer\nlogger = tf.get_logger()\n\"\"\"\ncompare_test_output is used to evaluate wer and cer given a text file from tensorflow_asr code. The tensorflow_asr\nuses both greedy search and beam search to generate transcribed texts. The greedy search is working, and the beam ",
        "detail": "Demo.EMSConformer.evaluate_asr",
        "documentation": {}
    },
    {
        "label": "visible_gpus",
        "kind": 5,
        "importPath": "Demo.EMSConformer.evaluate_asr",
        "description": "Demo.EMSConformer.evaluate_asr",
        "peekOfCode": "visible_gpus = [gpus[i] for i in devices]\ntf.config.set_visible_devices(visible_gpus, \"GPU\")\nfrom tensorflow_asr.metrics.error_rates import ErrorRate\nfrom tensorflow_asr.utils.file_util import read_file\nfrom tensorflow_asr.utils.metric_util import cer, wer\nlogger = tf.get_logger()\n\"\"\"\ncompare_test_output is used to evaluate wer and cer given a text file from tensorflow_asr code. The tensorflow_asr\nuses both greedy search and beam search to generate transcribed texts. The greedy search is working, and the beam \nsearch is under development.",
        "detail": "Demo.EMSConformer.evaluate_asr",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "Demo.EMSConformer.evaluate_asr",
        "description": "Demo.EMSConformer.evaluate_asr",
        "peekOfCode": "logger = tf.get_logger()\n\"\"\"\ncompare_test_output is used to evaluate wer and cer given a text file from tensorflow_asr code. The tensorflow_asr\nuses both greedy search and beam search to generate transcribed texts. The greedy search is working, and the beam \nsearch is under development.\nInput: the path of text file with transcribed text and ground true text for both greedy search and beam search.\nOutput: the WER and CER of the given text file for both greedy search and beam search.\n\"\"\"\ndef compare_test_output(filepath: str):\n    logger.info(f\"Evaluating result from {filepath} ...\")",
        "detail": "Demo.EMSConformer.evaluate_asr",
        "documentation": {}
    },
    {
        "label": "compare_test_output",
        "kind": 2,
        "importPath": "Demo.EMSConformer.evaluate_asr_google_cloud",
        "description": "Demo.EMSConformer.evaluate_asr_google_cloud",
        "peekOfCode": "def compare_test_output(filepath: str):\n    logger.info(f\"Evaluating result from {filepath} ...\")\n    metrics = {\n        \"greedy_wer\": ErrorRate(wer, name=\"greedy_wer\", dtype=tf.float32),\n        \"greedy_cer\": ErrorRate(cer, name=\"greedy_cer\", dtype=tf.float32),\n        \"beamsearch_wer\": ErrorRate(wer, name=\"beamsearch_wer\", dtype=tf.float32),\n        \"beamsearch_cer\": ErrorRate(cer, name=\"beamsearch_cer\", dtype=tf.float32),\n    }\n    with read_file(filepath) as path:\n        with open(path, \"r\", encoding=\"utf-8\") as openfile:",
        "detail": "Demo.EMSConformer.evaluate_asr_google_cloud",
        "documentation": {}
    },
    {
        "label": "compare_normal_output",
        "kind": 2,
        "importPath": "Demo.EMSConformer.evaluate_asr_google_cloud",
        "description": "Demo.EMSConformer.evaluate_asr_google_cloud",
        "peekOfCode": "def compare_normal_output(filepath: str):\n    #logger = tf.get_logger()\n    logger.info(f\"Evaluating result from {filepath} ...\")\n    metrics = {\n        \"wer\": ErrorRate(wer, name=\"wer\", dtype=tf.float32),\n        \"cer\": ErrorRate(cer, name=\"cer\", dtype=tf.float32),\n    }\n    with read_file(filepath) as path:\n        with open(path, \"r\", encoding=\"utf-8\") as openfile:\n            lines = openfile.read().splitlines()",
        "detail": "Demo.EMSConformer.evaluate_asr_google_cloud",
        "documentation": {}
    },
    {
        "label": "devices",
        "kind": 5,
        "importPath": "Demo.EMSConformer.evaluate_asr_google_cloud",
        "description": "Demo.EMSConformer.evaluate_asr_google_cloud",
        "peekOfCode": "devices = [0]\ngpus = tf.config.list_physical_devices(\"GPU\")\nvisible_gpus = [gpus[i] for i in devices]\ntf.config.set_visible_devices(visible_gpus, \"GPU\")\nfrom tensorflow_asr.metrics.error_rates import ErrorRate\nfrom tensorflow_asr.utils.file_util import read_file\nfrom tensorflow_asr.utils.metric_util import cer, wer\nlogger = tf.get_logger()\n\"\"\"\ncompare_test_output is used to evaluate wer and cer given a text file from tensorflow_asr code. The tensorflow_asr",
        "detail": "Demo.EMSConformer.evaluate_asr_google_cloud",
        "documentation": {}
    },
    {
        "label": "gpus",
        "kind": 5,
        "importPath": "Demo.EMSConformer.evaluate_asr_google_cloud",
        "description": "Demo.EMSConformer.evaluate_asr_google_cloud",
        "peekOfCode": "gpus = tf.config.list_physical_devices(\"GPU\")\nvisible_gpus = [gpus[i] for i in devices]\ntf.config.set_visible_devices(visible_gpus, \"GPU\")\nfrom tensorflow_asr.metrics.error_rates import ErrorRate\nfrom tensorflow_asr.utils.file_util import read_file\nfrom tensorflow_asr.utils.metric_util import cer, wer\nlogger = tf.get_logger()\n\"\"\"\ncompare_test_output is used to evaluate wer and cer given a text file from tensorflow_asr code. The tensorflow_asr\nuses both greedy search and beam search to generate transcribed texts. The greedy search is working, and the beam ",
        "detail": "Demo.EMSConformer.evaluate_asr_google_cloud",
        "documentation": {}
    },
    {
        "label": "visible_gpus",
        "kind": 5,
        "importPath": "Demo.EMSConformer.evaluate_asr_google_cloud",
        "description": "Demo.EMSConformer.evaluate_asr_google_cloud",
        "peekOfCode": "visible_gpus = [gpus[i] for i in devices]\ntf.config.set_visible_devices(visible_gpus, \"GPU\")\nfrom tensorflow_asr.metrics.error_rates import ErrorRate\nfrom tensorflow_asr.utils.file_util import read_file\nfrom tensorflow_asr.utils.metric_util import cer, wer\nlogger = tf.get_logger()\n\"\"\"\ncompare_test_output is used to evaluate wer and cer given a text file from tensorflow_asr code. The tensorflow_asr\nuses both greedy search and beam search to generate transcribed texts. The greedy search is working, and the beam \nsearch is under development.",
        "detail": "Demo.EMSConformer.evaluate_asr_google_cloud",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "Demo.EMSConformer.evaluate_asr_google_cloud",
        "description": "Demo.EMSConformer.evaluate_asr_google_cloud",
        "peekOfCode": "logger = tf.get_logger()\n\"\"\"\ncompare_test_output is used to evaluate wer and cer given a text file from tensorflow_asr code. The tensorflow_asr\nuses both greedy search and beam search to generate transcribed texts. The greedy search is working, and the beam \nsearch is under development.\nInput: the path of text file with transcribed text and ground true text for both greedy search and beam search.\nOutput: the WER and CER of the given text file for both greedy search and beam search.\n\"\"\"\ndef compare_test_output(filepath: str):\n    logger.info(f\"Evaluating result from {filepath} ...\")",
        "detail": "Demo.EMSConformer.evaluate_asr_google_cloud",
        "documentation": {}
    },
    {
        "label": "fn_list",
        "kind": 5,
        "importPath": "Demo.EMSConformer.evaluate_asr_google_cloud",
        "description": "Demo.EMSConformer.evaluate_asr_google_cloud",
        "peekOfCode": "fn_list = [\n    'eval_GC_transcript_default.txt',\n    'eval_GC_transcript_latest_long.txt',\n    'eval_GC_transcript_latest_short.txt',\n    'eval_GC_transcript_video.txt',\n    'eval_GC_transcript_phone_call.txt',\n    'eval_GC_transcript_command_and_search.txt',\n    'eval_GC_transcript_medical_dictation.txt',\n    'eval_GC_transcript_medical_conversation.txt',    \n]",
        "detail": "Demo.EMSConformer.evaluate_asr_google_cloud",
        "documentation": {}
    },
    {
        "label": "spk_dirs",
        "kind": 5,
        "importPath": "Demo.EMSConformer.evaluate_asr_google_cloud",
        "description": "Demo.EMSConformer.evaluate_asr_google_cloud",
        "peekOfCode": "spk_dirs = [\n    \"sample_tian2\",\n    \"sample_liuyi\",\n    \"sample_yichen\",\n    \"sample_radu\",\n    \"sample_amran\",\n    \"sample_michael\"\n]   \n# python evaluate_asr_tian.py --dir \n\"\"\"",
        "detail": "Demo.EMSConformer.evaluate_asr_google_cloud",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Demo.EMSConformer.test",
        "description": "Demo.EMSConformer.test",
        "peekOfCode": "def main(\n    config: str = DEFAULT_YAML,\n    saved: str = None,\n    mxp: bool = False,\n    bs: int = None,\n    sentence_piece: bool = False,\n    subwords: bool = True,\n    device: int = 0,\n    cpu: bool = False,\n    output: str = \"test.tsv\",",
        "detail": "Demo.EMSConformer.test",
        "documentation": {}
    },
    {
        "label": "devices",
        "kind": 5,
        "importPath": "Demo.EMSConformer.test",
        "description": "Demo.EMSConformer.test",
        "peekOfCode": "devices = [0]\ngpus = tf.config.list_physical_devices(\"GPU\")\nvisible_gpus = [gpus[i] for i in devices]\ntf.config.set_visible_devices(visible_gpus, \"GPU\")\n# strategy = tf.distribute.MirroredStrategy()\nimport fire\nfrom tensorflow_asr.utils import env_util\nlogger = env_util.setup_environment()\nfrom tensorflow_asr.configs.config import Config\nfrom tensorflow_asr.helpers import dataset_helpers, exec_helpers, featurizer_helpers",
        "detail": "Demo.EMSConformer.test",
        "documentation": {}
    },
    {
        "label": "gpus",
        "kind": 5,
        "importPath": "Demo.EMSConformer.test",
        "description": "Demo.EMSConformer.test",
        "peekOfCode": "gpus = tf.config.list_physical_devices(\"GPU\")\nvisible_gpus = [gpus[i] for i in devices]\ntf.config.set_visible_devices(visible_gpus, \"GPU\")\n# strategy = tf.distribute.MirroredStrategy()\nimport fire\nfrom tensorflow_asr.utils import env_util\nlogger = env_util.setup_environment()\nfrom tensorflow_asr.configs.config import Config\nfrom tensorflow_asr.helpers import dataset_helpers, exec_helpers, featurizer_helpers\nfrom tensorflow_asr.models.transducer.conformer import Conformer",
        "detail": "Demo.EMSConformer.test",
        "documentation": {}
    },
    {
        "label": "visible_gpus",
        "kind": 5,
        "importPath": "Demo.EMSConformer.test",
        "description": "Demo.EMSConformer.test",
        "peekOfCode": "visible_gpus = [gpus[i] for i in devices]\ntf.config.set_visible_devices(visible_gpus, \"GPU\")\n# strategy = tf.distribute.MirroredStrategy()\nimport fire\nfrom tensorflow_asr.utils import env_util\nlogger = env_util.setup_environment()\nfrom tensorflow_asr.configs.config import Config\nfrom tensorflow_asr.helpers import dataset_helpers, exec_helpers, featurizer_helpers\nfrom tensorflow_asr.models.transducer.conformer import Conformer\nDEFAULT_YAML = os.path.join(os.path.abspath(os.path.dirname(__file__)), \"config.yml\")",
        "detail": "Demo.EMSConformer.test",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "Demo.EMSConformer.test",
        "description": "Demo.EMSConformer.test",
        "peekOfCode": "logger = env_util.setup_environment()\nfrom tensorflow_asr.configs.config import Config\nfrom tensorflow_asr.helpers import dataset_helpers, exec_helpers, featurizer_helpers\nfrom tensorflow_asr.models.transducer.conformer import Conformer\nDEFAULT_YAML = os.path.join(os.path.abspath(os.path.dirname(__file__)), \"config.yml\")\nfrom datetime import datetime\nimport argparse\n\"\"\"\nThe main function to evaluate the WER and CER of EMSConformer on our EMS audio dataset. It uses EMSConformer to \ntranscribe EMS audio dataset and correspondinly compute WER and CER",
        "detail": "Demo.EMSConformer.test",
        "documentation": {}
    },
    {
        "label": "DEFAULT_YAML",
        "kind": 5,
        "importPath": "Demo.EMSConformer.test",
        "description": "Demo.EMSConformer.test",
        "peekOfCode": "DEFAULT_YAML = os.path.join(os.path.abspath(os.path.dirname(__file__)), \"config.yml\")\nfrom datetime import datetime\nimport argparse\n\"\"\"\nThe main function to evaluate the WER and CER of EMSConformer on our EMS audio dataset. It uses EMSConformer to \ntranscribe EMS audio dataset and correspondinly compute WER and CER\nInput:  1) output: path of the intermediate transcription result output text file\n        2) saved: the conformer h5 model used to transcribe the EMS audio files\n        3) config: the configuration file to configure the running parameters, including the testset path\nOutput: the WER and CER for both greedy search and beam search. The beam search is still under development.",
        "detail": "Demo.EMSConformer.test",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Demo.EMSConformer.train",
        "description": "Demo.EMSConformer.train",
        "peekOfCode": "def main(\n    config: str = DEFAULT_YAML,\n    tfrecords: bool = False,\n    sentence_piece: bool = False,\n    subwords: bool = True,\n    bs: int = None,\n    spx: int = 1,\n    metadata: str = None,\n    static_length: bool = False,\n#    devices: list = [2],",
        "detail": "Demo.EMSConformer.train",
        "documentation": {}
    },
    {
        "label": "devices",
        "kind": 5,
        "importPath": "Demo.EMSConformer.train",
        "description": "Demo.EMSConformer.train",
        "peekOfCode": "devices = [0]\ngpus = tf.config.list_physical_devices(\"GPU\")\nvisible_gpus = [gpus[i] for i in devices]\ntf.config.set_visible_devices(visible_gpus, \"GPU\")\n#strategy = tf.distribute.MirroredStrategy()\nimport os\nimport fire\nimport math\nfrom tensorflow_asr.utils import env_util\nlogger = env_util.setup_environment()",
        "detail": "Demo.EMSConformer.train",
        "documentation": {}
    },
    {
        "label": "gpus",
        "kind": 5,
        "importPath": "Demo.EMSConformer.train",
        "description": "Demo.EMSConformer.train",
        "peekOfCode": "gpus = tf.config.list_physical_devices(\"GPU\")\nvisible_gpus = [gpus[i] for i in devices]\ntf.config.set_visible_devices(visible_gpus, \"GPU\")\n#strategy = tf.distribute.MirroredStrategy()\nimport os\nimport fire\nimport math\nfrom tensorflow_asr.utils import env_util\nlogger = env_util.setup_environment()\nfrom tensorflow_asr.configs.config import Config",
        "detail": "Demo.EMSConformer.train",
        "documentation": {}
    },
    {
        "label": "visible_gpus",
        "kind": 5,
        "importPath": "Demo.EMSConformer.train",
        "description": "Demo.EMSConformer.train",
        "peekOfCode": "visible_gpus = [gpus[i] for i in devices]\ntf.config.set_visible_devices(visible_gpus, \"GPU\")\n#strategy = tf.distribute.MirroredStrategy()\nimport os\nimport fire\nimport math\nfrom tensorflow_asr.utils import env_util\nlogger = env_util.setup_environment()\nfrom tensorflow_asr.configs.config import Config\nfrom tensorflow_asr.helpers import featurizer_helpers, dataset_helpers",
        "detail": "Demo.EMSConformer.train",
        "documentation": {}
    },
    {
        "label": "#strategy",
        "kind": 5,
        "importPath": "Demo.EMSConformer.train",
        "description": "Demo.EMSConformer.train",
        "peekOfCode": "#strategy = tf.distribute.MirroredStrategy()\nimport os\nimport fire\nimport math\nfrom tensorflow_asr.utils import env_util\nlogger = env_util.setup_environment()\nfrom tensorflow_asr.configs.config import Config\nfrom tensorflow_asr.helpers import featurizer_helpers, dataset_helpers\nfrom tensorflow_asr.models.transducer.conformer import Conformer\nfrom tensorflow_asr.optimizers.schedules import TransformerSchedule",
        "detail": "Demo.EMSConformer.train",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "Demo.EMSConformer.train",
        "description": "Demo.EMSConformer.train",
        "peekOfCode": "logger = env_util.setup_environment()\nfrom tensorflow_asr.configs.config import Config\nfrom tensorflow_asr.helpers import featurizer_helpers, dataset_helpers\nfrom tensorflow_asr.models.transducer.conformer import Conformer\nfrom tensorflow_asr.optimizers.schedules import TransformerSchedule\nDEFAULT_YAML = os.path.join(os.path.abspath(os.path.dirname(__file__)), \"config.yml\")\nfrom datetime import datetime\nimport argparse\ndef main(\n    config: str = DEFAULT_YAML,",
        "detail": "Demo.EMSConformer.train",
        "documentation": {}
    },
    {
        "label": "DEFAULT_YAML",
        "kind": 5,
        "importPath": "Demo.EMSConformer.train",
        "description": "Demo.EMSConformer.train",
        "peekOfCode": "DEFAULT_YAML = os.path.join(os.path.abspath(os.path.dirname(__file__)), \"config.yml\")\nfrom datetime import datetime\nimport argparse\ndef main(\n    config: str = DEFAULT_YAML,\n    tfrecords: bool = False,\n    sentence_piece: bool = False,\n    subwords: bool = True,\n    bs: int = None,\n    spx: int = 1,",
        "detail": "Demo.EMSConformer.train",
        "documentation": {}
    },
    {
        "label": "EMSVision",
        "kind": 2,
        "importPath": "Demo.EMSVision.EMSVisionSystem",
        "description": "Demo.EMSVision.EMSVisionSystem",
        "peekOfCode": "def EMSVision(FeedbackQueue, VideoDataQueue):\n    # More models in the model hub.\n    model_name = pipeline_config.vision_model_type\n    classifier = pipeline(\"zero-shot-image-classification\", model = model_name, device=0)\n    while True:\n        try:\n            protocol_msg = FeedbackQueue.get()\n            message = VideoDataQueue.get()\n            if(message[\"signal\"] == \"Kill\" or protocol_msg == \"Kill\"): \n                print(\"[EMS Vision Thread received Kill Signal. Bye!]\")",
        "detail": "Demo.EMSVision.EMSVisionSystem",
        "documentation": {}
    },
    {
        "label": "cv_ecg_12_lead",
        "kind": 5,
        "importPath": "Demo.EMSVision.ems_action_keywords",
        "description": "Demo.EMSVision.ems_action_keywords",
        "peekOfCode": "cv_ecg_12_lead = [\"An ECG monitor\",\"Attaching Twelve ECG leads on chest\"]\ncv_defibrillation_manual = [\"Defibrillator\",\"Attaching two Defib pads on chest\"]\ncpr_manual = []\nadv_airway_capnography = []\nadmin_epi = []\n# respiratory distress\nresp_assist_ventilation_bvm_via_mask = [\"Placing Oxygen mask on face\",\"BVM Mask\" ]\nresp_nebulizer_therapy = [\"Attaching nebulizer\", \"Nebulizer mask\"]\nresp_airway_adjunct = [\"Inserting airway adjunct\"]\nresp_endotracheal_tube = [\"Inserting endotracheal tube\", \"Endotracheal intubation\"]",
        "detail": "Demo.EMSVision.ems_action_keywords",
        "documentation": {}
    },
    {
        "label": "cv_defibrillation_manual",
        "kind": 5,
        "importPath": "Demo.EMSVision.ems_action_keywords",
        "description": "Demo.EMSVision.ems_action_keywords",
        "peekOfCode": "cv_defibrillation_manual = [\"Defibrillator\",\"Attaching two Defib pads on chest\"]\ncpr_manual = []\nadv_airway_capnography = []\nadmin_epi = []\n# respiratory distress\nresp_assist_ventilation_bvm_via_mask = [\"Placing Oxygen mask on face\",\"BVM Mask\" ]\nresp_nebulizer_therapy = [\"Attaching nebulizer\", \"Nebulizer mask\"]\nresp_airway_adjunct = [\"Inserting airway adjunct\"]\nresp_endotracheal_tube = [\"Inserting endotracheal tube\", \"Endotracheal intubation\"]\nresp_administer_albuterol = [\"Administering albuterol\", \"Attaching metered dose inhaler\"]",
        "detail": "Demo.EMSVision.ems_action_keywords",
        "documentation": {}
    },
    {
        "label": "cpr_manual",
        "kind": 5,
        "importPath": "Demo.EMSVision.ems_action_keywords",
        "description": "Demo.EMSVision.ems_action_keywords",
        "peekOfCode": "cpr_manual = []\nadv_airway_capnography = []\nadmin_epi = []\n# respiratory distress\nresp_assist_ventilation_bvm_via_mask = [\"Placing Oxygen mask on face\",\"BVM Mask\" ]\nresp_nebulizer_therapy = [\"Attaching nebulizer\", \"Nebulizer mask\"]\nresp_airway_adjunct = [\"Inserting airway adjunct\"]\nresp_endotracheal_tube = [\"Inserting endotracheal tube\", \"Endotracheal intubation\"]\nresp_administer_albuterol = [\"Administering albuterol\", \"Attaching metered dose inhaler\"]\nresp_administer_albuterol = [\"Administering albuterol\"]",
        "detail": "Demo.EMSVision.ems_action_keywords",
        "documentation": {}
    },
    {
        "label": "adv_airway_capnography",
        "kind": 5,
        "importPath": "Demo.EMSVision.ems_action_keywords",
        "description": "Demo.EMSVision.ems_action_keywords",
        "peekOfCode": "adv_airway_capnography = []\nadmin_epi = []\n# respiratory distress\nresp_assist_ventilation_bvm_via_mask = [\"Placing Oxygen mask on face\",\"BVM Mask\" ]\nresp_nebulizer_therapy = [\"Attaching nebulizer\", \"Nebulizer mask\"]\nresp_airway_adjunct = [\"Inserting airway adjunct\"]\nresp_endotracheal_tube = [\"Inserting endotracheal tube\", \"Endotracheal intubation\"]\nresp_administer_albuterol = [\"Administering albuterol\", \"Attaching metered dose inhaler\"]\nresp_administer_albuterol = [\"Administering albuterol\"]\n# general",
        "detail": "Demo.EMSVision.ems_action_keywords",
        "documentation": {}
    },
    {
        "label": "admin_epi",
        "kind": 5,
        "importPath": "Demo.EMSVision.ems_action_keywords",
        "description": "Demo.EMSVision.ems_action_keywords",
        "peekOfCode": "admin_epi = []\n# respiratory distress\nresp_assist_ventilation_bvm_via_mask = [\"Placing Oxygen mask on face\",\"BVM Mask\" ]\nresp_nebulizer_therapy = [\"Attaching nebulizer\", \"Nebulizer mask\"]\nresp_airway_adjunct = [\"Inserting airway adjunct\"]\nresp_endotracheal_tube = [\"Inserting endotracheal tube\", \"Endotracheal intubation\"]\nresp_administer_albuterol = [\"Administering albuterol\", \"Attaching metered dose inhaler\"]\nresp_administer_albuterol = [\"Administering albuterol\"]\n# general\niv_access = [\"Inserting IV to arm\",\"Inserting IV to leg\"]",
        "detail": "Demo.EMSVision.ems_action_keywords",
        "documentation": {}
    },
    {
        "label": "resp_assist_ventilation_bvm_via_mask",
        "kind": 5,
        "importPath": "Demo.EMSVision.ems_action_keywords",
        "description": "Demo.EMSVision.ems_action_keywords",
        "peekOfCode": "resp_assist_ventilation_bvm_via_mask = [\"Placing Oxygen mask on face\",\"BVM Mask\" ]\nresp_nebulizer_therapy = [\"Attaching nebulizer\", \"Nebulizer mask\"]\nresp_airway_adjunct = [\"Inserting airway adjunct\"]\nresp_endotracheal_tube = [\"Inserting endotracheal tube\", \"Endotracheal intubation\"]\nresp_administer_albuterol = [\"Administering albuterol\", \"Attaching metered dose inhaler\"]\nresp_administer_albuterol = [\"Administering albuterol\"]\n# general\niv_access = [\"Inserting IV to arm\",\"Inserting IV to leg\"]",
        "detail": "Demo.EMSVision.ems_action_keywords",
        "documentation": {}
    },
    {
        "label": "resp_nebulizer_therapy",
        "kind": 5,
        "importPath": "Demo.EMSVision.ems_action_keywords",
        "description": "Demo.EMSVision.ems_action_keywords",
        "peekOfCode": "resp_nebulizer_therapy = [\"Attaching nebulizer\", \"Nebulizer mask\"]\nresp_airway_adjunct = [\"Inserting airway adjunct\"]\nresp_endotracheal_tube = [\"Inserting endotracheal tube\", \"Endotracheal intubation\"]\nresp_administer_albuterol = [\"Administering albuterol\", \"Attaching metered dose inhaler\"]\nresp_administer_albuterol = [\"Administering albuterol\"]\n# general\niv_access = [\"Inserting IV to arm\",\"Inserting IV to leg\"]",
        "detail": "Demo.EMSVision.ems_action_keywords",
        "documentation": {}
    },
    {
        "label": "resp_airway_adjunct",
        "kind": 5,
        "importPath": "Demo.EMSVision.ems_action_keywords",
        "description": "Demo.EMSVision.ems_action_keywords",
        "peekOfCode": "resp_airway_adjunct = [\"Inserting airway adjunct\"]\nresp_endotracheal_tube = [\"Inserting endotracheal tube\", \"Endotracheal intubation\"]\nresp_administer_albuterol = [\"Administering albuterol\", \"Attaching metered dose inhaler\"]\nresp_administer_albuterol = [\"Administering albuterol\"]\n# general\niv_access = [\"Inserting IV to arm\",\"Inserting IV to leg\"]",
        "detail": "Demo.EMSVision.ems_action_keywords",
        "documentation": {}
    },
    {
        "label": "resp_endotracheal_tube",
        "kind": 5,
        "importPath": "Demo.EMSVision.ems_action_keywords",
        "description": "Demo.EMSVision.ems_action_keywords",
        "peekOfCode": "resp_endotracheal_tube = [\"Inserting endotracheal tube\", \"Endotracheal intubation\"]\nresp_administer_albuterol = [\"Administering albuterol\", \"Attaching metered dose inhaler\"]\nresp_administer_albuterol = [\"Administering albuterol\"]\n# general\niv_access = [\"Inserting IV to arm\",\"Inserting IV to leg\"]",
        "detail": "Demo.EMSVision.ems_action_keywords",
        "documentation": {}
    },
    {
        "label": "resp_administer_albuterol",
        "kind": 5,
        "importPath": "Demo.EMSVision.ems_action_keywords",
        "description": "Demo.EMSVision.ems_action_keywords",
        "peekOfCode": "resp_administer_albuterol = [\"Administering albuterol\", \"Attaching metered dose inhaler\"]\nresp_administer_albuterol = [\"Administering albuterol\"]\n# general\niv_access = [\"Inserting IV to arm\",\"Inserting IV to leg\"]",
        "detail": "Demo.EMSVision.ems_action_keywords",
        "documentation": {}
    },
    {
        "label": "resp_administer_albuterol",
        "kind": 5,
        "importPath": "Demo.EMSVision.ems_action_keywords",
        "description": "Demo.EMSVision.ems_action_keywords",
        "peekOfCode": "resp_administer_albuterol = [\"Administering albuterol\"]\n# general\niv_access = [\"Inserting IV to arm\",\"Inserting IV to leg\"]",
        "detail": "Demo.EMSVision.ems_action_keywords",
        "documentation": {}
    },
    {
        "label": "iv_access",
        "kind": 5,
        "importPath": "Demo.EMSVision.ems_action_keywords",
        "description": "Demo.EMSVision.ems_action_keywords",
        "peekOfCode": "iv_access = [\"Inserting IV to arm\",\"Inserting IV to leg\"]",
        "detail": "Demo.EMSVision.ems_action_keywords",
        "documentation": {}
    },
    {
        "label": "adult_cardiact_arrest_protocol",
        "kind": 5,
        "importPath": "Demo.EMSVision.ems_knowledge",
        "description": "Demo.EMSVision.ems_knowledge",
        "peekOfCode": "adult_cardiact_arrest_protocol = {\"cv_ecg_12_lead\":cv_ecg_12_lead,\n                                  \"cv_defibrillation_manual\":cv_defibrillation_manual,\n                                  \"cpr_manual\":cpr_manual,\n                                  \"iv_access\":iv_access,\n                                  \"adv_airway_capnography\":adv_airway_capnography,\n                                  \"admin_epi\":admin_epi\n                                  }\nrespiratory_distress_protocol = {\"resp_assist_ventilation_bvm_via_mask\":resp_assist_ventilation_bvm_via_mask,\n                                 \"resp_nebulizer_therapy\":resp_nebulizer_therapy,\n                                 \"resp_airway_adjunct\":resp_airway_adjunct,",
        "detail": "Demo.EMSVision.ems_knowledge",
        "documentation": {}
    },
    {
        "label": "respiratory_distress_protocol",
        "kind": 5,
        "importPath": "Demo.EMSVision.ems_knowledge",
        "description": "Demo.EMSVision.ems_knowledge",
        "peekOfCode": "respiratory_distress_protocol = {\"resp_assist_ventilation_bvm_via_mask\":resp_assist_ventilation_bvm_via_mask,\n                                 \"resp_nebulizer_therapy\":resp_nebulizer_therapy,\n                                 \"resp_airway_adjunct\":resp_airway_adjunct,\n                                 \"resp_endotracheal_tube\":resp_endotracheal_tube,\n                                 \"resp_administer_albuterol\":resp_administer_albuterol,\n                                }\nems_interventions = {\"medical - chest pain - cardiac suspected (protocol 2 - 1)\":adult_cardiact_arrest_protocol,\n                             'medical - respiratory distress/asthma/copd/croup/reactive airway (respiratory distress)' :respiratory_distress_protocol}",
        "detail": "Demo.EMSVision.ems_knowledge",
        "documentation": {}
    },
    {
        "label": "ems_interventions",
        "kind": 5,
        "importPath": "Demo.EMSVision.ems_knowledge",
        "description": "Demo.EMSVision.ems_knowledge",
        "peekOfCode": "ems_interventions = {\"medical - chest pain - cardiac suspected (protocol 2 - 1)\":adult_cardiact_arrest_protocol,\n                             'medical - respiratory distress/asthma/copd/croup/reactive airway (respiratory distress)' :respiratory_distress_protocol}",
        "detail": "Demo.EMSVision.ems_knowledge",
        "documentation": {}
    },
    {
        "label": "generate_labels",
        "kind": 2,
        "importPath": "Demo.EMSVision.utils",
        "description": "Demo.EMSVision.utils",
        "peekOfCode": "def generate_labels(protocol):\n    if protocol in ems_interventions:\n        return list(itertools.chain.from_iterable(list(ems_interventions.get(protocol).values())))\n    else:\n        return None\ndef classify(image,labels,classifier):\n    start_t = time.time_ns()\n    scores = classifier(image, \n                        candidate_labels = labels)\n    end_t = time.time_ns()",
        "detail": "Demo.EMSVision.utils",
        "documentation": {}
    },
    {
        "label": "classify",
        "kind": 2,
        "importPath": "Demo.EMSVision.utils",
        "description": "Demo.EMSVision.utils",
        "peekOfCode": "def classify(image,labels,classifier):\n    start_t = time.time_ns()\n    scores = classifier(image, \n                        candidate_labels = labels)\n    end_t = time.time_ns()\n    latency = (end_t-start_t)/1e6\n    return scores,latency",
        "detail": "Demo.EMSVision.utils",
        "documentation": {}
    },
    {
        "label": "transcribe",
        "kind": 2,
        "importPath": "Demo.EMSWhisper.WhisperAgent",
        "description": "Demo.EMSWhisper.WhisperAgent",
        "peekOfCode": "def transcribe(SpeechToNLPQueue,pipe, myrecording):\n    # Code for the second part of the task\n    # Send data to process1 through the queue\n                # run inference\n    pipeaudio = myrecording.ravel()\n    start_t = time.perf_counter()\n    output = pipe(pipeaudio)\n    end_t = time.perf_counter()\n    latency = (end_t-start_t)*1e3\n    transcript = output[\"text\"]",
        "detail": "Demo.EMSWhisper.WhisperAgent",
        "documentation": {}
    },
    {
        "label": "WhisperPipeline",
        "kind": 2,
        "importPath": "Demo.EMSWhisper.WhisperAgent",
        "description": "Demo.EMSWhisper.WhisperAgent",
        "peekOfCode": "def WhisperPipeline( SpeechToNLPQueue, WhisperSignalQueue):\n    # read in audio file\n    # audio_file = \"jfk.wav\"\n    # audio, rate = librosa.load(audio_file, sr=16000)\n    model_repo = tiny_version_to_name_dict[version] if model_type == \"tiny\" else base_version_to_name_dict[version]\n    pipe = pipeline(model=model_repo, batch_size=1, device=0, chunk_length_s=5, stride_length_s=(2, 2)) # set device to 0 to run on cuda\n    data_queue = queue.Queue()\n    audio_buffer = np.zeros((1600,1))\n    while True:\n        print(\"SignalQueue\",WhisperSignalQueue, WhisperSignalQueue.qsize())",
        "detail": "Demo.EMSWhisper.WhisperAgent",
        "documentation": {}
    },
    {
        "label": "model_type",
        "kind": 5,
        "importPath": "Demo.EMSWhisper.WhisperAgent",
        "description": "Demo.EMSWhisper.WhisperAgent",
        "peekOfCode": "model_type = \"tiny\" # or \"base\"\nmodel_type = \"base\"\nversion = 3\ntiny_version_to_name_dict = {\n    2: \"saahith/tiny.en-combined_v4-1-0.1-32-1e-05-fragrant-sweep-3\",\n    3: \"saahith/tiny.en-combined_v4-1-0.1-32-1e-05-revived-sweep-7\",\n    4: \"saahith/tiny.en-combined_v4-1-0-32-1e-05-glamorous-sweep-1\",\n    5: \"saahith/tiny.en-tiny.en-combined_v4-1-0.1-8-1e-06-sunny-sweep-55\",\n}\nbase_version_to_name_dict = {",
        "detail": "Demo.EMSWhisper.WhisperAgent",
        "documentation": {}
    },
    {
        "label": "model_type",
        "kind": 5,
        "importPath": "Demo.EMSWhisper.WhisperAgent",
        "description": "Demo.EMSWhisper.WhisperAgent",
        "peekOfCode": "model_type = \"base\"\nversion = 3\ntiny_version_to_name_dict = {\n    2: \"saahith/tiny.en-combined_v4-1-0.1-32-1e-05-fragrant-sweep-3\",\n    3: \"saahith/tiny.en-combined_v4-1-0.1-32-1e-05-revived-sweep-7\",\n    4: \"saahith/tiny.en-combined_v4-1-0-32-1e-05-glamorous-sweep-1\",\n    5: \"saahith/tiny.en-tiny.en-combined_v4-1-0.1-8-1e-06-sunny-sweep-55\",\n}\nbase_version_to_name_dict = {\n    3: \"saahith/base.en-combined_v4-2-0-8-1e-05-deft-sweep-10\",",
        "detail": "Demo.EMSWhisper.WhisperAgent",
        "documentation": {}
    },
    {
        "label": "version",
        "kind": 5,
        "importPath": "Demo.EMSWhisper.WhisperAgent",
        "description": "Demo.EMSWhisper.WhisperAgent",
        "peekOfCode": "version = 3\ntiny_version_to_name_dict = {\n    2: \"saahith/tiny.en-combined_v4-1-0.1-32-1e-05-fragrant-sweep-3\",\n    3: \"saahith/tiny.en-combined_v4-1-0.1-32-1e-05-revived-sweep-7\",\n    4: \"saahith/tiny.en-combined_v4-1-0-32-1e-05-glamorous-sweep-1\",\n    5: \"saahith/tiny.en-tiny.en-combined_v4-1-0.1-8-1e-06-sunny-sweep-55\",\n}\nbase_version_to_name_dict = {\n    3: \"saahith/base.en-combined_v4-2-0-8-1e-05-deft-sweep-10\",\n    4: \"saahith/base.en-combined_v4-1-0-8-1e-05-elated-sweep-2\",",
        "detail": "Demo.EMSWhisper.WhisperAgent",
        "documentation": {}
    },
    {
        "label": "tiny_version_to_name_dict",
        "kind": 5,
        "importPath": "Demo.EMSWhisper.WhisperAgent",
        "description": "Demo.EMSWhisper.WhisperAgent",
        "peekOfCode": "tiny_version_to_name_dict = {\n    2: \"saahith/tiny.en-combined_v4-1-0.1-32-1e-05-fragrant-sweep-3\",\n    3: \"saahith/tiny.en-combined_v4-1-0.1-32-1e-05-revived-sweep-7\",\n    4: \"saahith/tiny.en-combined_v4-1-0-32-1e-05-glamorous-sweep-1\",\n    5: \"saahith/tiny.en-tiny.en-combined_v4-1-0.1-8-1e-06-sunny-sweep-55\",\n}\nbase_version_to_name_dict = {\n    3: \"saahith/base.en-combined_v4-2-0-8-1e-05-deft-sweep-10\",\n    4: \"saahith/base.en-combined_v4-1-0-8-1e-05-elated-sweep-2\",\n    5: \"saahith/base.en-combined_v4-1-0.1-8-1e-05-charmed-sweep-25\",",
        "detail": "Demo.EMSWhisper.WhisperAgent",
        "documentation": {}
    },
    {
        "label": "base_version_to_name_dict",
        "kind": 5,
        "importPath": "Demo.EMSWhisper.WhisperAgent",
        "description": "Demo.EMSWhisper.WhisperAgent",
        "peekOfCode": "base_version_to_name_dict = {\n    3: \"saahith/base.en-combined_v4-2-0-8-1e-05-deft-sweep-10\",\n    4: \"saahith/base.en-combined_v4-1-0-8-1e-05-elated-sweep-2\",\n    5: \"saahith/base.en-combined_v4-1-0.1-8-1e-05-charmed-sweep-25\",\n    6: \"saahith/base.en-combined_v4-1-0-16-1e-05-swept-sweep-3\",\n}\ndef transcribe(SpeechToNLPQueue,pipe, myrecording):\n    # Code for the second part of the task\n    # Send data to process1 through the queue\n                # run inference",
        "detail": "Demo.EMSWhisper.WhisperAgent",
        "documentation": {}
    },
    {
        "label": "getAudioStremText",
        "kind": 2,
        "importPath": "Demo.Form_Filling.gui2",
        "description": "Demo.Form_Filling.gui2",
        "peekOfCode": "def getAudioStremText():\n    return dummy13",
        "detail": "Demo.Form_Filling.gui2",
        "documentation": {}
    },
    {
        "label": "metamapExtract",
        "kind": 2,
        "importPath": "Demo.Form_Filling.metamap2",
        "description": "Demo.Form_Filling.metamap2",
        "peekOfCode": "def metamapExtract(sentenceList):\n    conceptNumList=[]\n    UMLSconceptList = []\n    for i in range(len(sentenceList)):\n        conceptNumList.append(i+1)\n    concepts,error = mm.extract_concepts(sentenceList, conceptNumList)\n    for concept in concepts:\n        if (float(concept[2])>09.99):\n            UMLSconceptList.append(concept)\n# =============================================================================",
        "detail": "Demo.Form_Filling.metamap2",
        "documentation": {}
    },
    {
        "label": "metamapExtractt",
        "kind": 2,
        "importPath": "Demo.Form_Filling.metamap2",
        "description": "Demo.Form_Filling.metamap2",
        "peekOfCode": "def metamapExtractt(sentenceList,threshold):\n    conceptNumList=[]\n    UMLSconceptList = []\n    for i in range(len(sentenceList)):\n        conceptNumList.append(i+1)\n    concepts,error = mm.extract_concepts(sentenceList, conceptNumList)\n    for concept in concepts:\n        if (float(concept[2])>threshold):\n            UMLSconceptList.append(concept)\n# =============================================================================",
        "detail": "Demo.Form_Filling.metamap2",
        "documentation": {}
    },
    {
        "label": "#mm",
        "kind": 5,
        "importPath": "Demo.Form_Filling.metamap2",
        "description": "Demo.Form_Filling.metamap2",
        "peekOfCode": "#mm = MetaMap.get_instance('/home/arif/Desktop/metamap/public_mm/bin/metamap16')\nmm = MetaMap.get_instance('./public_mm/bin/metamap16')\n#for abs path, add \"/home/arif/Desktop/metamap/\"\n#dummy=[\"Ems and fire were dispatched to the behavioral clinic for a 51 year old female complaining of shortness of breath. Upon arriving on location, we found the female with Richmond fire department, the patient stated she had not been feeling well since this morning and had shortness of breath. She said that she had not been feeling well since this morning and it was describing a feeling of chest fullness with shortness of breath. She stated it felt similar to asthma and acid reflux, but worse than either of them, she said she had taken. She had attempted two treatments with her albuterol inhaler 30 minutes before our arrival, but did not gain any relief. She also complained of a headache and dizziness patient had no significant cardiac history and her only medical history with asthma, hypertension and behavioral disorders. The patient was alert and oriented times 4 answering all questions appropriately. She was able to speak in full sentences between breaths upon auscultation of the lungs only found wheezing in all quadrants. A 12-lead was performed and showed no ectopy and sinus rhythm. She was also placed on intitle monitoring after hearing the wheezing when she was given a duoneb treatment via nebulizer. At this time she decided that she wanted to be transported to VCU hospital to be checked out. We then assisted her from the wheelchair to the stretcher, secured her to the stretcher and transported without incident to VCU hospital care was transferred to nursing staff upon arrival.\"]\n#dummy1=[\"We'Ve initiated an IV given a saline bolus and 4 mg of ondansetron and with accompanied Improvement in patient condition.\"]\ndef metamapExtract(sentenceList):\n    conceptNumList=[]\n    UMLSconceptList = []\n    for i in range(len(sentenceList)):\n        conceptNumList.append(i+1)",
        "detail": "Demo.Form_Filling.metamap2",
        "documentation": {}
    },
    {
        "label": "mm",
        "kind": 5,
        "importPath": "Demo.Form_Filling.metamap2",
        "description": "Demo.Form_Filling.metamap2",
        "peekOfCode": "mm = MetaMap.get_instance('./public_mm/bin/metamap16')\n#for abs path, add \"/home/arif/Desktop/metamap/\"\n#dummy=[\"Ems and fire were dispatched to the behavioral clinic for a 51 year old female complaining of shortness of breath. Upon arriving on location, we found the female with Richmond fire department, the patient stated she had not been feeling well since this morning and had shortness of breath. She said that she had not been feeling well since this morning and it was describing a feeling of chest fullness with shortness of breath. She stated it felt similar to asthma and acid reflux, but worse than either of them, she said she had taken. She had attempted two treatments with her albuterol inhaler 30 minutes before our arrival, but did not gain any relief. She also complained of a headache and dizziness patient had no significant cardiac history and her only medical history with asthma, hypertension and behavioral disorders. The patient was alert and oriented times 4 answering all questions appropriately. She was able to speak in full sentences between breaths upon auscultation of the lungs only found wheezing in all quadrants. A 12-lead was performed and showed no ectopy and sinus rhythm. She was also placed on intitle monitoring after hearing the wheezing when she was given a duoneb treatment via nebulizer. At this time she decided that she wanted to be transported to VCU hospital to be checked out. We then assisted her from the wheelchair to the stretcher, secured her to the stretcher and transported without incident to VCU hospital care was transferred to nursing staff upon arrival.\"]\n#dummy1=[\"We'Ve initiated an IV given a saline bolus and 4 mg of ondansetron and with accompanied Improvement in patient condition.\"]\ndef metamapExtract(sentenceList):\n    conceptNumList=[]\n    UMLSconceptList = []\n    for i in range(len(sentenceList)):\n        conceptNumList.append(i+1)\n    concepts,error = mm.extract_concepts(sentenceList, conceptNumList)",
        "detail": "Demo.Form_Filling.metamap2",
        "documentation": {}
    },
    {
        "label": "generateFields",
        "kind": 2,
        "importPath": "Demo.Form_Filling.prescription_form2",
        "description": "Demo.Form_Filling.prescription_form2",
        "peekOfCode": "def generateFields(sentenceList, wordList, narrative, file_name):\n    wordList=[x.lower() for x in wordList]\n    numOfSentences=len(sentenceList)\n    numOfWords=len(wordList)\n    print(\"Sentences: \"+str(numOfSentences)+\" Words: \" +str(numOfWords))\n    #print(\"HELLO\")\n    UMLSconceptListGT10=metamap2.metamapExtract(sentenceList)\n    ####################################### name\n    namePatient=''\n    namePatient2=''",
        "detail": "Demo.Form_Filling.prescription_form2",
        "documentation": {}
    },
    {
        "label": "#mm",
        "kind": 5,
        "importPath": "Demo.Form_Filling.prescription_form2",
        "description": "Demo.Form_Filling.prescription_form2",
        "peekOfCode": "#mm = MetaMap.get_instance('/home/arif/Desktop/metamap/public_mm/bin/metamap16')\nmm = MetaMap.get_instance('./public_mm/bin/metamap16')\n#for abs path, add \"/home/arif/Desktop/metamap/\"\ncaseNo='023'\ndef generateFields(sentenceList, wordList, narrative, file_name):\n    wordList=[x.lower() for x in wordList]\n    numOfSentences=len(sentenceList)\n    numOfWords=len(wordList)\n    print(\"Sentences: \"+str(numOfSentences)+\" Words: \" +str(numOfWords))\n    #print(\"HELLO\")",
        "detail": "Demo.Form_Filling.prescription_form2",
        "documentation": {}
    },
    {
        "label": "mm",
        "kind": 5,
        "importPath": "Demo.Form_Filling.prescription_form2",
        "description": "Demo.Form_Filling.prescription_form2",
        "peekOfCode": "mm = MetaMap.get_instance('./public_mm/bin/metamap16')\n#for abs path, add \"/home/arif/Desktop/metamap/\"\ncaseNo='023'\ndef generateFields(sentenceList, wordList, narrative, file_name):\n    wordList=[x.lower() for x in wordList]\n    numOfSentences=len(sentenceList)\n    numOfWords=len(wordList)\n    print(\"Sentences: \"+str(numOfSentences)+\" Words: \" +str(numOfWords))\n    #print(\"HELLO\")\n    UMLSconceptListGT10=metamap2.metamapExtract(sentenceList)",
        "detail": "Demo.Form_Filling.prescription_form2",
        "documentation": {}
    },
    {
        "label": "op",
        "kind": 5,
        "importPath": "Demo.Form_Filling.readText2",
        "description": "Demo.Form_Filling.readText2",
        "peekOfCode": "op = commands.getstatusoutput(part1)\noutput = op[1].rsplit('\\n', 1)[1]",
        "detail": "Demo.Form_Filling.readText2",
        "documentation": {}
    },
    {
        "label": "output",
        "kind": 5,
        "importPath": "Demo.Form_Filling.readText2",
        "description": "Demo.Form_Filling.readText2",
        "peekOfCode": "output = op[1].rsplit('\\n', 1)[1]",
        "detail": "Demo.Form_Filling.readText2",
        "documentation": {}
    },
    {
        "label": "posTagger",
        "kind": 2,
        "importPath": "Demo.Form_Filling.stanfordNER2",
        "description": "Demo.Form_Filling.stanfordNER2",
        "peekOfCode": "def posTagger(sentences):\n    #print(sentenceList)\n    #text=listToText(sentenceList)\n    #print(text)\n    tokenizedWordlist=word_tokenize(sentences)\n    #print(tokenizedWordlist)   \n    wordListPOS=pos_tag(tokenizedWordlist)\n    #print(wordListPOS)\n    return wordListPOS\ndef getEntity(sentences,choice):",
        "detail": "Demo.Form_Filling.stanfordNER2",
        "documentation": {}
    },
    {
        "label": "getEntity",
        "kind": 2,
        "importPath": "Demo.Form_Filling.stanfordNER2",
        "description": "Demo.Form_Filling.stanfordNER2",
        "peekOfCode": "def getEntity(sentences,choice):\n    #print(sentenceList)\n    #text=listToText(sentenceList)\n    #print(text)\n    tokenizedWordlist=word_tokenize(sentences)\n    #print(tokenizedWordlist)\n    if (choice==3):\n        wordListPOS=st3.tag(tokenizedWordlist)\n    if (choice==4):\n        wordListPOS=st4.tag(tokenizedWordlist)",
        "detail": "Demo.Form_Filling.stanfordNER2",
        "documentation": {}
    },
    {
        "label": "textParsing",
        "kind": 2,
        "importPath": "Demo.Form_Filling.textParse2",
        "description": "Demo.Form_Filling.textParse2",
        "peekOfCode": "def textParsing(fileList1):\n    sentsList0=[] #temp\n    for files in fileList1:\n        with open(files, 'r') as in_file:\n            text = in_file.read()\n            #wordsList0.append(nltk.word_tokenize(text))\n            sentsList0.append(sent_tokenize(text))\n        in_file.close()\n    #print(sentsList[1])\n    sentences0=''",
        "detail": "Demo.Form_Filling.textParse2",
        "documentation": {}
    },
    {
        "label": "listToText",
        "kind": 2,
        "importPath": "Demo.Form_Filling.textParse2",
        "description": "Demo.Form_Filling.textParse2",
        "peekOfCode": "def listToText(sentenceList):\n    sentences0=''\n    for i in range(len(sentenceList)):\n        sentences0=sentences0+str(sentenceList[i])\n    sentences1=sentences0.replace(\"[\",\"\")\n    sentences2=sentences1.replace(\"]\",\"\")\n    sentences3=sentences2.replace(\"'\",\"\")\n    sentences4=sentences3.replace(\".,\",\".\")\n    sentences5=sentences4.replace(\".\",\". \")\n    return sentences5",
        "detail": "Demo.Form_Filling.textParse2",
        "documentation": {}
    },
    {
        "label": "StoppableThread",
        "kind": 6,
        "importPath": "Demo.StoppableThread.StoppableThread",
        "description": "Demo.StoppableThread.StoppableThread",
        "peekOfCode": "class StoppableThread(threading.Thread):\n    def __init__(self, *args, **kwargs):\n        super(StoppableThread, self).__init__(*args, **kwargs)\n        self._stop_event = threading.Event()\n    def stop(self):\n        self._stop_event.set()\n    def stopped(self):\n        return self._stop_event.is_set()",
        "detail": "Demo.StoppableThread.StoppableThread",
        "documentation": {}
    },
    {
        "label": "TTS",
        "kind": 2,
        "importPath": "Demo.TTS.TTS",
        "description": "Demo.TTS.TTS",
        "peekOfCode": "def TTS(text):\n    engine.say(text)\n    engine.runAndWait()\nif __name__ == '__main__':\n    #TTS(\"Hello, my name is Joe! I am a cognitive assistant for EMS. How may I help you?\")\n    #StoppableThread(target = TTS, args=(str(pr),)).start() # Text to speech",
        "detail": "Demo.TTS.TTS",
        "documentation": {}
    },
    {
        "label": "engine",
        "kind": 5,
        "importPath": "Demo.TTS.TTS",
        "description": "Demo.TTS.TTS",
        "peekOfCode": "engine = pyttsx.init()\ndef TTS(text):\n    engine.say(text)\n    engine.runAndWait()\nif __name__ == '__main__':\n    #TTS(\"Hello, my name is Joe! I am a cognitive assistant for EMS. How may I help you?\")\n    #StoppableThread(target = TTS, args=(str(pr),)).start() # Text to speech",
        "detail": "Demo.TTS.TTS",
        "documentation": {}
    },
    {
        "label": "ConceptMMI",
        "kind": 6,
        "importPath": "Demo.pymetamap.pymetamap.Concept",
        "description": "Demo.pymetamap.pymetamap.Concept",
        "peekOfCode": "class ConceptMMI(namedtuple('Concept', FIELD_NAMES_MMI)):\n    def __repr__(self):\n        items = [(field, getattr(self, field, None)) for field in FIELD_NAMES_MMI]\n        fields = ['%s=%r' % (k, v) for k, v in items if v is not None]\n        return '%s(%s)' % (self.__class__.__name__, ', '.join(fields))\n    def as_mmi(self):\n        return '|'.join([get(field) for field in FIELD_NAMES_MMI])\n    @classmethod\n    def from_mmi(this_class, line):\n         fields = line.split('|')",
        "detail": "Demo.pymetamap.pymetamap.Concept",
        "documentation": {}
    },
    {
        "label": "ConceptAA",
        "kind": 6,
        "importPath": "Demo.pymetamap.pymetamap.Concept",
        "description": "Demo.pymetamap.pymetamap.Concept",
        "peekOfCode": "class ConceptAA(namedtuple('Concept', FIELD_NAMES_AA)):\n    def __repr__(self):\n        items = [(field, getattr(self, field, None)) for field in FIELD_NAMES_AA]\n        fields = ['%s=%r' % (k, v) for k, v in items if v is not None]\n        return '%s(%s)' % (self.__class__.__name__, ', '.join(fields))\n    def as_mmi(self):\n        return '|'.join([get(field) for field in FIELD_NAMES_AA])\n    @classmethod\n    def from_mmi(this_class, line):\n         fields = line.split('|')",
        "detail": "Demo.pymetamap.pymetamap.Concept",
        "documentation": {}
    },
    {
        "label": "ConceptUA",
        "kind": 6,
        "importPath": "Demo.pymetamap.pymetamap.Concept",
        "description": "Demo.pymetamap.pymetamap.Concept",
        "peekOfCode": "class ConceptUA(namedtuple('Concept', FIELD_NAMES_UA)):\n    def __repr__(self):\n        items = [(field, getattr(self, field, None)) for field in FIELD_NAMES_UA]\n        fields = ['%s=%r' % (k, v) for k, v in items if v is not None]\n        return '%s(%s)' % (self.__class__.__name__, ', '.join(fields))\n    def as_mmi(self):\n        return '|'.join([get(field) for field in FIELD_NAMES_UA])\n    @classmethod\n    def from_mmi(this_class, line):\n         fields = line.split('|')",
        "detail": "Demo.pymetamap.pymetamap.Concept",
        "documentation": {}
    },
    {
        "label": "Corpus",
        "kind": 6,
        "importPath": "Demo.pymetamap.pymetamap.Concept",
        "description": "Demo.pymetamap.pymetamap.Concept",
        "peekOfCode": "class Corpus(list):\n    @classmethod\n    def load(this_class, stream):\n        stream = iter(stream)\n        corpus = this_class()\n        for line in stream:\n            fields = line.split('|')\n            if fields[1] == 'MMI':\n                corpus.append(ConceptMMI.from_mmi(line))\n            elif fields[1] == 'AA':",
        "detail": "Demo.pymetamap.pymetamap.Concept",
        "documentation": {}
    },
    {
        "label": "FIELD_NAMES_MMI",
        "kind": 5,
        "importPath": "Demo.pymetamap.pymetamap.Concept",
        "description": "Demo.pymetamap.pymetamap.Concept",
        "peekOfCode": "FIELD_NAMES_MMI = ('index', 'mm', 'score', 'preferred_name', 'cui', 'semtypes',\n               'trigger', 'location', 'pos_info', 'tree_codes')\nFIELD_NAMES_AA = ('index', 'aa', 'short_form', 'long_form', 'num_tokens_short_form',\n                  'num_chars_short_form', 'num_tokens_long_form',\n                  'num_chars_long_form', 'pos_info')\nFIELD_NAMES_UA = ('index', 'ua', 'short_form', 'long_form', 'num_tokens_short_form',\n                  'num_chars_short_form', 'num_tokens_long_form',\n                  'num_chars_long_form', 'pos_info')\nclass ConceptMMI(namedtuple('Concept', FIELD_NAMES_MMI)):\n    def __repr__(self):",
        "detail": "Demo.pymetamap.pymetamap.Concept",
        "documentation": {}
    },
    {
        "label": "FIELD_NAMES_AA",
        "kind": 5,
        "importPath": "Demo.pymetamap.pymetamap.Concept",
        "description": "Demo.pymetamap.pymetamap.Concept",
        "peekOfCode": "FIELD_NAMES_AA = ('index', 'aa', 'short_form', 'long_form', 'num_tokens_short_form',\n                  'num_chars_short_form', 'num_tokens_long_form',\n                  'num_chars_long_form', 'pos_info')\nFIELD_NAMES_UA = ('index', 'ua', 'short_form', 'long_form', 'num_tokens_short_form',\n                  'num_chars_short_form', 'num_tokens_long_form',\n                  'num_chars_long_form', 'pos_info')\nclass ConceptMMI(namedtuple('Concept', FIELD_NAMES_MMI)):\n    def __repr__(self):\n        items = [(field, getattr(self, field, None)) for field in FIELD_NAMES_MMI]\n        fields = ['%s=%r' % (k, v) for k, v in items if v is not None]",
        "detail": "Demo.pymetamap.pymetamap.Concept",
        "documentation": {}
    },
    {
        "label": "FIELD_NAMES_UA",
        "kind": 5,
        "importPath": "Demo.pymetamap.pymetamap.Concept",
        "description": "Demo.pymetamap.pymetamap.Concept",
        "peekOfCode": "FIELD_NAMES_UA = ('index', 'ua', 'short_form', 'long_form', 'num_tokens_short_form',\n                  'num_chars_short_form', 'num_tokens_long_form',\n                  'num_chars_long_form', 'pos_info')\nclass ConceptMMI(namedtuple('Concept', FIELD_NAMES_MMI)):\n    def __repr__(self):\n        items = [(field, getattr(self, field, None)) for field in FIELD_NAMES_MMI]\n        fields = ['%s=%r' % (k, v) for k, v in items if v is not None]\n        return '%s(%s)' % (self.__class__.__name__, ', '.join(fields))\n    def as_mmi(self):\n        return '|'.join([get(field) for field in FIELD_NAMES_MMI])",
        "detail": "Demo.pymetamap.pymetamap.Concept",
        "documentation": {}
    },
    {
        "label": "MetaMap",
        "kind": 6,
        "importPath": "Demo.pymetamap.pymetamap.MetaMap",
        "description": "Demo.pymetamap.pymetamap.MetaMap",
        "peekOfCode": "class MetaMap:\n    \"\"\" Abstract base class for extracting concepts from text using\n        MetaMap. To use this you will need to have downloaded the\n        recent MetaMap software from NLM. metamap_filename should point\n        to the binary you intend to use.\n        Subclasses need to override the extract_concepts method.\n    \"\"\"\n    __metaclass__ = abc.ABCMeta\n    def __init__(self, metamap_filename, version=None):\n        self.metamap_filename = str(metamap_filename)",
        "detail": "Demo.pymetamap.pymetamap.MetaMap",
        "documentation": {}
    },
    {
        "label": "DEFAULT_METAMAP_VERSION",
        "kind": 5,
        "importPath": "Demo.pymetamap.pymetamap.MetaMap",
        "description": "Demo.pymetamap.pymetamap.MetaMap",
        "peekOfCode": "DEFAULT_METAMAP_VERSION = '2012'\nclass MetaMap:\n    \"\"\" Abstract base class for extracting concepts from text using\n        MetaMap. To use this you will need to have downloaded the\n        recent MetaMap software from NLM. metamap_filename should point\n        to the binary you intend to use.\n        Subclasses need to override the extract_concepts method.\n    \"\"\"\n    __metaclass__ = abc.ABCMeta\n    def __init__(self, metamap_filename, version=None):",
        "detail": "Demo.pymetamap.pymetamap.MetaMap",
        "documentation": {}
    },
    {
        "label": "SubprocessBackend",
        "kind": 6,
        "importPath": "Demo.pymetamap.pymetamap.SubprocessBackend",
        "description": "Demo.pymetamap.pymetamap.SubprocessBackend",
        "peekOfCode": "class SubprocessBackend(MetaMap):\n    def __init__(self, metamap_filename, version=None):\n        \"\"\" Interface to MetaMap using subprocess. This creates a\n            command line call to a specified metamap process.\n        \"\"\"\n        MetaMap.__init__(self, metamap_filename, version)\n    def extract_concepts(self, sentences=None, ids=None,\n                         composite_phrase=4, filename=None,\n                         file_format='sldi', mmi_output=True, allow_acronym_variants=False,\n                         word_sense_disambiguation=False, allow_large_n=False,",
        "detail": "Demo.pymetamap.pymetamap.SubprocessBackend",
        "documentation": {}
    },
    {
        "label": "readme",
        "kind": 2,
        "importPath": "Demo.pymetamap.setup",
        "description": "Demo.pymetamap.setup",
        "peekOfCode": "def readme():\n    with open('README.rst') as f:\n        return f.read()\nsetup(name='pymetamap',\n      version='0.1',\n      description='Python wrapper around MetaMap',\n      long_description=readme(),\n      url='https://github.com/AnthonyMRios/pymetamap',\n      author='Anthony Rios',\n      author_email='anthonymrios@gmail.com',",
        "detail": "Demo.pymetamap.setup",
        "documentation": {}
    },
    {
        "label": "bytes_to_unicode",
        "kind": 2,
        "importPath": "Demo.whisper.cpp.models.convert-h5-to-ggml",
        "description": "Demo.whisper.cpp.models.convert-h5-to-ggml",
        "peekOfCode": "def bytes_to_unicode():\n    \"\"\"\n    Returns list of utf-8 byte and a corresponding list of unicode strings.\n    The reversible bpe codes work on unicode strings.\n    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n    This is a significant percentage of your normal, say, 32K bpe vocab.\n    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n    And avoids mapping to whitespace/control characters the bpe code barfs on.\n    \"\"\"",
        "detail": "Demo.whisper.cpp.models.convert-h5-to-ggml",
        "documentation": {}
    },
    {
        "label": "conv_map",
        "kind": 5,
        "importPath": "Demo.whisper.cpp.models.convert-h5-to-ggml",
        "description": "Demo.whisper.cpp.models.convert-h5-to-ggml",
        "peekOfCode": "conv_map = {\n        'self_attn.k_proj'              : 'attn.key',\n        'self_attn.q_proj'              : 'attn.query',\n        'self_attn.v_proj'              : 'attn.value',\n        'self_attn.out_proj'            : 'attn.out',\n        'self_attn_layer_norm'          : 'attn_ln',\n        'encoder_attn.q_proj'           : 'cross_attn.query',\n        'encoder_attn.v_proj'           : 'cross_attn.value',\n        'encoder_attn.out_proj'         : 'cross_attn.out',\n        'encoder_attn_layer_norm'       : 'cross_attn_ln',",
        "detail": "Demo.whisper.cpp.models.convert-h5-to-ggml",
        "documentation": {}
    },
    {
        "label": "dir_whisper",
        "kind": 5,
        "importPath": "Demo.whisper.cpp.models.convert-h5-to-ggml",
        "description": "Demo.whisper.cpp.models.convert-h5-to-ggml",
        "peekOfCode": "dir_whisper = Path(sys.argv[2])\ndir_out     = Path(sys.argv[3])\nencoder = json.load((dir_model / \"vocab.json\").open(\"r\", encoding=\"utf8\"))\nencoder_added = json.load((dir_model / \"added_tokens.json\").open( \"r\", encoding=\"utf8\"))\nhparams = json.load((dir_model / \"config.json\").open(\"r\", encoding=\"utf8\") )\nmodel = WhisperForConditionalGeneration.from_pretrained(dir_model)\n#code.interact(local=locals())\nn_mels = hparams[\"num_mel_bins\"]\nwith np.load(os.path.join(dir_whisper, \"whisper/assets\", \"mel_filters.npz\")) as f:\n    filters = torch.from_numpy(f[f\"mel_{n_mels}\"])",
        "detail": "Demo.whisper.cpp.models.convert-h5-to-ggml",
        "documentation": {}
    },
    {
        "label": "encoder",
        "kind": 5,
        "importPath": "Demo.whisper.cpp.models.convert-h5-to-ggml",
        "description": "Demo.whisper.cpp.models.convert-h5-to-ggml",
        "peekOfCode": "encoder = json.load((dir_model / \"vocab.json\").open(\"r\", encoding=\"utf8\"))\nencoder_added = json.load((dir_model / \"added_tokens.json\").open( \"r\", encoding=\"utf8\"))\nhparams = json.load((dir_model / \"config.json\").open(\"r\", encoding=\"utf8\") )\nmodel = WhisperForConditionalGeneration.from_pretrained(dir_model)\n#code.interact(local=locals())\nn_mels = hparams[\"num_mel_bins\"]\nwith np.load(os.path.join(dir_whisper, \"whisper/assets\", \"mel_filters.npz\")) as f:\n    filters = torch.from_numpy(f[f\"mel_{n_mels}\"])\ndir_tokenizer = dir_model\nfname_out = dir_out / \"ggml-model.bin\"",
        "detail": "Demo.whisper.cpp.models.convert-h5-to-ggml",
        "documentation": {}
    },
    {
        "label": "encoder_added",
        "kind": 5,
        "importPath": "Demo.whisper.cpp.models.convert-h5-to-ggml",
        "description": "Demo.whisper.cpp.models.convert-h5-to-ggml",
        "peekOfCode": "encoder_added = json.load((dir_model / \"added_tokens.json\").open( \"r\", encoding=\"utf8\"))\nhparams = json.load((dir_model / \"config.json\").open(\"r\", encoding=\"utf8\") )\nmodel = WhisperForConditionalGeneration.from_pretrained(dir_model)\n#code.interact(local=locals())\nn_mels = hparams[\"num_mel_bins\"]\nwith np.load(os.path.join(dir_whisper, \"whisper/assets\", \"mel_filters.npz\")) as f:\n    filters = torch.from_numpy(f[f\"mel_{n_mels}\"])\ndir_tokenizer = dir_model\nfname_out = dir_out / \"ggml-model.bin\"\ntokens = json.load(open(dir_tokenizer / \"vocab.json\", \"r\", encoding=\"utf8\"))",
        "detail": "Demo.whisper.cpp.models.convert-h5-to-ggml",
        "documentation": {}
    },
    {
        "label": "hparams",
        "kind": 5,
        "importPath": "Demo.whisper.cpp.models.convert-h5-to-ggml",
        "description": "Demo.whisper.cpp.models.convert-h5-to-ggml",
        "peekOfCode": "hparams = json.load((dir_model / \"config.json\").open(\"r\", encoding=\"utf8\") )\nmodel = WhisperForConditionalGeneration.from_pretrained(dir_model)\n#code.interact(local=locals())\nn_mels = hparams[\"num_mel_bins\"]\nwith np.load(os.path.join(dir_whisper, \"whisper/assets\", \"mel_filters.npz\")) as f:\n    filters = torch.from_numpy(f[f\"mel_{n_mels}\"])\ndir_tokenizer = dir_model\nfname_out = dir_out / \"ggml-model.bin\"\ntokens = json.load(open(dir_tokenizer / \"vocab.json\", \"r\", encoding=\"utf8\"))\n# use 16-bit or 32-bit floats",
        "detail": "Demo.whisper.cpp.models.convert-h5-to-ggml",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "Demo.whisper.cpp.models.convert-h5-to-ggml",
        "description": "Demo.whisper.cpp.models.convert-h5-to-ggml",
        "peekOfCode": "model = WhisperForConditionalGeneration.from_pretrained(dir_model)\n#code.interact(local=locals())\nn_mels = hparams[\"num_mel_bins\"]\nwith np.load(os.path.join(dir_whisper, \"whisper/assets\", \"mel_filters.npz\")) as f:\n    filters = torch.from_numpy(f[f\"mel_{n_mels}\"])\ndir_tokenizer = dir_model\nfname_out = dir_out / \"ggml-model.bin\"\ntokens = json.load(open(dir_tokenizer / \"vocab.json\", \"r\", encoding=\"utf8\"))\n# use 16-bit or 32-bit floats\nuse_f16 = True",
        "detail": "Demo.whisper.cpp.models.convert-h5-to-ggml",
        "documentation": {}
    },
    {
        "label": "n_mels",
        "kind": 5,
        "importPath": "Demo.whisper.cpp.models.convert-h5-to-ggml",
        "description": "Demo.whisper.cpp.models.convert-h5-to-ggml",
        "peekOfCode": "n_mels = hparams[\"num_mel_bins\"]\nwith np.load(os.path.join(dir_whisper, \"whisper/assets\", \"mel_filters.npz\")) as f:\n    filters = torch.from_numpy(f[f\"mel_{n_mels}\"])\ndir_tokenizer = dir_model\nfname_out = dir_out / \"ggml-model.bin\"\ntokens = json.load(open(dir_tokenizer / \"vocab.json\", \"r\", encoding=\"utf8\"))\n# use 16-bit or 32-bit floats\nuse_f16 = True\nif len(sys.argv) > 4:\n    use_f16 = False",
        "detail": "Demo.whisper.cpp.models.convert-h5-to-ggml",
        "documentation": {}
    },
    {
        "label": "dir_tokenizer",
        "kind": 5,
        "importPath": "Demo.whisper.cpp.models.convert-h5-to-ggml",
        "description": "Demo.whisper.cpp.models.convert-h5-to-ggml",
        "peekOfCode": "dir_tokenizer = dir_model\nfname_out = dir_out / \"ggml-model.bin\"\ntokens = json.load(open(dir_tokenizer / \"vocab.json\", \"r\", encoding=\"utf8\"))\n# use 16-bit or 32-bit floats\nuse_f16 = True\nif len(sys.argv) > 4:\n    use_f16 = False\n    fname_out = dir_out / \"ggml-model-f32.bin\"\nfout = open(fname_out, \"wb\")\nfout.write(struct.pack(\"i\", 0x67676d6c)) # magic: ggml in hex",
        "detail": "Demo.whisper.cpp.models.convert-h5-to-ggml",
        "documentation": {}
    },
    {
        "label": "fname_out",
        "kind": 5,
        "importPath": "Demo.whisper.cpp.models.convert-h5-to-ggml",
        "description": "Demo.whisper.cpp.models.convert-h5-to-ggml",
        "peekOfCode": "fname_out = dir_out / \"ggml-model.bin\"\ntokens = json.load(open(dir_tokenizer / \"vocab.json\", \"r\", encoding=\"utf8\"))\n# use 16-bit or 32-bit floats\nuse_f16 = True\nif len(sys.argv) > 4:\n    use_f16 = False\n    fname_out = dir_out / \"ggml-model-f32.bin\"\nfout = open(fname_out, \"wb\")\nfout.write(struct.pack(\"i\", 0x67676d6c)) # magic: ggml in hex\nfout.write(struct.pack(\"i\", hparams[\"vocab_size\"]))",
        "detail": "Demo.whisper.cpp.models.convert-h5-to-ggml",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": "Demo.whisper.cpp.models.convert-h5-to-ggml",
        "description": "Demo.whisper.cpp.models.convert-h5-to-ggml",
        "peekOfCode": "tokens = json.load(open(dir_tokenizer / \"vocab.json\", \"r\", encoding=\"utf8\"))\n# use 16-bit or 32-bit floats\nuse_f16 = True\nif len(sys.argv) > 4:\n    use_f16 = False\n    fname_out = dir_out / \"ggml-model-f32.bin\"\nfout = open(fname_out, \"wb\")\nfout.write(struct.pack(\"i\", 0x67676d6c)) # magic: ggml in hex\nfout.write(struct.pack(\"i\", hparams[\"vocab_size\"]))\nfout.write(struct.pack(\"i\", hparams[\"max_source_positions\"]))",
        "detail": "Demo.whisper.cpp.models.convert-h5-to-ggml",
        "documentation": {}
    },
    {
        "label": "use_f16",
        "kind": 5,
        "importPath": "Demo.whisper.cpp.models.convert-h5-to-ggml",
        "description": "Demo.whisper.cpp.models.convert-h5-to-ggml",
        "peekOfCode": "use_f16 = True\nif len(sys.argv) > 4:\n    use_f16 = False\n    fname_out = dir_out / \"ggml-model-f32.bin\"\nfout = open(fname_out, \"wb\")\nfout.write(struct.pack(\"i\", 0x67676d6c)) # magic: ggml in hex\nfout.write(struct.pack(\"i\", hparams[\"vocab_size\"]))\nfout.write(struct.pack(\"i\", hparams[\"max_source_positions\"]))\nfout.write(struct.pack(\"i\", hparams[\"d_model\"]))\nfout.write(struct.pack(\"i\", hparams[\"encoder_attention_heads\"]))",
        "detail": "Demo.whisper.cpp.models.convert-h5-to-ggml",
        "documentation": {}
    },
    {
        "label": "fout",
        "kind": 5,
        "importPath": "Demo.whisper.cpp.models.convert-h5-to-ggml",
        "description": "Demo.whisper.cpp.models.convert-h5-to-ggml",
        "peekOfCode": "fout = open(fname_out, \"wb\")\nfout.write(struct.pack(\"i\", 0x67676d6c)) # magic: ggml in hex\nfout.write(struct.pack(\"i\", hparams[\"vocab_size\"]))\nfout.write(struct.pack(\"i\", hparams[\"max_source_positions\"]))\nfout.write(struct.pack(\"i\", hparams[\"d_model\"]))\nfout.write(struct.pack(\"i\", hparams[\"encoder_attention_heads\"]))\nfout.write(struct.pack(\"i\", hparams[\"encoder_layers\"]))\nfout.write(struct.pack(\"i\", hparams[\"max_length\"]))\nfout.write(struct.pack(\"i\", hparams[\"d_model\"]))\nfout.write(struct.pack(\"i\", hparams[\"decoder_attention_heads\"]))",
        "detail": "Demo.whisper.cpp.models.convert-h5-to-ggml",
        "documentation": {}
    },
    {
        "label": "byte_encoder",
        "kind": 5,
        "importPath": "Demo.whisper.cpp.models.convert-h5-to-ggml",
        "description": "Demo.whisper.cpp.models.convert-h5-to-ggml",
        "peekOfCode": "byte_encoder = bytes_to_unicode()\nbyte_decoder = {v:k for k, v in byte_encoder.items()}\nfout.write(struct.pack(\"i\", len(tokens)))\ntokens = sorted(tokens.items(), key=lambda x: x[1])\nfor key in tokens:\n    text = bytearray([byte_decoder[c] for c in key[0]])\n    fout.write(struct.pack(\"i\", len(text)))\n    fout.write(text)\nlist_vars = model.state_dict()\nfor name in list_vars.keys():",
        "detail": "Demo.whisper.cpp.models.convert-h5-to-ggml",
        "documentation": {}
    },
    {
        "label": "byte_decoder",
        "kind": 5,
        "importPath": "Demo.whisper.cpp.models.convert-h5-to-ggml",
        "description": "Demo.whisper.cpp.models.convert-h5-to-ggml",
        "peekOfCode": "byte_decoder = {v:k for k, v in byte_encoder.items()}\nfout.write(struct.pack(\"i\", len(tokens)))\ntokens = sorted(tokens.items(), key=lambda x: x[1])\nfor key in tokens:\n    text = bytearray([byte_decoder[c] for c in key[0]])\n    fout.write(struct.pack(\"i\", len(text)))\n    fout.write(text)\nlist_vars = model.state_dict()\nfor name in list_vars.keys():\n    # this seems to not be used",
        "detail": "Demo.whisper.cpp.models.convert-h5-to-ggml",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": "Demo.whisper.cpp.models.convert-h5-to-ggml",
        "description": "Demo.whisper.cpp.models.convert-h5-to-ggml",
        "peekOfCode": "tokens = sorted(tokens.items(), key=lambda x: x[1])\nfor key in tokens:\n    text = bytearray([byte_decoder[c] for c in key[0]])\n    fout.write(struct.pack(\"i\", len(text)))\n    fout.write(text)\nlist_vars = model.state_dict()\nfor name in list_vars.keys():\n    # this seems to not be used\n    # ref: https://github.com/huggingface/transformers/blob/9a5b84a0076a04fe9596da72e8668069d4f09ea0/src/transformers/models/whisper/modeling_whisper.py#L1099-L1106\n    if name == \"proj_out.weight\":",
        "detail": "Demo.whisper.cpp.models.convert-h5-to-ggml",
        "documentation": {}
    },
    {
        "label": "list_vars",
        "kind": 5,
        "importPath": "Demo.whisper.cpp.models.convert-h5-to-ggml",
        "description": "Demo.whisper.cpp.models.convert-h5-to-ggml",
        "peekOfCode": "list_vars = model.state_dict()\nfor name in list_vars.keys():\n    # this seems to not be used\n    # ref: https://github.com/huggingface/transformers/blob/9a5b84a0076a04fe9596da72e8668069d4f09ea0/src/transformers/models/whisper/modeling_whisper.py#L1099-L1106\n    if name == \"proj_out.weight\":\n        print('Skipping', name)\n        continue\n    src = name\n    nn = name\n    if name != \"proj_out.weight\":",
        "detail": "Demo.whisper.cpp.models.convert-h5-to-ggml",
        "documentation": {}
    },
    {
        "label": "bytes_to_unicode",
        "kind": 2,
        "importPath": "Demo.whisper.cpp.models.convert-pt-to-ggml",
        "description": "Demo.whisper.cpp.models.convert-pt-to-ggml",
        "peekOfCode": "def bytes_to_unicode():\n    \"\"\"\n    Returns list of utf-8 byte and a corresponding list of unicode strings.\n    The reversible bpe codes work on unicode strings.\n    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n    This is a signficant percentage of your normal, say, 32K bpe vocab.\n    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n    And avoids mapping to whitespace/control characters the bpe code barfs on.\n    \"\"\"",
        "detail": "Demo.whisper.cpp.models.convert-pt-to-ggml",
        "documentation": {}
    },
    {
        "label": "#LANGUAGES",
        "kind": 5,
        "importPath": "Demo.whisper.cpp.models.convert-pt-to-ggml",
        "description": "Demo.whisper.cpp.models.convert-pt-to-ggml",
        "peekOfCode": "#LANGUAGES = {\n#    \"en\": \"english\",\n#    \"zh\": \"chinese\",\n#    \"de\": \"german\",\n#    \"es\": \"spanish\",\n#    \"ru\": \"russian\",\n#    \"ko\": \"korean\",\n#    \"fr\": \"french\",\n#    \"ja\": \"japanese\",\n#    \"pt\": \"portuguese\",",
        "detail": "Demo.whisper.cpp.models.convert-pt-to-ggml",
        "documentation": {}
    },
    {
        "label": "dir_whisper",
        "kind": 5,
        "importPath": "Demo.whisper.cpp.models.convert-pt-to-ggml",
        "description": "Demo.whisper.cpp.models.convert-pt-to-ggml",
        "peekOfCode": "dir_whisper = Path(sys.argv[2])\ndir_out     = Path(sys.argv[3])\n# try to load PyTorch binary data\ntry:\n    model_bytes = open(fname_inp, \"rb\").read()\n    with io.BytesIO(model_bytes) as fp:\n        checkpoint = torch.load(fp, map_location=\"cpu\")\nexcept Exception:\n    print(\"Error: failed to load PyTorch model file:\" , fname_inp)\n    sys.exit(1)",
        "detail": "Demo.whisper.cpp.models.convert-pt-to-ggml",
        "documentation": {}
    },
    {
        "label": "hparams",
        "kind": 5,
        "importPath": "Demo.whisper.cpp.models.convert-pt-to-ggml",
        "description": "Demo.whisper.cpp.models.convert-pt-to-ggml",
        "peekOfCode": "hparams = checkpoint[\"dims\"]\nprint(\"hparams:\", hparams)\nlist_vars = checkpoint[\"model_state_dict\"]\n#print(list_vars['encoder.positional_embedding'])\n#print(list_vars['encoder.conv1.weight'])\n#print(list_vars['encoder.conv1.weight'].shape)\n# load mel filters\nn_mels = hparams[\"n_mels\"]\nwith np.load(dir_whisper / \"whisper\" / \"assets\" / \"mel_filters.npz\") as f:\n    filters = torch.from_numpy(f[f\"mel_{n_mels}\"])",
        "detail": "Demo.whisper.cpp.models.convert-pt-to-ggml",
        "documentation": {}
    },
    {
        "label": "list_vars",
        "kind": 5,
        "importPath": "Demo.whisper.cpp.models.convert-pt-to-ggml",
        "description": "Demo.whisper.cpp.models.convert-pt-to-ggml",
        "peekOfCode": "list_vars = checkpoint[\"model_state_dict\"]\n#print(list_vars['encoder.positional_embedding'])\n#print(list_vars['encoder.conv1.weight'])\n#print(list_vars['encoder.conv1.weight'].shape)\n# load mel filters\nn_mels = hparams[\"n_mels\"]\nwith np.load(dir_whisper / \"whisper\" / \"assets\" / \"mel_filters.npz\") as f:\n    filters = torch.from_numpy(f[f\"mel_{n_mels}\"])\n    #print (filters)\n#code.interact(local=locals())",
        "detail": "Demo.whisper.cpp.models.convert-pt-to-ggml",
        "documentation": {}
    },
    {
        "label": "n_mels",
        "kind": 5,
        "importPath": "Demo.whisper.cpp.models.convert-pt-to-ggml",
        "description": "Demo.whisper.cpp.models.convert-pt-to-ggml",
        "peekOfCode": "n_mels = hparams[\"n_mels\"]\nwith np.load(dir_whisper / \"whisper\" / \"assets\" / \"mel_filters.npz\") as f:\n    filters = torch.from_numpy(f[f\"mel_{n_mels}\"])\n    #print (filters)\n#code.interact(local=locals())\n# load tokenizer\n# for backwards compatibility, also check for older hf_transformers format tokenizer files\n# old format: dir_whisper/whisper/assets/[multilingual/gpt2]/vocab.json\n# new format: dir_whisper/whisper/assets/[multilingual/gpt2].tiktoken\nmultilingual = hparams[\"n_vocab\"] == 51865",
        "detail": "Demo.whisper.cpp.models.convert-pt-to-ggml",
        "documentation": {}
    },
    {
        "label": "multilingual",
        "kind": 5,
        "importPath": "Demo.whisper.cpp.models.convert-pt-to-ggml",
        "description": "Demo.whisper.cpp.models.convert-pt-to-ggml",
        "peekOfCode": "multilingual = hparams[\"n_vocab\"] == 51865\ntokenizer = dir_whisper / \"whisper\" / \"assets\" / (multilingual and \"multilingual.tiktoken\" or \"gpt2.tiktoken\")\ntokenizer_type = \"tiktoken\"\nif not tokenizer.is_file():\n    tokenizer = dir_whisper / \"whisper\" / \"assets\" / (multilingual and \"multilingual\" or \"gpt2\") / \"vocab.json\"\n    tokenizer_type = \"hf_transformers\"\n    if not tokenizer.is_file():\n        print(\"Error: failed to find either tiktoken or hf_transformers tokenizer file:\", tokenizer)\n        sys.exit(1)\nbyte_encoder = bytes_to_unicode()",
        "detail": "Demo.whisper.cpp.models.convert-pt-to-ggml",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "Demo.whisper.cpp.models.convert-pt-to-ggml",
        "description": "Demo.whisper.cpp.models.convert-pt-to-ggml",
        "peekOfCode": "tokenizer = dir_whisper / \"whisper\" / \"assets\" / (multilingual and \"multilingual.tiktoken\" or \"gpt2.tiktoken\")\ntokenizer_type = \"tiktoken\"\nif not tokenizer.is_file():\n    tokenizer = dir_whisper / \"whisper\" / \"assets\" / (multilingual and \"multilingual\" or \"gpt2\") / \"vocab.json\"\n    tokenizer_type = \"hf_transformers\"\n    if not tokenizer.is_file():\n        print(\"Error: failed to find either tiktoken or hf_transformers tokenizer file:\", tokenizer)\n        sys.exit(1)\nbyte_encoder = bytes_to_unicode()\nbyte_decoder = {v:k for k, v in byte_encoder.items()}",
        "detail": "Demo.whisper.cpp.models.convert-pt-to-ggml",
        "documentation": {}
    },
    {
        "label": "tokenizer_type",
        "kind": 5,
        "importPath": "Demo.whisper.cpp.models.convert-pt-to-ggml",
        "description": "Demo.whisper.cpp.models.convert-pt-to-ggml",
        "peekOfCode": "tokenizer_type = \"tiktoken\"\nif not tokenizer.is_file():\n    tokenizer = dir_whisper / \"whisper\" / \"assets\" / (multilingual and \"multilingual\" or \"gpt2\") / \"vocab.json\"\n    tokenizer_type = \"hf_transformers\"\n    if not tokenizer.is_file():\n        print(\"Error: failed to find either tiktoken or hf_transformers tokenizer file:\", tokenizer)\n        sys.exit(1)\nbyte_encoder = bytes_to_unicode()\nbyte_decoder = {v:k for k, v in byte_encoder.items()}\nif tokenizer_type == \"tiktoken\":",
        "detail": "Demo.whisper.cpp.models.convert-pt-to-ggml",
        "documentation": {}
    },
    {
        "label": "byte_encoder",
        "kind": 5,
        "importPath": "Demo.whisper.cpp.models.convert-pt-to-ggml",
        "description": "Demo.whisper.cpp.models.convert-pt-to-ggml",
        "peekOfCode": "byte_encoder = bytes_to_unicode()\nbyte_decoder = {v:k for k, v in byte_encoder.items()}\nif tokenizer_type == \"tiktoken\":\n    with open(tokenizer, \"rb\") as f:\n        contents = f.read()\n        tokens = {base64.b64decode(token): int(rank) for token, rank in (line.split() for line in contents.splitlines() if line)}\nelif tokenizer_type == \"hf_transformers\":\n    with open(tokenizer, \"r\", encoding=\"utf8\") as f:\n        _tokens_raw = json.load(f)\n        if '<|endoftext|>' in _tokens_raw:",
        "detail": "Demo.whisper.cpp.models.convert-pt-to-ggml",
        "documentation": {}
    },
    {
        "label": "byte_decoder",
        "kind": 5,
        "importPath": "Demo.whisper.cpp.models.convert-pt-to-ggml",
        "description": "Demo.whisper.cpp.models.convert-pt-to-ggml",
        "peekOfCode": "byte_decoder = {v:k for k, v in byte_encoder.items()}\nif tokenizer_type == \"tiktoken\":\n    with open(tokenizer, \"rb\") as f:\n        contents = f.read()\n        tokens = {base64.b64decode(token): int(rank) for token, rank in (line.split() for line in contents.splitlines() if line)}\nelif tokenizer_type == \"hf_transformers\":\n    with open(tokenizer, \"r\", encoding=\"utf8\") as f:\n        _tokens_raw = json.load(f)\n        if '<|endoftext|>' in _tokens_raw:\n            # ensures exact same model as tokenizer_type == tiktoken",
        "detail": "Demo.whisper.cpp.models.convert-pt-to-ggml",
        "documentation": {}
    },
    {
        "label": "fname_out",
        "kind": 5,
        "importPath": "Demo.whisper.cpp.models.convert-pt-to-ggml",
        "description": "Demo.whisper.cpp.models.convert-pt-to-ggml",
        "peekOfCode": "fname_out = dir_out / \"ggml-model.bin\"\n# use 16-bit or 32-bit floats\nuse_f16 = True\nif len(sys.argv) > 4:\n    use_f16 = False\n    fname_out = dir_out / \"ggml-model-f32.bin\"\nfout = fname_out.open(\"wb\")\nfout.write(struct.pack(\"i\", 0x67676d6c)) # magic: ggml in hex\nfout.write(struct.pack(\"i\", hparams[\"n_vocab\"]))\nfout.write(struct.pack(\"i\", hparams[\"n_audio_ctx\"]))",
        "detail": "Demo.whisper.cpp.models.convert-pt-to-ggml",
        "documentation": {}
    },
    {
        "label": "use_f16",
        "kind": 5,
        "importPath": "Demo.whisper.cpp.models.convert-pt-to-ggml",
        "description": "Demo.whisper.cpp.models.convert-pt-to-ggml",
        "peekOfCode": "use_f16 = True\nif len(sys.argv) > 4:\n    use_f16 = False\n    fname_out = dir_out / \"ggml-model-f32.bin\"\nfout = fname_out.open(\"wb\")\nfout.write(struct.pack(\"i\", 0x67676d6c)) # magic: ggml in hex\nfout.write(struct.pack(\"i\", hparams[\"n_vocab\"]))\nfout.write(struct.pack(\"i\", hparams[\"n_audio_ctx\"]))\nfout.write(struct.pack(\"i\", hparams[\"n_audio_state\"]))\nfout.write(struct.pack(\"i\", hparams[\"n_audio_head\"]))",
        "detail": "Demo.whisper.cpp.models.convert-pt-to-ggml",
        "documentation": {}
    },
    {
        "label": "fout",
        "kind": 5,
        "importPath": "Demo.whisper.cpp.models.convert-pt-to-ggml",
        "description": "Demo.whisper.cpp.models.convert-pt-to-ggml",
        "peekOfCode": "fout = fname_out.open(\"wb\")\nfout.write(struct.pack(\"i\", 0x67676d6c)) # magic: ggml in hex\nfout.write(struct.pack(\"i\", hparams[\"n_vocab\"]))\nfout.write(struct.pack(\"i\", hparams[\"n_audio_ctx\"]))\nfout.write(struct.pack(\"i\", hparams[\"n_audio_state\"]))\nfout.write(struct.pack(\"i\", hparams[\"n_audio_head\"]))\nfout.write(struct.pack(\"i\", hparams[\"n_audio_layer\"]))\nfout.write(struct.pack(\"i\", hparams[\"n_text_ctx\"]))\nfout.write(struct.pack(\"i\", hparams[\"n_text_state\"]))\nfout.write(struct.pack(\"i\", hparams[\"n_text_head\"]))",
        "detail": "Demo.whisper.cpp.models.convert-pt-to-ggml",
        "documentation": {}
    },
    {
        "label": "LayerNormANE",
        "kind": 6,
        "importPath": "Demo.whisper.cpp.models.convert-whisper-to-coreml",
        "description": "Demo.whisper.cpp.models.convert-whisper-to-coreml",
        "peekOfCode": "class LayerNormANE(LayerNormANEBase):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._register_load_state_dict_pre_hook(\n            correct_for_bias_scale_order_inversion)\nclass MultiHeadAttentionANE(MultiHeadAttention):\n    def __init__(self, n_state: int, n_head: int):\n        super().__init__(n_state, n_head)\n        self.query =  nn.Conv2d(n_state, n_state, kernel_size=1)\n        self.key = nn.Conv2d(n_state, n_state, kernel_size=1, bias=False)",
        "detail": "Demo.whisper.cpp.models.convert-whisper-to-coreml",
        "documentation": {}
    },
    {
        "label": "MultiHeadAttentionANE",
        "kind": 6,
        "importPath": "Demo.whisper.cpp.models.convert-whisper-to-coreml",
        "description": "Demo.whisper.cpp.models.convert-whisper-to-coreml",
        "peekOfCode": "class MultiHeadAttentionANE(MultiHeadAttention):\n    def __init__(self, n_state: int, n_head: int):\n        super().__init__(n_state, n_head)\n        self.query =  nn.Conv2d(n_state, n_state, kernel_size=1)\n        self.key = nn.Conv2d(n_state, n_state, kernel_size=1, bias=False)\n        self.value = nn.Conv2d(n_state, n_state, kernel_size=1)\n        self.out = nn.Conv2d(n_state, n_state, kernel_size=1)\n    def forward(self,\n                x: Tensor,\n                xa: Optional[Tensor] = None,",
        "detail": "Demo.whisper.cpp.models.convert-whisper-to-coreml",
        "documentation": {}
    },
    {
        "label": "ResidualAttentionBlockANE",
        "kind": 6,
        "importPath": "Demo.whisper.cpp.models.convert-whisper-to-coreml",
        "description": "Demo.whisper.cpp.models.convert-whisper-to-coreml",
        "peekOfCode": "class ResidualAttentionBlockANE(ResidualAttentionBlock):\n    def __init__(self, n_state: int, n_head: int, cross_attention: bool = False):\n        super().__init__(n_state, n_head, cross_attention)\n        self.attn =  MultiHeadAttentionANE(n_state, n_head)\n        self.attn_ln = LayerNormANE(n_state)\n        self.cross_attn =  MultiHeadAttentionANE(n_state, n_head) if cross_attention else None\n        self.cross_attn_ln =  LayerNormANE(n_state) if cross_attention else None\n        n_mlp = n_state * 4\n        self.mlp =  nn.Sequential(\n            nn.Conv2d(n_state, n_mlp, kernel_size=1),",
        "detail": "Demo.whisper.cpp.models.convert-whisper-to-coreml",
        "documentation": {}
    },
    {
        "label": "AudioEncoderANE",
        "kind": 6,
        "importPath": "Demo.whisper.cpp.models.convert-whisper-to-coreml",
        "description": "Demo.whisper.cpp.models.convert-whisper-to-coreml",
        "peekOfCode": "class AudioEncoderANE(AudioEncoder):\n    def __init__(self, n_mels: int, n_ctx: int, n_state: int, n_head: int, n_layer: int):\n        super().__init__(n_mels, n_ctx, n_state, n_head, n_layer)\n        self.blocks = nn.ModuleList(\n            [ResidualAttentionBlockANE(n_state, n_head) for _ in range(n_layer)]\n        )\n        self.ln_post = LayerNormANE(n_state)\n    def forward(self, x: Tensor):\n        \"\"\"\n        x : torch.Tensor, shape = (batch_size, n_mels, n_ctx)",
        "detail": "Demo.whisper.cpp.models.convert-whisper-to-coreml",
        "documentation": {}
    },
    {
        "label": "TextDecoderANE",
        "kind": 6,
        "importPath": "Demo.whisper.cpp.models.convert-whisper-to-coreml",
        "description": "Demo.whisper.cpp.models.convert-whisper-to-coreml",
        "peekOfCode": "class TextDecoderANE(TextDecoder):\n    def __init__(self, n_vocab: int, n_ctx: int, n_state: int, n_head: int, n_layer: int):\n        super().__init__(n_vocab, n_ctx, n_state, n_head, n_layer)\n        self.blocks= nn.ModuleList(\n            [ResidualAttentionBlockANE(n_state, n_head, cross_attention=True) for _ in range(n_layer)]\n        )\n        self.ln= LayerNormANE(n_state)\n    def forward(self, x: Tensor, xa: Tensor, kv_cache: Optional[dict] = None):\n        \"\"\"\n        x : torch.LongTensor, shape = (batch_size, <= n_ctx)",
        "detail": "Demo.whisper.cpp.models.convert-whisper-to-coreml",
        "documentation": {}
    },
    {
        "label": "WhisperANE",
        "kind": 6,
        "importPath": "Demo.whisper.cpp.models.convert-whisper-to-coreml",
        "description": "Demo.whisper.cpp.models.convert-whisper-to-coreml",
        "peekOfCode": "class WhisperANE(Whisper):\n    def __init__(self, dims: ModelDimensions):\n        super().__init__(dims)\n        self.encoder = AudioEncoderANE(\n            self.dims.n_mels,\n            self.dims.n_audio_ctx,\n            self.dims.n_audio_state,\n            self.dims.n_audio_head,\n            self.dims.n_audio_layer,\n        )",
        "detail": "Demo.whisper.cpp.models.convert-whisper-to-coreml",
        "documentation": {}
    },
    {
        "label": "linear_to_conv2d_map",
        "kind": 2,
        "importPath": "Demo.whisper.cpp.models.convert-whisper-to-coreml",
        "description": "Demo.whisper.cpp.models.convert-whisper-to-coreml",
        "peekOfCode": "def linear_to_conv2d_map(state_dict, prefix, local_metadata, strict,\n                         missing_keys, unexpected_keys, error_msgs):\n    \"\"\"\n    Unsqueeze twice to map nn.Linear weights to nn.Conv2d weights\n    \"\"\"\n    for k in state_dict:\n        is_attention = all(substr in k for substr in ['attn', '.weight'])\n        is_mlp = any(k.endswith(s) for s in ['mlp.0.weight', 'mlp.2.weight'])\n        if (is_attention or is_mlp) and len(state_dict[k].shape) == 2:\n            state_dict[k] = state_dict[k][:, :, None, None]",
        "detail": "Demo.whisper.cpp.models.convert-whisper-to-coreml",
        "documentation": {}
    },
    {
        "label": "correct_for_bias_scale_order_inversion",
        "kind": 2,
        "importPath": "Demo.whisper.cpp.models.convert-whisper-to-coreml",
        "description": "Demo.whisper.cpp.models.convert-whisper-to-coreml",
        "peekOfCode": "def correct_for_bias_scale_order_inversion(state_dict, prefix, local_metadata,\n                                           strict, missing_keys,\n                                           unexpected_keys, error_msgs):\n    state_dict[prefix + 'bias'] = state_dict[prefix + 'bias'] / state_dict[prefix + 'weight']\n    return state_dict\nclass LayerNormANE(LayerNormANEBase):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._register_load_state_dict_pre_hook(\n            correct_for_bias_scale_order_inversion)",
        "detail": "Demo.whisper.cpp.models.convert-whisper-to-coreml",
        "documentation": {}
    },
    {
        "label": "convert_encoder",
        "kind": 2,
        "importPath": "Demo.whisper.cpp.models.convert-whisper-to-coreml",
        "description": "Demo.whisper.cpp.models.convert-whisper-to-coreml",
        "peekOfCode": "def convert_encoder(hparams, model, quantize=False):\n    model.eval()\n    input_shape = (1, 80, 3000)\n    input_data = torch.randn(input_shape)\n    traced_model = torch.jit.trace(model, input_data)\n    model = ct.convert(\n        traced_model,\n        convert_to=None if quantize else \"mlprogram\", # convert will fail if weights are quantized, not sure why\n        inputs=[ct.TensorType(name=\"logmel_data\", shape=input_shape)],\n        outputs=[ct.TensorType(name=\"output\")],",
        "detail": "Demo.whisper.cpp.models.convert-whisper-to-coreml",
        "documentation": {}
    },
    {
        "label": "convert_decoder",
        "kind": 2,
        "importPath": "Demo.whisper.cpp.models.convert-whisper-to-coreml",
        "description": "Demo.whisper.cpp.models.convert-whisper-to-coreml",
        "peekOfCode": "def convert_decoder(hparams, model, quantize=False):\n    model.eval()\n    tokens_shape = (1, 1)\n    audio_shape = (1, hparams.n_audio_state, 1, 1500)\n    audio_data = torch.randn(audio_shape)\n    token_data = torch.randint(50257, tokens_shape).long()\n    traced_model = torch.jit.trace(model, (token_data, audio_data))\n    model = ct.convert(\n        traced_model,\n        convert_to=None if quantize else \"mlprogram\", # convert will fail if weights are quantized, not sure why",
        "detail": "Demo.whisper.cpp.models.convert-whisper-to-coreml",
        "documentation": {}
    },
    {
        "label": "convert_encoder",
        "kind": 2,
        "importPath": "Demo.whisper.cpp.models.convert-whisper-to-openvino",
        "description": "Demo.whisper.cpp.models.convert-whisper-to-openvino",
        "peekOfCode": "def convert_encoder(hparams, encoder, mname):\n    encoder.eval()\n    mel = torch.zeros((1, 80, 3000))\n    onnx_folder=os.path.join(os.path.dirname(__file__),\"onnx_encoder\")\n    #create a directory to store the onnx model, and other collateral that is saved during onnx export procedure\n    if not os.path.isdir(onnx_folder):\n        os.makedirs(onnx_folder)\n    onnx_path = os.path.join(onnx_folder, \"whisper_encoder.onnx\")\n    torch.onnx.export(\n        encoder,",
        "detail": "Demo.whisper.cpp.models.convert-whisper-to-openvino",
        "documentation": {}
    },
    {
        "label": "fname_inp",
        "kind": 5,
        "importPath": "Demo.whisper.cpp.models.ggml_to_pt",
        "description": "Demo.whisper.cpp.models.ggml_to_pt",
        "peekOfCode": "fname_inp = Path(sys.argv[1])\ndir_out = Path(sys.argv[2])\nfname_out = dir_out / \"torch-model.pt\"\n# Open the ggml file\nwith open(fname_inp, \"rb\") as f:\n    # Read magic number and hyperparameters\n    magic_number, n_vocab, n_audio_ctx, n_audio_state, n_audio_head, n_audio_layer, n_text_ctx, n_text_state, n_text_head, n_text_layer, n_mels, use_f16 = struct.unpack(\"12i\", f.read(48))\n    print(f\"Magic number: {magic_number}\")\n    print(f\"Vocab size: {n_vocab}\")\n    print(f\"Audio context size: {n_audio_ctx}\")",
        "detail": "Demo.whisper.cpp.models.ggml_to_pt",
        "documentation": {}
    },
    {
        "label": "dir_out",
        "kind": 5,
        "importPath": "Demo.whisper.cpp.models.ggml_to_pt",
        "description": "Demo.whisper.cpp.models.ggml_to_pt",
        "peekOfCode": "dir_out = Path(sys.argv[2])\nfname_out = dir_out / \"torch-model.pt\"\n# Open the ggml file\nwith open(fname_inp, \"rb\") as f:\n    # Read magic number and hyperparameters\n    magic_number, n_vocab, n_audio_ctx, n_audio_state, n_audio_head, n_audio_layer, n_text_ctx, n_text_state, n_text_head, n_text_layer, n_mels, use_f16 = struct.unpack(\"12i\", f.read(48))\n    print(f\"Magic number: {magic_number}\")\n    print(f\"Vocab size: {n_vocab}\")\n    print(f\"Audio context size: {n_audio_ctx}\")\n    print(f\"Audio state size: {n_audio_state}\")",
        "detail": "Demo.whisper.cpp.models.ggml_to_pt",
        "documentation": {}
    },
    {
        "label": "fname_out",
        "kind": 5,
        "importPath": "Demo.whisper.cpp.models.ggml_to_pt",
        "description": "Demo.whisper.cpp.models.ggml_to_pt",
        "peekOfCode": "fname_out = dir_out / \"torch-model.pt\"\n# Open the ggml file\nwith open(fname_inp, \"rb\") as f:\n    # Read magic number and hyperparameters\n    magic_number, n_vocab, n_audio_ctx, n_audio_state, n_audio_head, n_audio_layer, n_text_ctx, n_text_state, n_text_head, n_text_layer, n_mels, use_f16 = struct.unpack(\"12i\", f.read(48))\n    print(f\"Magic number: {magic_number}\")\n    print(f\"Vocab size: {n_vocab}\")\n    print(f\"Audio context size: {n_audio_ctx}\")\n    print(f\"Audio state size: {n_audio_state}\")\n    print(f\"Audio head size: {n_audio_head}\")",
        "detail": "Demo.whisper.cpp.models.ggml_to_pt",
        "documentation": {}
    },
    {
        "label": "dims",
        "kind": 5,
        "importPath": "Demo.whisper.cpp.models.ggml_to_pt",
        "description": "Demo.whisper.cpp.models.ggml_to_pt",
        "peekOfCode": "dims = ModelDimensions(\n    n_mels=n_mels,\n    n_audio_ctx=n_audio_ctx,\n    n_audio_state=n_audio_state,\n    n_audio_head=n_audio_head,\n    n_audio_layer=n_audio_layer,\n    n_text_ctx=n_text_ctx,\n    n_text_state=n_text_state,\n    n_text_head=n_text_head,\n    n_text_layer=n_text_layer,",
        "detail": "Demo.whisper.cpp.models.ggml_to_pt",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "Demo.whisper.cpp.models.ggml_to_pt",
        "description": "Demo.whisper.cpp.models.ggml_to_pt",
        "peekOfCode": "model = Whisper(dims)  # Replace with your model's class\nmodel.load_state_dict(model_state_dict)\n# Save the model in PyTorch format\ntorch.save(model.state_dict(), fname_out)",
        "detail": "Demo.whisper.cpp.models.ggml_to_pt",
        "documentation": {}
    },
    {
        "label": "Authentication",
        "kind": 6,
        "importPath": "Demo.Authentication",
        "description": "Demo.Authentication",
        "peekOfCode": "class Authentication:\n   #def __init__(self, username,password):\n   def __init__(self, apikey):\n    #self.username=username\n    #self.password=password\n    self.apikey=apikey\n    self.service=\"http://umlsks.nlm.nih.gov\"\n   def gettgt(self):\n     #params = {'username': self.username,'password': self.password}\n     params = {'apikey': self.apikey}",
        "detail": "Demo.Authentication",
        "documentation": {}
    },
    {
        "label": "#auth_endpoint",
        "kind": 5,
        "importPath": "Demo.Authentication",
        "description": "Demo.Authentication",
        "peekOfCode": "#auth_endpoint = \"/cas/v1/tickets/\"\n#option 2 - api key authentication at /cas/v1/api-key\nauth_endpoint = \"/cas/v1/api-key\"\nclass Authentication:\n   #def __init__(self, username,password):\n   def __init__(self, apikey):\n    #self.username=username\n    #self.password=password\n    self.apikey=apikey\n    self.service=\"http://umlsks.nlm.nih.gov\"",
        "detail": "Demo.Authentication",
        "documentation": {}
    },
    {
        "label": "auth_endpoint",
        "kind": 5,
        "importPath": "Demo.Authentication",
        "description": "Demo.Authentication",
        "peekOfCode": "auth_endpoint = \"/cas/v1/api-key\"\nclass Authentication:\n   #def __init__(self, username,password):\n   def __init__(self, apikey):\n    #self.username=username\n    #self.password=password\n    self.apikey=apikey\n    self.service=\"http://umlsks.nlm.nih.gov\"\n   def gettgt(self):\n     #params = {'username': self.username,'password': self.password}",
        "detail": "Demo.Authentication",
        "documentation": {}
    },
    {
        "label": "FeedbackObj",
        "kind": 6,
        "importPath": "Demo.CognitiveSystem",
        "description": "Demo.CognitiveSystem",
        "peekOfCode": "class FeedbackObj:\n    def __init__(self, intervention, protocol, concept):\n        super(FeedbackObj, self).__init__()\n        self.intervention = intervention\n        self.protocol = protocol\n        self.concept = concept\n# ------------ End Feedback Obj Class ------------\n# Cognitive System Thread\ndef CognitiveSystem(Window, SpeechToNLPQueue, FeedbackQueue, data_path_str, conceptBool, interventionBool):\n    # Create GUI signal objects",
        "detail": "Demo.CognitiveSystem",
        "documentation": {}
    },
    {
        "label": "CognitiveSystem",
        "kind": 2,
        "importPath": "Demo.CognitiveSystem",
        "description": "Demo.CognitiveSystem",
        "peekOfCode": "def CognitiveSystem(Window, SpeechToNLPQueue, FeedbackQueue, data_path_str, conceptBool, interventionBool):\n    # Create GUI signal objects\n    SpeechSignal = GUISignal()\n    SpeechSignal.signal.connect(Window.UpdateSpeechBox)\n    ConceptExtractionSignal = GUISignal()\n    ConceptExtractionSignal.signal.connect(Window.UpdateConceptExtractionBox)\n    # Initialize BT framework parameters\n    exec(open(\"./bt_parameters.py\").read())\n    # Setup BT Framework\n    #blackboard = Blackboard()",
        "detail": "Demo.CognitiveSystem",
        "documentation": {}
    },
    {
        "label": "TickResults",
        "kind": 2,
        "importPath": "Demo.CognitiveSystem",
        "description": "Demo.CognitiveSystem",
        "peekOfCode": "def TickResults(Window, NLP_Items, data_path_str, conceptBool, interventionBool, FeedbackQueue):\n    Concepts_Graph = dict()\n    # print(NLP_Items)\n    if conceptBool == True:\n        if not os.path.exists(data_path_str + \"conceptextractiondata/\"):\n            os.makedirs(data_path_str+\"conceptextractiondata/\")\n        CE_outputfile = open(data_path_str +\"conceptextractiondata/\"+ \"cedata.txt\", 'w')\n    if interventionBool == True:\n        if not os.path.exists(data_path_str + \"interventiondata/\"):\n            os.makedirs(data_path_str+\"interventiondata/\")",
        "detail": "Demo.CognitiveSystem",
        "documentation": {}
    },
    {
        "label": "pre_tick_handler",
        "kind": 2,
        "importPath": "Demo.CognitiveSystem",
        "description": "Demo.CognitiveSystem",
        "peekOfCode": "def pre_tick_handler(behaviour_tree):\n    #blackboard = Blackboard()\n    global blackboard\n    blackboard.tick_num += 1",
        "detail": "Demo.CognitiveSystem",
        "documentation": {}
    },
    {
        "label": "blackboard.tick_num",
        "kind": 5,
        "importPath": "Demo.CognitiveSystem",
        "description": "Demo.CognitiveSystem",
        "peekOfCode": "blackboard.tick_num = 0\n# ------------ For Feedback ------------\nclass FeedbackObj:\n    def __init__(self, intervention, protocol, concept):\n        super(FeedbackObj, self).__init__()\n        self.intervention = intervention\n        self.protocol = protocol\n        self.concept = concept\n# ------------ End Feedback Obj Class ------------\n# Cognitive System Thread",
        "detail": "Demo.CognitiveSystem",
        "documentation": {}
    },
    {
        "label": "PatientStatus",
        "kind": 6,
        "importPath": "Demo.ConceptExtract",
        "description": "Demo.ConceptExtract",
        "peekOfCode": "class PatientStatus(object):\n    def __init__(self, name, binary, value='', content=''):\n        self.name = name\n        self.binary = binary\n        self.value = value\n        self.content = content\n        self.tick = 0\n        self.score = 0\nclass ConceptExtractor(object):\n    def __init__(self, List_route):",
        "detail": "Demo.ConceptExtract",
        "documentation": {}
    },
    {
        "label": "ConceptExtractor",
        "kind": 6,
        "importPath": "Demo.ConceptExtract",
        "description": "Demo.ConceptExtract",
        "peekOfCode": "class ConceptExtractor(object):\n    def __init__(self, List_route):\n        '''\n        list_route: route of the list file\n        extend list: a .csv file, two colum: required concepts and its cuis.\n        status: a dict, keys are concepts, values are corresponding informations.\n                Indicates the default status of the patient.\n        self.CUIs: list of the requied CUIs\n        self.CUI2Concept: mapping the CUIs to the concepts\n        self.Status: dict to store the information",
        "detail": "Demo.ConceptExtract",
        "documentation": {}
    },
    {
        "label": "CEWithoutMM",
        "kind": 6,
        "importPath": "Demo.ConceptExtract",
        "description": "Demo.ConceptExtract",
        "peekOfCode": "class CEWithoutMM(object):\n    def __init__(self, SS_List_route, Int_List_route, AllinOne_SS=None,\n                 AllinOne_Int=None, neg_res=None, WDistance=False, aio_only=False):\n        '''\n        build mapping w/without score\n        store neg res\n        '''\n        # stop words\n        stop_words = set(stopwords.words('english'))\n        stop_words.update(['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine',",
        "detail": "Demo.ConceptExtract",
        "documentation": {}
    },
    {
        "label": "get_bp",
        "kind": 2,
        "importPath": "Demo.ConceptExtract",
        "description": "Demo.ConceptExtract",
        "peekOfCode": "def get_bp(scores, text):\n    if not \"C1271104\" in scores:\n        check = text[0].find(\"blood pressure\")\n        if not check == -1:\n            bp_object = ['00000000', 'MMI', '1', 'bp', 'C1271104', 'x',\n                         '[\"bp-tx-1-\"spo2\"-noun-0]', 'TX', str(check) + '/14', '']\n            return True, bp_object\n        return False, []\n    return False, []\ndef get_sp(scores, text):",
        "detail": "Demo.ConceptExtract",
        "documentation": {}
    },
    {
        "label": "get_sp",
        "kind": 2,
        "importPath": "Demo.ConceptExtract",
        "description": "Demo.ConceptExtract",
        "peekOfCode": "def get_sp(scores, text):\n    if not \"C0428179\" in scores:\n        check = text[0].find(\"spo2\")\n        if not check == -1:\n            sp_object = ['00000000', 'MMI', '1', 'spo2', 'C0428179', 'x',\n                         '[\"spo2-tx-1-\"spo2\"-noun-0]', 'TX', str(check) + '/4', '']\n            return True, sp_object\n        return False, []\n    return False, []\nCUI_pulse = \"C0391850\"",
        "detail": "Demo.ConceptExtract",
        "documentation": {}
    },
    {
        "label": "check_pulse",
        "kind": 2,
        "importPath": "Demo.ConceptExtract",
        "description": "Demo.ConceptExtract",
        "peekOfCode": "def check_pulse(concepts, scores, text):\n    if not CUI_pulse in scores:  # if no pulses found\n        # print(\"Check initial\")\n        return -1, 0\n    if not CUI_ox in scores and not CUI_oximetry in scores:  # pulse but no spo2\n        # print(\"Check A\")\n        return -1, 0\n    text = text[0]\n    regex = re.compile(r'([Pp]ulse),? (ox)?')\n    results = regex.finditer(text)",
        "detail": "Demo.ConceptExtract",
        "documentation": {}
    },
    {
        "label": "CUI_pulse",
        "kind": 5,
        "importPath": "Demo.ConceptExtract",
        "description": "Demo.ConceptExtract",
        "peekOfCode": "CUI_pulse = \"C0391850\"\nCUI_ox = \"C0300971\"\nCUI_oximetry = \"C0523807\"\ndef check_pulse(concepts, scores, text):\n    if not CUI_pulse in scores:  # if no pulses found\n        # print(\"Check initial\")\n        return -1, 0\n    if not CUI_ox in scores and not CUI_oximetry in scores:  # pulse but no spo2\n        # print(\"Check A\")\n        return -1, 0",
        "detail": "Demo.ConceptExtract",
        "documentation": {}
    },
    {
        "label": "CUI_ox",
        "kind": 5,
        "importPath": "Demo.ConceptExtract",
        "description": "Demo.ConceptExtract",
        "peekOfCode": "CUI_ox = \"C0300971\"\nCUI_oximetry = \"C0523807\"\ndef check_pulse(concepts, scores, text):\n    if not CUI_pulse in scores:  # if no pulses found\n        # print(\"Check initial\")\n        return -1, 0\n    if not CUI_ox in scores and not CUI_oximetry in scores:  # pulse but no spo2\n        # print(\"Check A\")\n        return -1, 0\n    text = text[0]",
        "detail": "Demo.ConceptExtract",
        "documentation": {}
    },
    {
        "label": "CUI_oximetry",
        "kind": 5,
        "importPath": "Demo.ConceptExtract",
        "description": "Demo.ConceptExtract",
        "peekOfCode": "CUI_oximetry = \"C0523807\"\ndef check_pulse(concepts, scores, text):\n    if not CUI_pulse in scores:  # if no pulses found\n        # print(\"Check initial\")\n        return -1, 0\n    if not CUI_ox in scores and not CUI_oximetry in scores:  # pulse but no spo2\n        # print(\"Check A\")\n        return -1, 0\n    text = text[0]\n    regex = re.compile(r'([Pp]ulse),? (ox)?')",
        "detail": "Demo.ConceptExtract",
        "documentation": {}
    },
    {
        "label": "VersionAction",
        "kind": 6,
        "importPath": "Demo.DeepSpeechFileStream",
        "description": "Demo.DeepSpeechFileStream",
        "peekOfCode": "class VersionAction(argparse.Action):\n    def __init__(self, *args, **kwargs):\n        super(VersionAction, self).__init__(nargs=0, *args, **kwargs)\n    def __call__(self, *args, **kwargs):\n        printVersions()\n        exit(0)\n# These constants control the beam search decoder\n# Beam width used in the CTC decoder when building candidate transcriptions\nBEAM_WIDTH = 500\n# The alpha hyperparameter of the CTC decoder. Language Model weight",
        "detail": "Demo.DeepSpeechFileStream",
        "documentation": {}
    },
    {
        "label": "convert_samplerate",
        "kind": 2,
        "importPath": "Demo.DeepSpeechFileStream",
        "description": "Demo.DeepSpeechFileStream",
        "peekOfCode": "def convert_samplerate(audio_path):\n    sox_cmd = 'sox {} --type raw --bits 16 --channels 1 --rate 16000 --encoding signed-integer --endian little --compression 0.0 --no-dither - '.format(quote(audio_path))\n    try:\n        output = subprocess.check_output(shlex.split(sox_cmd), stderr=subprocess.PIPE)\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError('SoX returned non-zero status: {}'.format(e.stderr))\n    except OSError as e:\n        raise OSError(e.errno, 'SoX not found, use 16kHz files or install it: {}'.format(e.strerror))\n    return 16000, np.frombuffer(output, dtype=np.int16)\ndef metadata_to_string(metadata):",
        "detail": "Demo.DeepSpeechFileStream",
        "documentation": {}
    },
    {
        "label": "metadata_to_string",
        "kind": 2,
        "importPath": "Demo.DeepSpeechFileStream",
        "description": "Demo.DeepSpeechFileStream",
        "peekOfCode": "def metadata_to_string(metadata):\n    return ''.join(item.character for item in metadata.items)\nclass VersionAction(argparse.Action):\n    def __init__(self, *args, **kwargs):\n        super(VersionAction, self).__init__(nargs=0, *args, **kwargs)\n    def __call__(self, *args, **kwargs):\n        printVersions()\n        exit(0)\n# These constants control the beam search decoder\n# Beam width used in the CTC decoder when building candidate transcriptions",
        "detail": "Demo.DeepSpeechFileStream",
        "documentation": {}
    },
    {
        "label": "DeepSpeech",
        "kind": 2,
        "importPath": "Demo.DeepSpeechFileStream",
        "description": "Demo.DeepSpeechFileStream",
        "peekOfCode": "def DeepSpeech(Window, SpeechToNLPQueue, wavefile):\n    # Create Signal Object\n    SpeechSignal = GUISignal()\n    SpeechSignal.signal.connect(Window.UpdateSpeechBox)\n    MsgSignal = GUISignal()\n    MsgSignal.signal.connect(Window.UpdateMsgBox)\n    # References to models:\n    model_path = 'DeepSpeech_Models/deepspeech-0.9.3-models.tflite' #'DeepSpeech_Models/deepspeech-0.9.3-models.pbmm'\n    scorer_path = 'DeepSpeech_Models/brandon.scorer' #'DeepSpeech_Models/deepspeech-0.9.3-models.scorer'\n    print('Loading model from file {}'.format(model_path), file=sys.stderr)",
        "detail": "Demo.DeepSpeechFileStream",
        "documentation": {}
    },
    {
        "label": "BEAM_WIDTH",
        "kind": 5,
        "importPath": "Demo.DeepSpeechFileStream",
        "description": "Demo.DeepSpeechFileStream",
        "peekOfCode": "BEAM_WIDTH = 500\n# The alpha hyperparameter of the CTC decoder. Language Model weight\nLM_ALPHA = 0.75\n# The beta hyperparameter of the CTC decoder. Word insertion bonus.\nLM_BETA = 1.85\ndef DeepSpeech(Window, SpeechToNLPQueue, wavefile):\n    # Create Signal Object\n    SpeechSignal = GUISignal()\n    SpeechSignal.signal.connect(Window.UpdateSpeechBox)\n    MsgSignal = GUISignal()",
        "detail": "Demo.DeepSpeechFileStream",
        "documentation": {}
    },
    {
        "label": "LM_ALPHA",
        "kind": 5,
        "importPath": "Demo.DeepSpeechFileStream",
        "description": "Demo.DeepSpeechFileStream",
        "peekOfCode": "LM_ALPHA = 0.75\n# The beta hyperparameter of the CTC decoder. Word insertion bonus.\nLM_BETA = 1.85\ndef DeepSpeech(Window, SpeechToNLPQueue, wavefile):\n    # Create Signal Object\n    SpeechSignal = GUISignal()\n    SpeechSignal.signal.connect(Window.UpdateSpeechBox)\n    MsgSignal = GUISignal()\n    MsgSignal.signal.connect(Window.UpdateMsgBox)\n    # References to models:",
        "detail": "Demo.DeepSpeechFileStream",
        "documentation": {}
    },
    {
        "label": "LM_BETA",
        "kind": 5,
        "importPath": "Demo.DeepSpeechFileStream",
        "description": "Demo.DeepSpeechFileStream",
        "peekOfCode": "LM_BETA = 1.85\ndef DeepSpeech(Window, SpeechToNLPQueue, wavefile):\n    # Create Signal Object\n    SpeechSignal = GUISignal()\n    SpeechSignal.signal.connect(Window.UpdateSpeechBox)\n    MsgSignal = GUISignal()\n    MsgSignal.signal.connect(Window.UpdateMsgBox)\n    # References to models:\n    model_path = 'DeepSpeech_Models/deepspeech-0.9.3-models.tflite' #'DeepSpeech_Models/deepspeech-0.9.3-models.pbmm'\n    scorer_path = 'DeepSpeech_Models/brandon.scorer' #'DeepSpeech_Models/deepspeech-0.9.3-models.scorer'",
        "detail": "Demo.DeepSpeechFileStream",
        "documentation": {}
    },
    {
        "label": "MicrophoneStream",
        "kind": 6,
        "importPath": "Demo.DeepSpeechMicStream",
        "description": "Demo.DeepSpeechMicStream",
        "peekOfCode": "class MicrophoneStream(object):\n    micSessionCounter = 0\n    \"\"\"Opens a recording stream as a generator yielding the audio chunks.\"\"\"\n    def __init__(self, Window, rate, chunk):\n        MicrophoneStream.micSessionCounter += 1\n        self.Window = Window\n        self._rate = rate\n        self._chunk = chunk\n        self.samplesCounter = 0\n        self.start_time = time.time()",
        "detail": "Demo.DeepSpeechMicStream",
        "documentation": {}
    },
    {
        "label": "VersionAction",
        "kind": 6,
        "importPath": "Demo.DeepSpeechMicStream",
        "description": "Demo.DeepSpeechMicStream",
        "peekOfCode": "class VersionAction(argparse.Action):\n    def __init__(self, *args, **kwargs):\n        super(VersionAction, self).__init__(nargs=0, *args, **kwargs)\n    def __call__(self, *args, **kwargs):\n        printVersions()\n        exit(0)\n# DeepSpeech Recognition Thread for Microphone\ndef DeepSpeech(Window, SpeechToNLPQueue):\n    # Create Signal Object\n    SpeechSignal = GUISignal()",
        "detail": "Demo.DeepSpeechMicStream",
        "documentation": {}
    },
    {
        "label": "convert_samplerate",
        "kind": 2,
        "importPath": "Demo.DeepSpeechMicStream",
        "description": "Demo.DeepSpeechMicStream",
        "peekOfCode": "def convert_samplerate(audio_path):\n    sox_cmd = 'sox {} --type raw --bits 16 --channels 1 --rate 16000 --encoding signed-integer --endian little --compression 0.0 --no-dither - '.format(quote(audio_path))\n    try:\n        output = subprocess.check_output(shlex.split(sox_cmd), stderr=subprocess.PIPE)\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError('SoX returned non-zero status: {}'.format(e.stderr))\n    except OSError as e:\n        raise OSError(e.errno, 'SoX not found, use 16kHz files or install it: {}'.format(e.strerror))\n    return 16000, np.frombuffer(output, np.int16)\ndef metadata_to_string(metadata):",
        "detail": "Demo.DeepSpeechMicStream",
        "documentation": {}
    },
    {
        "label": "metadata_to_string",
        "kind": 2,
        "importPath": "Demo.DeepSpeechMicStream",
        "description": "Demo.DeepSpeechMicStream",
        "peekOfCode": "def metadata_to_string(metadata):\n    return ''.join(item.character for item in metadata.items)\nclass VersionAction(argparse.Action):\n    def __init__(self, *args, **kwargs):\n        super(VersionAction, self).__init__(nargs=0, *args, **kwargs)\n    def __call__(self, *args, **kwargs):\n        printVersions()\n        exit(0)\n# DeepSpeech Recognition Thread for Microphone\ndef DeepSpeech(Window, SpeechToNLPQueue):",
        "detail": "Demo.DeepSpeechMicStream",
        "documentation": {}
    },
    {
        "label": "DeepSpeech",
        "kind": 2,
        "importPath": "Demo.DeepSpeechMicStream",
        "description": "Demo.DeepSpeechMicStream",
        "peekOfCode": "def DeepSpeech(Window, SpeechToNLPQueue):\n    # Create Signal Object\n    SpeechSignal = GUISignal()\n    SpeechSignal.signal.connect(Window.UpdateSpeechBox)\n    MsgSignal = GUISignal()\n    MsgSignal.signal.connect(Window.UpdateMsgBox)\n     # References to models:\n    model_path = 'DeepSpeech_Models/deepspeech-0.9.3-models.tflite' #'DeepSpeech_Models/deepspeech-0.9.3-models.pbmm'\n    scorer_path = 'DeepSpeech_Models/brandon.scorer' #'DeepSpeech_Models/deepspeech-0.9.3-models.scorer'\n    print('Loading model from file {}'.format(model_path), file=sys.stderr)",
        "detail": "Demo.DeepSpeechMicStream",
        "documentation": {}
    },
    {
        "label": "RATE",
        "kind": 5,
        "importPath": "Demo.DeepSpeechMicStream",
        "description": "Demo.DeepSpeechMicStream",
        "peekOfCode": "RATE = 16000\nCHUNK = int(RATE / 10)  # 100ms\nclass MicrophoneStream(object):\n    micSessionCounter = 0\n    \"\"\"Opens a recording stream as a generator yielding the audio chunks.\"\"\"\n    def __init__(self, Window, rate, chunk):\n        MicrophoneStream.micSessionCounter += 1\n        self.Window = Window\n        self._rate = rate\n        self._chunk = chunk",
        "detail": "Demo.DeepSpeechMicStream",
        "documentation": {}
    },
    {
        "label": "CHUNK",
        "kind": 5,
        "importPath": "Demo.DeepSpeechMicStream",
        "description": "Demo.DeepSpeechMicStream",
        "peekOfCode": "CHUNK = int(RATE / 10)  # 100ms\nclass MicrophoneStream(object):\n    micSessionCounter = 0\n    \"\"\"Opens a recording stream as a generator yielding the audio chunks.\"\"\"\n    def __init__(self, Window, rate, chunk):\n        MicrophoneStream.micSessionCounter += 1\n        self.Window = Window\n        self._rate = rate\n        self._chunk = chunk\n        self.samplesCounter = 0",
        "detail": "Demo.DeepSpeechMicStream",
        "documentation": {}
    },
    {
        "label": "BEAM_WIDTH",
        "kind": 5,
        "importPath": "Demo.DeepSpeechMicStream",
        "description": "Demo.DeepSpeechMicStream",
        "peekOfCode": "BEAM_WIDTH = 500\n# The alpha hyperparameter of the CTC decoder. Language Model weight\nLM_ALPHA = 0.75\n# The beta hyperparameter of the CTC decoder. Word insertion bonus.\nLM_BETA = 1.85 #\ndef convert_samplerate(audio_path):\n    sox_cmd = 'sox {} --type raw --bits 16 --channels 1 --rate 16000 --encoding signed-integer --endian little --compression 0.0 --no-dither - '.format(quote(audio_path))\n    try:\n        output = subprocess.check_output(shlex.split(sox_cmd), stderr=subprocess.PIPE)\n    except subprocess.CalledProcessError as e:",
        "detail": "Demo.DeepSpeechMicStream",
        "documentation": {}
    },
    {
        "label": "LM_ALPHA",
        "kind": 5,
        "importPath": "Demo.DeepSpeechMicStream",
        "description": "Demo.DeepSpeechMicStream",
        "peekOfCode": "LM_ALPHA = 0.75\n# The beta hyperparameter of the CTC decoder. Word insertion bonus.\nLM_BETA = 1.85 #\ndef convert_samplerate(audio_path):\n    sox_cmd = 'sox {} --type raw --bits 16 --channels 1 --rate 16000 --encoding signed-integer --endian little --compression 0.0 --no-dither - '.format(quote(audio_path))\n    try:\n        output = subprocess.check_output(shlex.split(sox_cmd), stderr=subprocess.PIPE)\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError('SoX returned non-zero status: {}'.format(e.stderr))\n    except OSError as e:",
        "detail": "Demo.DeepSpeechMicStream",
        "documentation": {}
    },
    {
        "label": "LM_BETA",
        "kind": 5,
        "importPath": "Demo.DeepSpeechMicStream",
        "description": "Demo.DeepSpeechMicStream",
        "peekOfCode": "LM_BETA = 1.85 #\ndef convert_samplerate(audio_path):\n    sox_cmd = 'sox {} --type raw --bits 16 --channels 1 --rate 16000 --encoding signed-integer --endian little --compression 0.0 --no-dither - '.format(quote(audio_path))\n    try:\n        output = subprocess.check_output(shlex.split(sox_cmd), stderr=subprocess.PIPE)\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError('SoX returned non-zero status: {}'.format(e.stderr))\n    except OSError as e:\n        raise OSError(e.errno, 'SoX not found, use 16kHz files or install it: {}'.format(e.strerror))\n    return 16000, np.frombuffer(output, np.int16)",
        "detail": "Demo.DeepSpeechMicStream",
        "documentation": {}
    },
    {
        "label": "ConformerStream",
        "kind": 2,
        "importPath": "Demo.EMSConformerFileStream",
        "description": "Demo.EMSConformerFileStream",
        "peekOfCode": "def ConformerStream(SpeechToNLPQueue,VideoSignalQueue, ConformerSignalQueue, wavefile_name):\n    with wave.open(wavefile_name, 'rb') as wf:\n        try:\n            # Instantiate PyAudio and initialize PortAudio system resources (1)\n            p = pyaudio.PyAudio()\n            info = p.get_default_host_api_info()\n            device_index = info.get('deviceCount') - 1 # get default device as output device\n            stream = p.open(format = pyaudio.paInt16, channels = 1, rate = RATE, output = True, frames_per_buffer = CHUNK, output_device_index=device_index)\n            # Play samples from the wave file (3)\n            while len(data:=wf.readframes(CHUNK)):  # Requires Python 3.8+ for :=",
        "detail": "Demo.EMSConformerFileStream",
        "documentation": {}
    },
    {
        "label": "RATE",
        "kind": 5,
        "importPath": "Demo.EMSConformerFileStream",
        "description": "Demo.EMSConformerFileStream",
        "peekOfCode": "RATE = 16000\nCHUNK = RATE // 10  # 100ms\ndef ConformerStream(SpeechToNLPQueue,VideoSignalQueue, ConformerSignalQueue, wavefile_name):\n    with wave.open(wavefile_name, 'rb') as wf:\n        try:\n            # Instantiate PyAudio and initialize PortAudio system resources (1)\n            p = pyaudio.PyAudio()\n            info = p.get_default_host_api_info()\n            device_index = info.get('deviceCount') - 1 # get default device as output device\n            stream = p.open(format = pyaudio.paInt16, channels = 1, rate = RATE, output = True, frames_per_buffer = CHUNK, output_device_index=device_index)",
        "detail": "Demo.EMSConformerFileStream",
        "documentation": {}
    },
    {
        "label": "CHUNK",
        "kind": 5,
        "importPath": "Demo.EMSConformerFileStream",
        "description": "Demo.EMSConformerFileStream",
        "peekOfCode": "CHUNK = RATE // 10  # 100ms\ndef ConformerStream(SpeechToNLPQueue,VideoSignalQueue, ConformerSignalQueue, wavefile_name):\n    with wave.open(wavefile_name, 'rb') as wf:\n        try:\n            # Instantiate PyAudio and initialize PortAudio system resources (1)\n            p = pyaudio.PyAudio()\n            info = p.get_default_host_api_info()\n            device_index = info.get('deviceCount') - 1 # get default device as output device\n            stream = p.open(format = pyaudio.paInt16, channels = 1, rate = RATE, output = True, frames_per_buffer = CHUNK, output_device_index=device_index)\n            # Play samples from the wave file (3)",
        "detail": "Demo.EMSConformerFileStream",
        "documentation": {}
    },
    {
        "label": "get_ground_truth_transcript",
        "kind": 2,
        "importPath": "Demo.EndToEndEval",
        "description": "Demo.EndToEndEval",
        "peekOfCode": "def get_ground_truth_transcript(recording):\n    with open(f'Audio_Scenarios/2019_Test_Ground_Truth/{recording}-transcript.txt') as f:\n        ground_truth = f.read()\n    return ground_truth\ndef get_ground_truth_protocol(recording):\n    with open(f'Audio_Scenarios/2019_Test_Ground_Truth/{recording}-protocol.txt') as f:\n        ground_truth = f.read()\n    return ground_truth\ndef get_ground_truth_one_hot_vector(recording):\n    ground_truth = get_ground_truth_protocol(recording)",
        "detail": "Demo.EndToEndEval",
        "documentation": {}
    },
    {
        "label": "get_ground_truth_protocol",
        "kind": 2,
        "importPath": "Demo.EndToEndEval",
        "description": "Demo.EndToEndEval",
        "peekOfCode": "def get_ground_truth_protocol(recording):\n    with open(f'Audio_Scenarios/2019_Test_Ground_Truth/{recording}-protocol.txt') as f:\n        ground_truth = f.read()\n    return ground_truth\ndef get_ground_truth_one_hot_vector(recording):\n    ground_truth = get_ground_truth_protocol(recording)\n    one_hot_vector = [1 if label.lower() == ground_truth.lower() else 0 for label in ungroup_p_node]\n    return np.array(one_hot_vector)\ndef get_wer_and_cer(recording, transcript):\n    # tokenized_reference_text = Processor.tokenizer._normalize(get_ground_truth_transcript(recording))",
        "detail": "Demo.EndToEndEval",
        "documentation": {}
    },
    {
        "label": "get_ground_truth_one_hot_vector",
        "kind": 2,
        "importPath": "Demo.EndToEndEval",
        "description": "Demo.EndToEndEval",
        "peekOfCode": "def get_ground_truth_one_hot_vector(recording):\n    ground_truth = get_ground_truth_protocol(recording)\n    one_hot_vector = [1 if label.lower() == ground_truth.lower() else 0 for label in ungroup_p_node]\n    return np.array(one_hot_vector)\ndef get_wer_and_cer(recording, transcript):\n    # tokenized_reference_text = Processor.tokenizer._normalize(get_ground_truth_transcript(recording))\n    # tokenized_prediction_text = Processor.tokenizer._normalize(transcript)\n    # wer = wer_metric.compute(references=[tokenized_reference_text], predictions=[tokenized_prediction_text])\n    # cer = cer_metric.compute(references=[tokenized_reference_text], predictions=[tokenized_prediction_text])\n    return 0, 0",
        "detail": "Demo.EndToEndEval",
        "documentation": {}
    },
    {
        "label": "get_wer_and_cer",
        "kind": 2,
        "importPath": "Demo.EndToEndEval",
        "description": "Demo.EndToEndEval",
        "peekOfCode": "def get_wer_and_cer(recording, transcript):\n    # tokenized_reference_text = Processor.tokenizer._normalize(get_ground_truth_transcript(recording))\n    # tokenized_prediction_text = Processor.tokenizer._normalize(transcript)\n    # wer = wer_metric.compute(references=[tokenized_reference_text], predictions=[tokenized_prediction_text])\n    # cer = cer_metric.compute(references=[tokenized_reference_text], predictions=[tokenized_prediction_text])\n    return 0, 0\ndef check_protocol_correct(recording, protocol):\n    print('check protocol correct:',protocol)\n    if protocol == -1: return -1\n    ground_truth = get_ground_truth_protocol(recording)",
        "detail": "Demo.EndToEndEval",
        "documentation": {}
    },
    {
        "label": "check_protocol_correct",
        "kind": 2,
        "importPath": "Demo.EndToEndEval",
        "description": "Demo.EndToEndEval",
        "peekOfCode": "def check_protocol_correct(recording, protocol):\n    print('check protocol correct:',protocol)\n    if protocol == -1: return -1\n    ground_truth = get_ground_truth_protocol(recording)\n    return int(protocol.lower() == ground_truth.lower())\ndef is_singleton_array(arr):\n    shape = arr.shape\n    # Check if all dimensions have size 1\n    return all(dim == 1 for dim in shape)\n# --- main ---------------------------",
        "detail": "Demo.EndToEndEval",
        "documentation": {}
    },
    {
        "label": "is_singleton_array",
        "kind": 2,
        "importPath": "Demo.EndToEndEval",
        "description": "Demo.EndToEndEval",
        "peekOfCode": "def is_singleton_array(arr):\n    shape = arr.shape\n    # Check if all dimensions have size 1\n    return all(dim == 1 for dim in shape)\n# --- main ---------------------------\nif __name__ == '__main__':\n    one_hot_pred_all_recordings = []\n    one_hot_gt_all_recordings = []\n    for trial in range(pipeline_config.num_trials):\n        for whisper_model in pipeline_config.whisper_model_sizes:",
        "detail": "Demo.EndToEndEval",
        "documentation": {}
    },
    {
        "label": "wer_metric",
        "kind": 5,
        "importPath": "Demo.EndToEndEval",
        "description": "Demo.EndToEndEval",
        "peekOfCode": "wer_metric = load(\"wer\")\ncer_metric = load(\"cer\")\n# --- helper methods -----------------\ndef get_ground_truth_transcript(recording):\n    with open(f'Audio_Scenarios/2019_Test_Ground_Truth/{recording}-transcript.txt') as f:\n        ground_truth = f.read()\n    return ground_truth\ndef get_ground_truth_protocol(recording):\n    with open(f'Audio_Scenarios/2019_Test_Ground_Truth/{recording}-protocol.txt') as f:\n        ground_truth = f.read()",
        "detail": "Demo.EndToEndEval",
        "documentation": {}
    },
    {
        "label": "cer_metric",
        "kind": 5,
        "importPath": "Demo.EndToEndEval",
        "description": "Demo.EndToEndEval",
        "peekOfCode": "cer_metric = load(\"cer\")\n# --- helper methods -----------------\ndef get_ground_truth_transcript(recording):\n    with open(f'Audio_Scenarios/2019_Test_Ground_Truth/{recording}-transcript.txt') as f:\n        ground_truth = f.read()\n    return ground_truth\ndef get_ground_truth_protocol(recording):\n    with open(f'Audio_Scenarios/2019_Test_Ground_Truth/{recording}-protocol.txt') as f:\n        ground_truth = f.read()\n    return ground_truth",
        "detail": "Demo.EndToEndEval",
        "documentation": {}
    },
    {
        "label": "FeedbackObj",
        "kind": 6,
        "importPath": "Demo.Feedback",
        "description": "Demo.Feedback",
        "peekOfCode": "class FeedbackObj:\n    def __init__(self, intervention, protocol, concept):\n        super(FeedbackObj, self).__init__()\n        self.intervention = intervention\n        self.protocol = protocol\n        self.concept = concept\ndef sendMessage(feedbackObj:FeedbackObj, connection):\n    count = 0\n    data_string = b\"\"\n    # data_string = pickle.dumps(feedbackObj)   ",
        "detail": "Demo.Feedback",
        "documentation": {}
    },
    {
        "label": "sendMessage",
        "kind": 2,
        "importPath": "Demo.Feedback",
        "description": "Demo.Feedback",
        "peekOfCode": "def sendMessage(feedbackObj:FeedbackObj, connection):\n    count = 0\n    data_string = b\"\"\n    # data_string = pickle.dumps(feedbackObj)   \n    # data_string = json.dumps(feedbackObj)\n    if feedbackObj:\n        print(\"Feedback Object: \",feedbackObj.concept, feedbackObj.intervention, feedbackObj.protocol)\n        if feedbackObj.concept:\n            data_string = b\"Concepts: \" + feedbackObj.concept.encode('ascii') + b'\\0'\n        if feedbackObj.intervention:",
        "detail": "Demo.Feedback",
        "documentation": {}
    },
    {
        "label": "Feedbac",
        "kind": 2,
        "importPath": "Demo.Feedback",
        "description": "Demo.Feedback",
        "peekOfCode": "def Feedback (Window, data_path, FeedbackQueue):\n    #initialize tcp connection\n    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)  \n    sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    sock.bind((\"0.0.0.0\", TCP_PORT))  \n    sock.listen(5) \n    print(\"Waiting for client in feedback...\")\n    connection,address = sock.accept()  \n    print(\"Client connected for feedback: \",address)\n    # while(True):",
        "detail": "Demo.Feedback",
        "documentation": {}
    },
    {
        "label": "TCP_PORT",
        "kind": 5,
        "importPath": "Demo.Feedback",
        "description": "Demo.Feedback",
        "peekOfCode": "TCP_PORT = 7088\nclass FeedbackObj:\n    def __init__(self, intervention, protocol, concept):\n        super(FeedbackObj, self).__init__()\n        self.intervention = intervention\n        self.protocol = protocol\n        self.concept = concept\ndef sendMessage(feedbackObj:FeedbackObj, connection):\n    count = 0\n    data_string = b\"\"",
        "detail": "Demo.Feedback",
        "documentation": {}
    },
    {
        "label": "MainWindow",
        "kind": 6,
        "importPath": "Demo.GUI",
        "description": "Demo.GUI",
        "peekOfCode": "class MainWindow(QWidget):\n    def __init__(self, width, height):\n        super(MainWindow, self).__init__()\n        # Fields\n        self.width = width\n        self.height = height\n        self.SpeechThread = None\n        # Set geometry of the window\n        self.setWindowTitle('CognitiveEMS Demo')\n        self.setWindowIcon(QIcon('./Images/Logos/UVA.png'))",
        "detail": "Demo.GUI",
        "documentation": {}
    },
    {
        "label": "chunkdata",
        "kind": 5,
        "importPath": "Demo.GUI",
        "description": "Demo.GUI",
        "peekOfCode": "chunkdata = []\nConceptDict = dict()\ncurr_date = datetime.datetime.now()\ndt_string = curr_date.strftime(\"%d-%m-%Y-%H-%M-%S\")\ndata_path = \"./data_collection_folder/\"+dt_string+\"/\"\n# ================================================================== GUI ==================================================================\n# Main Window of the Application\nclass MainWindow(QWidget):\n    def __init__(self, width, height):\n        super(MainWindow, self).__init__()",
        "detail": "Demo.GUI",
        "documentation": {}
    },
    {
        "label": "ConceptDict",
        "kind": 5,
        "importPath": "Demo.GUI",
        "description": "Demo.GUI",
        "peekOfCode": "ConceptDict = dict()\ncurr_date = datetime.datetime.now()\ndt_string = curr_date.strftime(\"%d-%m-%Y-%H-%M-%S\")\ndata_path = \"./data_collection_folder/\"+dt_string+\"/\"\n# ================================================================== GUI ==================================================================\n# Main Window of the Application\nclass MainWindow(QWidget):\n    def __init__(self, width, height):\n        super(MainWindow, self).__init__()\n        # Fields",
        "detail": "Demo.GUI",
        "documentation": {}
    },
    {
        "label": "curr_date",
        "kind": 5,
        "importPath": "Demo.GUI",
        "description": "Demo.GUI",
        "peekOfCode": "curr_date = datetime.datetime.now()\ndt_string = curr_date.strftime(\"%d-%m-%Y-%H-%M-%S\")\ndata_path = \"./data_collection_folder/\"+dt_string+\"/\"\n# ================================================================== GUI ==================================================================\n# Main Window of the Application\nclass MainWindow(QWidget):\n    def __init__(self, width, height):\n        super(MainWindow, self).__init__()\n        # Fields\n        self.width = width",
        "detail": "Demo.GUI",
        "documentation": {}
    },
    {
        "label": "dt_string",
        "kind": 5,
        "importPath": "Demo.GUI",
        "description": "Demo.GUI",
        "peekOfCode": "dt_string = curr_date.strftime(\"%d-%m-%Y-%H-%M-%S\")\ndata_path = \"./data_collection_folder/\"+dt_string+\"/\"\n# ================================================================== GUI ==================================================================\n# Main Window of the Application\nclass MainWindow(QWidget):\n    def __init__(self, width, height):\n        super(MainWindow, self).__init__()\n        # Fields\n        self.width = width\n        self.height = height",
        "detail": "Demo.GUI",
        "documentation": {}
    },
    {
        "label": "data_path",
        "kind": 5,
        "importPath": "Demo.GUI",
        "description": "Demo.GUI",
        "peekOfCode": "data_path = \"./data_collection_folder/\"+dt_string+\"/\"\n# ================================================================== GUI ==================================================================\n# Main Window of the Application\nclass MainWindow(QWidget):\n    def __init__(self, width, height):\n        super(MainWindow, self).__init__()\n        # Fields\n        self.width = width\n        self.height = height\n        self.SpeechThread = None",
        "detail": "Demo.GUI",
        "documentation": {}
    },
    {
        "label": "GenerateForm",
        "kind": 2,
        "importPath": "Demo.GenerateForm",
        "description": "Demo.GenerateForm",
        "peekOfCode": "def GenerateForm(Window, text):\n    # Create Signal Objects\n    MsgSignal = GUISignal()\n    MsgSignal.signal.connect(Window.UpdateMsgBox)\n    ButtonsSignal = GUISignal()\n    ButtonsSignal.signal.connect(Window.ButtonsSetEnabled)\n    ButtonsSignal.signal.emit([(Window.GenerateFormButton, False)])\n    dummy12= text\n    dummy12 = dummy12.replace('\\r', '').replace('\\n', '')\n    dummyP2=dummy12.replace(' ','%20')",
        "detail": "Demo.GenerateForm",
        "documentation": {}
    },
    {
        "label": "FileStream",
        "kind": 6,
        "importPath": "Demo.GoogleSpeechFileStream",
        "description": "Demo.GoogleSpeechFileStream",
        "peekOfCode": "class FileStream(object):\n    fileSessionCounter = 0\n    position = 0\n    \"\"\"Opens a file stream as a generator yielding the audio chunks.\"\"\"\n    def __init__(self, Window, rate, chunk, wavefile):\n        FileStream.fileSessionCounter += 1\n        self.Window = Window\n        self._rate = rate\n        self._chunk = chunk\n        self.filename = wavefile",
        "detail": "Demo.GoogleSpeechFileStream",
        "documentation": {}
    },
    {
        "label": "GoogleSpeech",
        "kind": 2,
        "importPath": "Demo.GoogleSpeechFileStream",
        "description": "Demo.GoogleSpeechFileStream",
        "peekOfCode": "def GoogleSpeech(Window, SpeechToNLPQueue,EMSAgentSpeechToNLPQueue, wavefile_name, data_path_str, audioStreamBool, transcriptStreamBool):\n    def numberWords(s):\n        count = 0\n        for c in s:\n            if c == ' ': count += 1\n        return count\n    # Create GUI Signal Object\n    SpeechSignal = GUISignal()\n    SpeechSignal.signal.connect(Window.UpdateSpeechBox)\n    MsgSignal = GUISignal()",
        "detail": "Demo.GoogleSpeechFileStream",
        "documentation": {}
    },
    {
        "label": "RATE",
        "kind": 5,
        "importPath": "Demo.GoogleSpeechFileStream",
        "description": "Demo.GoogleSpeechFileStream",
        "peekOfCode": "RATE = 44100\nCHUNK = int(RATE / 10)  # 100ms\nclass FileStream(object):\n    fileSessionCounter = 0\n    position = 0\n    \"\"\"Opens a file stream as a generator yielding the audio chunks.\"\"\"\n    def __init__(self, Window, rate, chunk, wavefile):\n        FileStream.fileSessionCounter += 1\n        self.Window = Window\n        self._rate = rate",
        "detail": "Demo.GoogleSpeechFileStream",
        "documentation": {}
    },
    {
        "label": "CHUNK",
        "kind": 5,
        "importPath": "Demo.GoogleSpeechFileStream",
        "description": "Demo.GoogleSpeechFileStream",
        "peekOfCode": "CHUNK = int(RATE / 10)  # 100ms\nclass FileStream(object):\n    fileSessionCounter = 0\n    position = 0\n    \"\"\"Opens a file stream as a generator yielding the audio chunks.\"\"\"\n    def __init__(self, Window, rate, chunk, wavefile):\n        FileStream.fileSessionCounter += 1\n        self.Window = Window\n        self._rate = rate\n        self._chunk = chunk",
        "detail": "Demo.GoogleSpeechFileStream",
        "documentation": {}
    },
    {
        "label": "MicrophoneStream",
        "kind": 6,
        "importPath": "Demo.GoogleSpeechMicStream",
        "description": "Demo.GoogleSpeechMicStream",
        "peekOfCode": "class MicrophoneStream(object):\n    micSessionCounter = 0\n    audio_buffer = b\"\"\n    \"\"\"Opens a recording stream as a generator yielding the audio chunks.\"\"\"\n    def __init__(self, Window, rate, chunk, data_path_str, audioStreamBool, transcriptStreamBool):\n        MicrophoneStream.micSessionCounter += 1\n        self.Window = Window\n        self._rate = rate\n        self._chunk = chunk\n        self.samplesCounter = 0",
        "detail": "Demo.GoogleSpeechMicStream",
        "documentation": {}
    },
    {
        "label": "audio_stream_UDP",
        "kind": 2,
        "importPath": "Demo.GoogleSpeechMicStream",
        "description": "Demo.GoogleSpeechMicStream",
        "peekOfCode": "def audio_stream_UDP(stopped):\n    # AUDIO streaming variables and functions\n    host_name = socket.gethostname()\n    host_ip = '0.0.0.0' #socket.gethostbyname(host_name)\n    port = 8888\n    BUFF_SIZE = 1280 #65536\n    client_socket = socket.socket(socket.AF_INET,socket.SOCK_DGRAM)\n    client_socket.bind((host_ip,port))\n    RATE = 16000\n    CHUNK = int (RATE / 10)",
        "detail": "Demo.GoogleSpeechMicStream",
        "documentation": {}
    },
    {
        "label": "GoogleSpeech",
        "kind": 2,
        "importPath": "Demo.GoogleSpeechMicStream",
        "description": "Demo.GoogleSpeechMicStream",
        "peekOfCode": "def GoogleSpeech(Window, SpeechToNLPQueue,EMSAgentSpeechToNLPQueue, data_path_str, audioStreamBool, transcriptStreamBool):\n    data_path_str += \"audiodata/\"\n    with speech.SpeechClient() as client:\n        # Create GUI Signal Object\n        SpeechSignal = GUISignal()\n        SpeechSignal.signal.connect(Window.UpdateSpeechBox)\n        MsgSignal = GUISignal()\n        MsgSignal.signal.connect(Window.UpdateMsgBox)\n        ButtonsSignal = GUISignal()\n        ButtonsSignal.signal.connect(Window.ButtonsSetEnabled)",
        "detail": "Demo.GoogleSpeechMicStream",
        "documentation": {}
    },
    {
        "label": "client",
        "kind": 5,
        "importPath": "Demo.GoogleSpeechMicStream",
        "description": "Demo.GoogleSpeechMicStream",
        "peekOfCode": "client = None\nstreaming_config = None\n# Audio recording parameters\nRATE = 16000\nCHUNK = int(RATE / 20)  # 100ms #50ms\n#CHUNK = int(RATE * 5)  # 100ms\nstopped = False\naudio_buff = queue.Queue(maxsize=16)\nwav_audio_buffer = queue.Queue()\ndef audio_stream_UDP(stopped):",
        "detail": "Demo.GoogleSpeechMicStream",
        "documentation": {}
    },
    {
        "label": "streaming_config",
        "kind": 5,
        "importPath": "Demo.GoogleSpeechMicStream",
        "description": "Demo.GoogleSpeechMicStream",
        "peekOfCode": "streaming_config = None\n# Audio recording parameters\nRATE = 16000\nCHUNK = int(RATE / 20)  # 100ms #50ms\n#CHUNK = int(RATE * 5)  # 100ms\nstopped = False\naudio_buff = queue.Queue(maxsize=16)\nwav_audio_buffer = queue.Queue()\ndef audio_stream_UDP(stopped):\n    # AUDIO streaming variables and functions",
        "detail": "Demo.GoogleSpeechMicStream",
        "documentation": {}
    },
    {
        "label": "RATE",
        "kind": 5,
        "importPath": "Demo.GoogleSpeechMicStream",
        "description": "Demo.GoogleSpeechMicStream",
        "peekOfCode": "RATE = 16000\nCHUNK = int(RATE / 20)  # 100ms #50ms\n#CHUNK = int(RATE * 5)  # 100ms\nstopped = False\naudio_buff = queue.Queue(maxsize=16)\nwav_audio_buffer = queue.Queue()\ndef audio_stream_UDP(stopped):\n    # AUDIO streaming variables and functions\n    host_name = socket.gethostname()\n    host_ip = '0.0.0.0' #socket.gethostbyname(host_name)",
        "detail": "Demo.GoogleSpeechMicStream",
        "documentation": {}
    },
    {
        "label": "CHUNK",
        "kind": 5,
        "importPath": "Demo.GoogleSpeechMicStream",
        "description": "Demo.GoogleSpeechMicStream",
        "peekOfCode": "CHUNK = int(RATE / 20)  # 100ms #50ms\n#CHUNK = int(RATE * 5)  # 100ms\nstopped = False\naudio_buff = queue.Queue(maxsize=16)\nwav_audio_buffer = queue.Queue()\ndef audio_stream_UDP(stopped):\n    # AUDIO streaming variables and functions\n    host_name = socket.gethostname()\n    host_ip = '0.0.0.0' #socket.gethostbyname(host_name)\n    port = 8888",
        "detail": "Demo.GoogleSpeechMicStream",
        "documentation": {}
    },
    {
        "label": "#CHUNK",
        "kind": 5,
        "importPath": "Demo.GoogleSpeechMicStream",
        "description": "Demo.GoogleSpeechMicStream",
        "peekOfCode": "#CHUNK = int(RATE * 5)  # 100ms\nstopped = False\naudio_buff = queue.Queue(maxsize=16)\nwav_audio_buffer = queue.Queue()\ndef audio_stream_UDP(stopped):\n    # AUDIO streaming variables and functions\n    host_name = socket.gethostname()\n    host_ip = '0.0.0.0' #socket.gethostbyname(host_name)\n    port = 8888\n    BUFF_SIZE = 1280 #65536",
        "detail": "Demo.GoogleSpeechMicStream",
        "documentation": {}
    },
    {
        "label": "stopped",
        "kind": 5,
        "importPath": "Demo.GoogleSpeechMicStream",
        "description": "Demo.GoogleSpeechMicStream",
        "peekOfCode": "stopped = False\naudio_buff = queue.Queue(maxsize=16)\nwav_audio_buffer = queue.Queue()\ndef audio_stream_UDP(stopped):\n    # AUDIO streaming variables and functions\n    host_name = socket.gethostname()\n    host_ip = '0.0.0.0' #socket.gethostbyname(host_name)\n    port = 8888\n    BUFF_SIZE = 1280 #65536\n    client_socket = socket.socket(socket.AF_INET,socket.SOCK_DGRAM)",
        "detail": "Demo.GoogleSpeechMicStream",
        "documentation": {}
    },
    {
        "label": "audio_buff",
        "kind": 5,
        "importPath": "Demo.GoogleSpeechMicStream",
        "description": "Demo.GoogleSpeechMicStream",
        "peekOfCode": "audio_buff = queue.Queue(maxsize=16)\nwav_audio_buffer = queue.Queue()\ndef audio_stream_UDP(stopped):\n    # AUDIO streaming variables and functions\n    host_name = socket.gethostname()\n    host_ip = '0.0.0.0' #socket.gethostbyname(host_name)\n    port = 8888\n    BUFF_SIZE = 1280 #65536\n    client_socket = socket.socket(socket.AF_INET,socket.SOCK_DGRAM)\n    client_socket.bind((host_ip,port))",
        "detail": "Demo.GoogleSpeechMicStream",
        "documentation": {}
    },
    {
        "label": "wav_audio_buffer",
        "kind": 5,
        "importPath": "Demo.GoogleSpeechMicStream",
        "description": "Demo.GoogleSpeechMicStream",
        "peekOfCode": "wav_audio_buffer = queue.Queue()\ndef audio_stream_UDP(stopped):\n    # AUDIO streaming variables and functions\n    host_name = socket.gethostname()\n    host_ip = '0.0.0.0' #socket.gethostbyname(host_name)\n    port = 8888\n    BUFF_SIZE = 1280 #65536\n    client_socket = socket.socket(socket.AF_INET,socket.SOCK_DGRAM)\n    client_socket.bind((host_ip,port))\n    RATE = 16000",
        "detail": "Demo.GoogleSpeechMicStream",
        "documentation": {}
    },
    {
        "label": "t1",
        "kind": 5,
        "importPath": "Demo.GoogleSpeechMicStream",
        "description": "Demo.GoogleSpeechMicStream",
        "peekOfCode": "t1 = threading.Thread(target=audio_stream_UDP, args=(stopped,))\nt1.start()\nclass MicrophoneStream(object):\n    micSessionCounter = 0\n    audio_buffer = b\"\"\n    \"\"\"Opens a recording stream as a generator yielding the audio chunks.\"\"\"\n    def __init__(self, Window, rate, chunk, data_path_str, audioStreamBool, transcriptStreamBool):\n        MicrophoneStream.micSessionCounter += 1\n        self.Window = Window\n        self._rate = rate",
        "detail": "Demo.GoogleSpeechMicStream",
        "documentation": {}
    },
    {
        "label": "NLPutils",
        "kind": 6,
        "importPath": "Demo.NLPutils",
        "description": "Demo.NLPutils",
        "peekOfCode": "class NLPutils(object):\n    def __init__(self):\n        self.bigram_measures = nltk.collocations.BigramAssocMeasures()\n        self.trigram_measures = nltk.collocations.TrigramAssocMeasures()\n#pos = StanfordPOSTagger('./stanford-postagger-2013-06-20/models/english-left3words-distsim.tagger', \n#                './stanford-postagger-2013-06-20/stanford-postagger.jar')\n# N-gram patterns for technical terms\n        self.patterns = [['NN'],['JJ'], ['JJ','JJ'],['JJ', 'NN'], ['NN','NN'], ['JJ', 'JJ', 'NN'], ['JJ', 'NN', 'NN'], \n                        ['NN', 'JJ', 'NN'], ['NN', 'NN', 'NN'],['NN', 'IN', 'NN'],['JJ','JJ','JJ'],\n\t\t\t            ['JJ', 'JJ', 'JJ','NN'], ['JJ', 'JJ', 'NN','NN'], ['JJ', 'NN', 'NN','NN'],['JJ', 'NN', 'JJ','NN'],['NN', 'JJ', 'JJ','NN'], ",
        "detail": "Demo.NLPutils",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Demo.NLPutils",
        "description": "Demo.NLPutils",
        "peekOfCode": "def main():\n\tos.chdir(\"./dataset\")\n\t# Set default encoding of python to utf8\n\treload(sys)  \n\tsys.setdefaultencoding('utf8')\n\tngrams_set = set()\n\twith open('output.csv', 'w') as output:\n\t\tcsvwriter = csv.writer(output)\n\t\tcsvwriter.writerow([\"Data File\", \"Narrative\", \"N-grams\"])\n\t\tfor file in glob.glob(\"*.txt\"):",
        "detail": "Demo.NLPutils",
        "documentation": {}
    },
    {
        "label": "#pos",
        "kind": 5,
        "importPath": "Demo.NLPutils",
        "description": "Demo.NLPutils",
        "peekOfCode": "#pos = StanfordPOSTagger('./stanford-postagger-2013-06-20/models/english-left3words-distsim.tagger', \n#                './stanford-postagger-2013-06-20/stanford-postagger.jar')\n# N-gram patterns for technical terms\n        self.patterns = [['NN'],['JJ'], ['JJ','JJ'],['JJ', 'NN'], ['NN','NN'], ['JJ', 'JJ', 'NN'], ['JJ', 'NN', 'NN'], \n                        ['NN', 'JJ', 'NN'], ['NN', 'NN', 'NN'],['NN', 'IN', 'NN'],['JJ','JJ','JJ'],\n\t\t\t            ['JJ', 'JJ', 'JJ','NN'], ['JJ', 'JJ', 'NN','NN'], ['JJ', 'NN', 'NN','NN'],['JJ', 'NN', 'JJ','NN'],['NN', 'JJ', 'JJ','NN'], \n\t\t\t            ['NN', 'NN', 'NN','NN'],['NN','JJ', 'NN','NN'], ['NN', 'NN','JJ', 'NN']]\n# For filtering junk\n        self.non_tech_words = []\n        self.googlenews_model =  gensim.models.KeyedVectors.load_word2vec_format('/Users/sileshu/Downloads/GoogleNews-vectors-negative300.bin', binary=True)",
        "detail": "Demo.NLPutils",
        "documentation": {}
    },
    {
        "label": "\t\t#tags",
        "kind": 5,
        "importPath": "Demo.NLPutils",
        "description": "Demo.NLPutils",
        "peekOfCode": "\t\t#tags = tags + pos.tag(tokens[starti:endi])[0]; \t\t\n\t\t    starti = endi;\n\t    print str(len(tags))+\" words tagged..\"\n\t# Save all the Noun and Adjective unigrams in a hash table\n\t    tag_set = {'Word':'Tag'}\n\t    for tag in tags:\n\t\t    if (self.cleanseNN([str(tag[1])]) in self.patterns[0:2]):\n\t\t\t    tag_set[str(tag[0])] = str(tag[1])\t\n\t\t#print cleanseNN([str(tag[1])])\n\t#print tags",
        "detail": "Demo.NLPutils",
        "documentation": {}
    },
    {
        "label": "\tngrams_set",
        "kind": 5,
        "importPath": "Demo.NLPutils",
        "description": "Demo.NLPutils",
        "peekOfCode": "\tngrams_set = set()\n\twith open('output.csv', 'w') as output:\n\t\tcsvwriter = csv.writer(output)\n\t\tcsvwriter.writerow([\"Data File\", \"Narrative\", \"N-grams\"])\n\t\tfor file in glob.glob(\"*.txt\"):\n\t\t\twith open(file, 'r') as reader:\n\t\t\t\ttext = ''\n\t\t\t\tfor line in reader:\n\t\t\t\t\ttext = text + line.rstrip('\\n\\r').lower()\n\t\t\t\tprint(\"\\nProcessing \"+file)",
        "detail": "Demo.NLPutils",
        "documentation": {}
    },
    {
        "label": "\t\tcsvwriter",
        "kind": 5,
        "importPath": "Demo.NLPutils",
        "description": "Demo.NLPutils",
        "peekOfCode": "\t\tcsvwriter = csv.writer(output)\n\t\tcsvwriter.writerow([\"Data File\", \"Narrative\", \"N-grams\"])\n\t\tfor file in glob.glob(\"*.txt\"):\n\t\t\twith open(file, 'r') as reader:\n\t\t\t\ttext = ''\n\t\t\t\tfor line in reader:\n\t\t\t\t\ttext = text + line.rstrip('\\n\\r').lower()\n\t\t\t\tprint(\"\\nProcessing \"+file)\n\t\t\t\t# Tokenization\n\t\t\t\ttokens = get_tokens(text)",
        "detail": "Demo.NLPutils",
        "documentation": {}
    },
    {
        "label": "\t\t\t\ttext",
        "kind": 5,
        "importPath": "Demo.NLPutils",
        "description": "Demo.NLPutils",
        "peekOfCode": "\t\t\t\ttext = ''\n\t\t\t\tfor line in reader:\n\t\t\t\t\ttext = text + line.rstrip('\\n\\r').lower()\n\t\t\t\tprint(\"\\nProcessing \"+file)\n\t\t\t\t# Tokenization\n\t\t\t\ttokens = get_tokens(text)\n\t\t\t\t# Part of speech Tagging\n\t\t\t\ttag_set = get_pos_tags(tokens)\n\t\t\t\t# Technical N-gram extraction\n\t\t\t\tngrams = get_tech_ngrams(text, tag_set)",
        "detail": "Demo.NLPutils",
        "documentation": {}
    },
    {
        "label": "\t\t\t\t\ttext",
        "kind": 5,
        "importPath": "Demo.NLPutils",
        "description": "Demo.NLPutils",
        "peekOfCode": "\t\t\t\t\ttext = text + line.rstrip('\\n\\r').lower()\n\t\t\t\tprint(\"\\nProcessing \"+file)\n\t\t\t\t# Tokenization\n\t\t\t\ttokens = get_tokens(text)\n\t\t\t\t# Part of speech Tagging\n\t\t\t\ttag_set = get_pos_tags(tokens)\n\t\t\t\t# Technical N-gram extraction\n\t\t\t\tngrams = get_tech_ngrams(text, tag_set)\n\t\t\t\t# Write to output\n\t\t\t\tcsvwriter.writerow([file, text, ngrams.keys()])",
        "detail": "Demo.NLPutils",
        "documentation": {}
    },
    {
        "label": "\t\t\t\ttokens",
        "kind": 5,
        "importPath": "Demo.NLPutils",
        "description": "Demo.NLPutils",
        "peekOfCode": "\t\t\t\ttokens = get_tokens(text)\n\t\t\t\t# Part of speech Tagging\n\t\t\t\ttag_set = get_pos_tags(tokens)\n\t\t\t\t# Technical N-gram extraction\n\t\t\t\tngrams = get_tech_ngrams(text, tag_set)\n\t\t\t\t# Write to output\n\t\t\t\tcsvwriter.writerow([file, text, ngrams.keys()])\n\t\t\t\tngrams_set = ngrams_set.union(set(ngrams.keys()))\n\tngrams_list = lexicon_expansion(list(ngrams_set))\n\twith open('ngrams.txt', 'w') as writer:",
        "detail": "Demo.NLPutils",
        "documentation": {}
    },
    {
        "label": "\t\t\t\ttag_set",
        "kind": 5,
        "importPath": "Demo.NLPutils",
        "description": "Demo.NLPutils",
        "peekOfCode": "\t\t\t\ttag_set = get_pos_tags(tokens)\n\t\t\t\t# Technical N-gram extraction\n\t\t\t\tngrams = get_tech_ngrams(text, tag_set)\n\t\t\t\t# Write to output\n\t\t\t\tcsvwriter.writerow([file, text, ngrams.keys()])\n\t\t\t\tngrams_set = ngrams_set.union(set(ngrams.keys()))\n\tngrams_list = lexicon_expansion(list(ngrams_set))\n\twith open('ngrams.txt', 'w') as writer:\n\t\tfor s in ngrams_list:\n\t\t\twriter.write(str(s)+'\\n')",
        "detail": "Demo.NLPutils",
        "documentation": {}
    },
    {
        "label": "\t\t\t\tngrams",
        "kind": 5,
        "importPath": "Demo.NLPutils",
        "description": "Demo.NLPutils",
        "peekOfCode": "\t\t\t\tngrams = get_tech_ngrams(text, tag_set)\n\t\t\t\t# Write to output\n\t\t\t\tcsvwriter.writerow([file, text, ngrams.keys()])\n\t\t\t\tngrams_set = ngrams_set.union(set(ngrams.keys()))\n\tngrams_list = lexicon_expansion(list(ngrams_set))\n\twith open('ngrams.txt', 'w') as writer:\n\t\tfor s in ngrams_list:\n\t\t\twriter.write(str(s)+'\\n')\nif __name__ == '__main__':\n    sys.exit(main())",
        "detail": "Demo.NLPutils",
        "documentation": {}
    },
    {
        "label": "\t\t\t\tngrams_set",
        "kind": 5,
        "importPath": "Demo.NLPutils",
        "description": "Demo.NLPutils",
        "peekOfCode": "\t\t\t\tngrams_set = ngrams_set.union(set(ngrams.keys()))\n\tngrams_list = lexicon_expansion(list(ngrams_set))\n\twith open('ngrams.txt', 'w') as writer:\n\t\tfor s in ngrams_list:\n\t\t\twriter.write(str(s)+'\\n')\nif __name__ == '__main__':\n    sys.exit(main())\n'''",
        "detail": "Demo.NLPutils",
        "documentation": {}
    },
    {
        "label": "\tngrams_list",
        "kind": 5,
        "importPath": "Demo.NLPutils",
        "description": "Demo.NLPutils",
        "peekOfCode": "\tngrams_list = lexicon_expansion(list(ngrams_set))\n\twith open('ngrams.txt', 'w') as writer:\n\t\tfor s in ngrams_list:\n\t\t\twriter.write(str(s)+'\\n')\nif __name__ == '__main__':\n    sys.exit(main())\n'''",
        "detail": "Demo.NLPutils",
        "documentation": {}
    },
    {
        "label": "Pipeline",
        "kind": 2,
        "importPath": "Demo.Pipeline",
        "description": "Demo.Pipeline",
        "peekOfCode": "def Pipeline(recording=pipeline_config.recording_name, videofile=pipeline_config.video_name, whisper_model=pipeline_config.whisper_model_sizes[0]):\n# Set the Google Speech API service-account key environment variable\n    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"service-account.json\"\n# ===== Create thread-safe queues to pass on transcript and communication between modules ============\n    SpeechToNLPQueue = queue.Queue()\n    EMSAgentQueue  = queue.Queue()\n    FeedbackQueue = queue.Queue()\n    VideoDataQueue = queue.Queue()\n    SpeechSignalQueue = queue.Queue()\n    # SpeechSignalQueue = Queue()",
        "detail": "Demo.Pipeline",
        "documentation": {}
    },
    {
        "label": "TextSpeech",
        "kind": 2,
        "importPath": "Demo.TextSpeechStream",
        "description": "Demo.TextSpeechStream",
        "peekOfCode": "def TextSpeech(Window, SpeechToNLPQueue, textfile_name):\n    print(\"Entered TextSpeech\")\n    # Create GUI Signal Object\n    SpeechSignal = GUISignal()\n    SpeechSignal.signal.connect(Window.UpdateSpeechBox)\n    MsgSignal = GUISignal()\n    MsgSignal.signal.connect(Window.UpdateMsgBox)\n    ButtonsSignal = GUISignal()\n    ButtonsSignal.signal.connect(Window.ButtonsSetEnabled)\n    Text_File = open(textfile_name, \"rb\") ",
        "detail": "Demo.TextSpeechStream",
        "documentation": {}
    },
    {
        "label": "BETWEEN_CHARACTERS_PAUSE",
        "kind": 5,
        "importPath": "Demo.TextSpeechStream",
        "description": "Demo.TextSpeechStream",
        "peekOfCode": "BETWEEN_CHARACTERS_PAUSE = .06\nBETWEEN_WORDS_PAUSE = .1\nBETWEEN_SENETENCES_PAUSE = 1\nCOMMA_PAUSE = .3\ndef TextSpeech(Window, SpeechToNLPQueue, textfile_name):\n    print(\"Entered TextSpeech\")\n    # Create GUI Signal Object\n    SpeechSignal = GUISignal()\n    SpeechSignal.signal.connect(Window.UpdateSpeechBox)\n    MsgSignal = GUISignal()",
        "detail": "Demo.TextSpeechStream",
        "documentation": {}
    },
    {
        "label": "BETWEEN_WORDS_PAUSE",
        "kind": 5,
        "importPath": "Demo.TextSpeechStream",
        "description": "Demo.TextSpeechStream",
        "peekOfCode": "BETWEEN_WORDS_PAUSE = .1\nBETWEEN_SENETENCES_PAUSE = 1\nCOMMA_PAUSE = .3\ndef TextSpeech(Window, SpeechToNLPQueue, textfile_name):\n    print(\"Entered TextSpeech\")\n    # Create GUI Signal Object\n    SpeechSignal = GUISignal()\n    SpeechSignal.signal.connect(Window.UpdateSpeechBox)\n    MsgSignal = GUISignal()\n    MsgSignal.signal.connect(Window.UpdateMsgBox)",
        "detail": "Demo.TextSpeechStream",
        "documentation": {}
    },
    {
        "label": "BETWEEN_SENETENCES_PAUSE",
        "kind": 5,
        "importPath": "Demo.TextSpeechStream",
        "description": "Demo.TextSpeechStream",
        "peekOfCode": "BETWEEN_SENETENCES_PAUSE = 1\nCOMMA_PAUSE = .3\ndef TextSpeech(Window, SpeechToNLPQueue, textfile_name):\n    print(\"Entered TextSpeech\")\n    # Create GUI Signal Object\n    SpeechSignal = GUISignal()\n    SpeechSignal.signal.connect(Window.UpdateSpeechBox)\n    MsgSignal = GUISignal()\n    MsgSignal.signal.connect(Window.UpdateMsgBox)\n    ButtonsSignal = GUISignal()",
        "detail": "Demo.TextSpeechStream",
        "documentation": {}
    },
    {
        "label": "COMMA_PAUSE",
        "kind": 5,
        "importPath": "Demo.TextSpeechStream",
        "description": "Demo.TextSpeechStream",
        "peekOfCode": "COMMA_PAUSE = .3\ndef TextSpeech(Window, SpeechToNLPQueue, textfile_name):\n    print(\"Entered TextSpeech\")\n    # Create GUI Signal Object\n    SpeechSignal = GUISignal()\n    SpeechSignal.signal.connect(Window.UpdateSpeechBox)\n    MsgSignal = GUISignal()\n    MsgSignal.signal.connect(Window.UpdateMsgBox)\n    ButtonsSignal = GUISignal()\n    ButtonsSignal.signal.connect(Window.ButtonsSetEnabled)",
        "detail": "Demo.TextSpeechStream",
        "documentation": {}
    },
    {
        "label": "VideoStream",
        "kind": 2,
        "importPath": "Demo.VideoFileStream",
        "description": "Demo.VideoFileStream",
        "peekOfCode": "def VideoStream(VideoDataQueue,VideoSignalQueue, video_file_name):\n    cap = cv2.VideoCapture(video_file_name) # video file path\n    # Check if camera opened successfully\n    if (cap.isOpened()== False): \n      print(\"[Video Stream Thread: Error opening video stream or file]\")\n    frame = np.array([])\n    fps = (cap.get(cv2.CAP_PROP_FPS))\n    idx = 0\n    # Read until video is completed\n    while(cap.isOpened()):",
        "detail": "Demo.VideoFileStream",
        "documentation": {}
    },
    {
        "label": "process_whisper_response",
        "kind": 2,
        "importPath": "Demo.WhisperFileStream",
        "description": "Demo.WhisperFileStream",
        "peekOfCode": "def process_whisper_response(response):\n    # used to remove the background noise transcriptions from Whisper output\n    # Remove strings enclosed within parentheses\n    response = re.sub(r'\\([^)]*\\)', '', response)\n    # Remove strings enclosed within asterisks\n    response = re.sub(r'\\*[^*]*\\*', '', response)\n    # Remove strings enclosed within brackets\n    response = re.sub(r'\\[[^\\]]*\\]', '', response)\n    # Remove null terminator since it does not display properly in Speech Box\n    response = response.replace(\"\\x00\", \" \")",
        "detail": "Demo.WhisperFileStream",
        "documentation": {}
    },
    {
        "label": "SDStream",
        "kind": 2,
        "importPath": "Demo.WhisperFileStream",
        "description": "Demo.WhisperFileStream",
        "peekOfCode": "def SDStream(SpeechSignalQueue,wavefile_name):\n    blocksize = 1024\n    buffersize = 20\n    device = len(sd.query_devices()) -1\n    q = queue.Queue(maxsize=20)\n    event = threading.Event()\n    print(\"Audio Streaming Started! \",wavefile_name)\n    SpeechSignalQueue.put(\"Proceed\")\n    def callback(outdata, frames, time, status):\n        assert frames == blocksize",
        "detail": "Demo.WhisperFileStream",
        "documentation": {}
    },
    {
        "label": "PyAudioStream",
        "kind": 2,
        "importPath": "Demo.WhisperFileStream",
        "description": "Demo.WhisperFileStream",
        "peekOfCode": "def PyAudioStream(SpeechSignalQueue,wavefile_name):\n    with wave.open(wavefile_name, 'rb') as wf:\n        try:\n            # Instantiate PyAudio and initialize PortAudio system resources (1)\n            p = pyaudio.PyAudio()\n            info = p.get_default_host_api_info()\n            device_index = info.get('deviceCount') - 1 # get default device as output device\n            stream = p.open(format = pyaudio.paInt16, channels = 1, rate = RATE, output = True, frames_per_buffer = CHUNK, output_device_index=device_index)\n            # Play samples from the wave file (3)\n            while len(data:=wf.readframes(CHUNK)):  # Requires Python 3.8+ for :=",
        "detail": "Demo.WhisperFileStream",
        "documentation": {}
    },
    {
        "label": "ReadPipe",
        "kind": 2,
        "importPath": "Demo.WhisperFileStream",
        "description": "Demo.WhisperFileStream",
        "peekOfCode": "def ReadPipe(SpeechToNLPQueue,VideoSignalQueue, SpeechSignalQueue):\n    fifo_path = \"/tmp/myfifo\"\n    finalized_blocks = ''\n    with open(fifo_path, 'r') as fifo:\n        try:\n            old_response = \"\"\n            # Play samples from the wave file (3)\n            if(not SpeechSignalQueue.empty()): signal = SpeechSignalQueue.get(block=False)\n            else: signal = \"Proceed\"\n            while signal != 'Kill':  # Requires Python 3.8+ for :=",
        "detail": "Demo.WhisperFileStream",
        "documentation": {}
    },
    {
        "label": "Whisper",
        "kind": 2,
        "importPath": "Demo.WhisperFileStream",
        "description": "Demo.WhisperFileStream",
        "peekOfCode": "def Whisper(SpeechToNLPQueue,VideoSignalQueue, wavefile_name):\n    fifo_path = \"/tmp/myfifo\"\n    finalized_blocks = ''\n    with open(fifo_path, 'r') as fifo:\n        with wave.open(wavefile_name, 'rb') as wf:\n            try:\n                # Instantiate PyAudio and initialize PortAudio system resources (1)\n                p = pyaudio.PyAudio()\n                info = p.get_default_host_api_info()\n                device_index = info.get('deviceCount') - 1 # get default device as output device",
        "detail": "Demo.WhisperFileStream",
        "documentation": {}
    },
    {
        "label": "RATE",
        "kind": 5,
        "importPath": "Demo.WhisperFileStream",
        "description": "Demo.WhisperFileStream",
        "peekOfCode": "RATE = 16000\nCHUNK = 1600  # 100ms\nif(pipeline_config.endtoendspv):\n    RATE = 44100\n    CHUNK = RATE//10\nSIGNAL = False\n'''\nThis method processes the whisper response we receive from fifo pipe \nThe response is type string, and is in the format 'block{isFinal,avg_p,latency}'\nWe remove background noise strings from the block (such as *crash* and [DOOR OPENS]) ",
        "detail": "Demo.WhisperFileStream",
        "documentation": {}
    },
    {
        "label": "CHUNK",
        "kind": 5,
        "importPath": "Demo.WhisperFileStream",
        "description": "Demo.WhisperFileStream",
        "peekOfCode": "CHUNK = 1600  # 100ms\nif(pipeline_config.endtoendspv):\n    RATE = 44100\n    CHUNK = RATE//10\nSIGNAL = False\n'''\nThis method processes the whisper response we receive from fifo pipe \nThe response is type string, and is in the format 'block{isFinal,avg_p,latency}'\nWe remove background noise strings from the block (such as *crash* and [DOOR OPENS]) \nAnd separate out the parts of response into block, isFinal, avg_p, and latency",
        "detail": "Demo.WhisperFileStream",
        "documentation": {}
    },
    {
        "label": "SIGNAL",
        "kind": 5,
        "importPath": "Demo.WhisperFileStream",
        "description": "Demo.WhisperFileStream",
        "peekOfCode": "SIGNAL = False\n'''\nThis method processes the whisper response we receive from fifo pipe \nThe response is type string, and is in the format 'block{isFinal,avg_p,latency}'\nWe remove background noise strings from the block (such as *crash* and [DOOR OPENS]) \nAnd separate out the parts of response into block, isFinal, avg_p, and latency\nblock : speech converted into text\nisFinal : '1' = the block is a final block. '0' = the block is a interim block\navg_p : 'float' of inclusive range [0,1]. the average confidence probability of all the tokens in the block\nlatency : latency of whisper streaming per block in milliseconds",
        "detail": "Demo.WhisperFileStream",
        "documentation": {}
    },
    {
        "label": "MicrophoneStream",
        "kind": 6,
        "importPath": "Demo.WhisperMicStream",
        "description": "Demo.WhisperMicStream",
        "peekOfCode": "class MicrophoneStream(object):\n    micSessionCounter = 0\n    audio_buffer = b\"\"\n    \"\"\"Opens a recording stream as a generator yielding the audio chunks.\"\"\"\n    def __init__(self, Window, rate, chunk):\n        MicrophoneStream.micSessionCounter += 1\n        self.Window = Window\n        self._rate = rate\n        self._chunk = chunk\n        self.samplesCounter = 0",
        "detail": "Demo.WhisperMicStream",
        "documentation": {}
    },
    {
        "label": "Whisper",
        "kind": 2,
        "importPath": "Demo.WhisperMicStream",
        "description": "Demo.WhisperMicStream",
        "peekOfCode": "def Whisper(Window, SpeechToNLPQueue):\n    # Create GUI Signal Object\n    SpeechSignal = GUISignal()\n    SpeechSignal.signal.connect(Window.UpdateSpeechBox)\n    MsgSignal = GUISignal()\n    MsgSignal.signal.connect(Window.UpdateMsgBox)\n    ButtonsSignal = GUISignal()\n    ButtonsSignal.signal.connect(Window.ButtonsSetEnabled)\n    whisper = WhisperAPI()\n    with MicrophoneStream(Window, RATE, CHUNK) as stream:",
        "detail": "Demo.WhisperMicStream",
        "documentation": {}
    },
    {
        "label": "RATE",
        "kind": 5,
        "importPath": "Demo.WhisperMicStream",
        "description": "Demo.WhisperMicStream",
        "peekOfCode": "RATE = 16000\nCHUNK = int(RATE / 10)  # 100ms\n#CHUNK = int(RATE * 5)  # 100ms\nclass MicrophoneStream(object):\n    micSessionCounter = 0\n    audio_buffer = b\"\"\n    \"\"\"Opens a recording stream as a generator yielding the audio chunks.\"\"\"\n    def __init__(self, Window, rate, chunk):\n        MicrophoneStream.micSessionCounter += 1\n        self.Window = Window",
        "detail": "Demo.WhisperMicStream",
        "documentation": {}
    },
    {
        "label": "CHUNK",
        "kind": 5,
        "importPath": "Demo.WhisperMicStream",
        "description": "Demo.WhisperMicStream",
        "peekOfCode": "CHUNK = int(RATE / 10)  # 100ms\n#CHUNK = int(RATE * 5)  # 100ms\nclass MicrophoneStream(object):\n    micSessionCounter = 0\n    audio_buffer = b\"\"\n    \"\"\"Opens a recording stream as a generator yielding the audio chunks.\"\"\"\n    def __init__(self, Window, rate, chunk):\n        MicrophoneStream.micSessionCounter += 1\n        self.Window = Window\n        self._rate = rate",
        "detail": "Demo.WhisperMicStream",
        "documentation": {}
    },
    {
        "label": "#CHUNK",
        "kind": 5,
        "importPath": "Demo.WhisperMicStream",
        "description": "Demo.WhisperMicStream",
        "peekOfCode": "#CHUNK = int(RATE * 5)  # 100ms\nclass MicrophoneStream(object):\n    micSessionCounter = 0\n    audio_buffer = b\"\"\n    \"\"\"Opens a recording stream as a generator yielding the audio chunks.\"\"\"\n    def __init__(self, Window, rate, chunk):\n        MicrophoneStream.micSessionCounter += 1\n        self.Window = Window\n        self._rate = rate\n        self._chunk = chunk",
        "detail": "Demo.WhisperMicStream",
        "documentation": {}
    },
    {
        "label": "dummy",
        "kind": 6,
        "importPath": "Demo.behaviours_m",
        "description": "Demo.behaviours_m",
        "peekOfCode": "class dummy(py_trees.behaviour.Behaviour):\n    def __init__(self, name):\n        super(dummy, self).__init__(name)\nclass PROTOCOLi_Check(py_trees.behaviour.Behaviour):\n    def __init__(self, name='PROTOCOLi Check'):\n        super(PROTOCOLi_Check, self).__init__(name)\nclass PROTOCOLi_Action(py_trees.behaviour.Behaviour):\n    def __init__(self, name='PROTOCOLi Action'):\n        super(PROTOCOLi_Action, self).__init__(name)\nblackboard = Blackboard()",
        "detail": "Demo.behaviours_m",
        "documentation": {}
    },
    {
        "label": "PROTOCOLi_Check",
        "kind": 6,
        "importPath": "Demo.behaviours_m",
        "description": "Demo.behaviours_m",
        "peekOfCode": "class PROTOCOLi_Check(py_trees.behaviour.Behaviour):\n    def __init__(self, name='PROTOCOLi Check'):\n        super(PROTOCOLi_Check, self).__init__(name)\nclass PROTOCOLi_Action(py_trees.behaviour.Behaviour):\n    def __init__(self, name='PROTOCOLi Action'):\n        super(PROTOCOLi_Action, self).__init__(name)\nblackboard = Blackboard()\n# behaviors in framework\nclass InformationGathering(py_trees.behaviour.Behaviour):\n    def __init__(self, name='Information Extraction',",
        "detail": "Demo.behaviours_m",
        "documentation": {}
    },
    {
        "label": "PROTOCOLi_Action",
        "kind": 6,
        "importPath": "Demo.behaviours_m",
        "description": "Demo.behaviours_m",
        "peekOfCode": "class PROTOCOLi_Action(py_trees.behaviour.Behaviour):\n    def __init__(self, name='PROTOCOLi Action'):\n        super(PROTOCOLi_Action, self).__init__(name)\nblackboard = Blackboard()\n# behaviors in framework\nclass InformationGathering(py_trees.behaviour.Behaviour):\n    def __init__(self, name='Information Extraction',\n                 slist=\"concept_list(s&s)_revised.csv\",\n                 vlist=\"Vital_List.csv\",\n                 exlist=\"CLfromVt.csv\",",
        "detail": "Demo.behaviours_m",
        "documentation": {}
    },
    {
        "label": "InformationGathering",
        "kind": 6,
        "importPath": "Demo.behaviours_m",
        "description": "Demo.behaviours_m",
        "peekOfCode": "class InformationGathering(py_trees.behaviour.Behaviour):\n    def __init__(self, name='Information Extraction',\n                 slist=\"concept_list(s&s)_revised.csv\",\n                 vlist=\"Vital_List.csv\",\n                 exlist=\"CLfromVt.csv\",\n                 intlist=\"concept_list(interventions).csv\"):\n        super(InformationGathering, self).__init__(name)\n        self.slist = slist\n        self.vlist = vlist\n        self.exlist = exlist",
        "detail": "Demo.behaviours_m",
        "documentation": {}
    },
    {
        "label": "Vectorize",
        "kind": 6,
        "importPath": "Demo.behaviours_m",
        "description": "Demo.behaviours_m",
        "peekOfCode": "class Vectorize(py_trees.behaviour.Behaviour):\n    def __init__(self, name='Vectorize', protocols='ODEMSA_Protocols_weighted.xlsx'):\n        super(Vectorize, self).__init__(name)\n        self.protocols = protocols\n    def setup(self, unused_timeout=15):\n        #blackboard = Blackboard()\n        global blackboard\n        PC = dict()\n        pro_df = pd.read_excel(self.protocols, engine=\"openpyxl\")\n        for line in pro_df.iterrows():",
        "detail": "Demo.behaviours_m",
        "documentation": {}
    },
    {
        "label": "ProtocolSelector",
        "kind": 6,
        "importPath": "Demo.behaviours_m",
        "description": "Demo.behaviours_m",
        "peekOfCode": "class ProtocolSelector(py_trees.behaviour.Behaviour):\n    def __init__(self, name='Protocol Selector'):\n        super(ProtocolSelector, self).__init__(name)\n    def setup(self, unused_timeout=15):\n        # blackboard = Blackboard()\n        global blackboard\n        blackboard.protocol_flag = dict()\n        blackboard.feedback = dict()\n        for i in blackboard.PV:\n            blackboard.protocol_flag[i] = (False, 0.)",
        "detail": "Demo.behaviours_m",
        "documentation": {}
    },
    {
        "label": "TextCollection",
        "kind": 6,
        "importPath": "Demo.behaviours_m",
        "description": "Demo.behaviours_m",
        "peekOfCode": "class TextCollection(py_trees.behaviour.Behaviour):\n    def __init__(self, name='Speech To Text Conversion'):\n        super(TextCollection, self).__init__(name)\n    def setup(self, unused_timeout=15):\n        level = 'I/P'\n        #blackboard = Blackboard()\n        global blackboard\n        #blackboard.action = []\n        blackboard.level = level\n        blackboard.tick_num = 0",
        "detail": "Demo.behaviours_m",
        "documentation": {}
    },
    {
        "label": "Chestpain_Checker",
        "kind": 6,
        "importPath": "Demo.behaviours_m",
        "description": "Demo.behaviours_m",
        "peekOfCode": "class Chestpain_Checker(py_trees.behaviour.Behaviour):\n    def __init__(self, name='Chestpain_Checker'):\n        super(Chestpain_Checker, self).__init__(name)\n        p = pd.read_excel('ODEMSA_Protocols_weighted.xlsx', engine=\"openpyxl\")\n        self.key = p['Protocol'][0]\n    def update(self):\n        #blackboard = Blackboard()\n        global blackboard\n        if blackboard.protocol_flag[self.key][0]:\n            return py_trees.common.Status.SUCCESS",
        "detail": "Demo.behaviours_m",
        "documentation": {}
    },
    {
        "label": "Abdopain_Checker",
        "kind": 6,
        "importPath": "Demo.behaviours_m",
        "description": "Demo.behaviours_m",
        "peekOfCode": "class Abdopain_Checker(py_trees.behaviour.Behaviour):\n    def __init__(self, name='Abdopain_Checker'):\n        super(Abdopain_Checker, self).__init__(name)\n        p = pd.read_excel('ODEMSA_Protocols_weighted.xlsx', engine=\"openpyxl\")\n        self.key = p['Protocol'][1]\n    def update(self):\n        #blackboard = Blackboard()\n        global blackboard\n        if blackboard.protocol_flag[self.key][0]:\n            return py_trees.common.Status.SUCCESS",
        "detail": "Demo.behaviours_m",
        "documentation": {}
    },
    {
        "label": "Behavioral_Checker",
        "kind": 6,
        "importPath": "Demo.behaviours_m",
        "description": "Demo.behaviours_m",
        "peekOfCode": "class Behavioral_Checker(py_trees.behaviour.Behaviour):\n    def __init__(self, name='Behavioral_Checker'):\n        super(Behavioral_Checker, self).__init__(name)\n        p = pd.read_excel('ODEMSA_Protocols_weighted.xlsx', engine=\"openpyxl\")\n        self.key = p['Protocol'][2]\n    def update(self):\n        #blackboard = Blackboard()\n        global blackboard\n        if blackboard.protocol_flag[self.key][0]:\n            return py_trees.common.Status.SUCCESS",
        "detail": "Demo.behaviours_m",
        "documentation": {}
    },
    {
        "label": "Seizure_Checker",
        "kind": 6,
        "importPath": "Demo.behaviours_m",
        "description": "Demo.behaviours_m",
        "peekOfCode": "class Seizure_Checker(py_trees.behaviour.Behaviour):\n    def __init__(self, name='Seizure_Checker'):\n        super(Seizure_Checker, self).__init__(name)\n        p = pd.read_excel('ODEMSA_Protocols_weighted.xlsx', engine=\"openpyxl\")\n        self.key = p['Protocol'][3]\n    def update(self):\n        #blackboard = Blackboard()\n        global blackboard\n        if blackboard.protocol_flag[self.key][0]:\n            return py_trees.common.Status.SUCCESS",
        "detail": "Demo.behaviours_m",
        "documentation": {}
    },
    {
        "label": "resp_Checker",
        "kind": 6,
        "importPath": "Demo.behaviours_m",
        "description": "Demo.behaviours_m",
        "peekOfCode": "class resp_Checker(py_trees.behaviour.Behaviour):\n    def __init__(self, name='resp_Checker'):\n        super(resp_Checker, self).__init__(name)\n        p = pd.read_excel('ODEMSA_Protocols_weighted.xlsx', engine=\"openpyxl\")\n        self.key = p['Protocol'][4]\n    def update(self):\n        #blackboard = Blackboard()\n        global blackboard\n        if blackboard.protocol_flag[self.key][0]:\n            return py_trees.common.Status.SUCCESS",
        "detail": "Demo.behaviours_m",
        "documentation": {}
    },
    {
        "label": "AMS_Checker",
        "kind": 6,
        "importPath": "Demo.behaviours_m",
        "description": "Demo.behaviours_m",
        "peekOfCode": "class AMS_Checker(py_trees.behaviour.Behaviour):\n    def __init__(self, name='AMS_Checker'):\n        super(AMS_Checker, self).__init__(name)\n        p = pd.read_excel('ODEMSA_Protocols_weighted.xlsx', engine=\"openpyxl\")\n        self.key = p['Protocol'][5]\n    def update(self):\n        #blackboard = Blackboard()\n        global blackboard\n        if blackboard.protocol_flag[self.key][0]:\n            return py_trees.common.Status.SUCCESS",
        "detail": "Demo.behaviours_m",
        "documentation": {}
    },
    {
        "label": "Diab_Checker",
        "kind": 6,
        "importPath": "Demo.behaviours_m",
        "description": "Demo.behaviours_m",
        "peekOfCode": "class Diab_Checker(py_trees.behaviour.Behaviour):\n    def __init__(self, name='Diab_Checker'):\n        super(Diab_Checker, self).__init__(name)\n        p = pd.read_excel('ODEMSA_Protocols_weighted.xlsx', engine=\"openpyxl\")\n        self.key = p['Protocol'][6]\n    def update(self):\n        #blackboard = Blackboard()\n        global blackboard\n        if blackboard.protocol_flag[self.key][0]:\n            return py_trees.common.Status.SUCCESS",
        "detail": "Demo.behaviours_m",
        "documentation": {}
    },
    {
        "label": "Overdose_Checker",
        "kind": 6,
        "importPath": "Demo.behaviours_m",
        "description": "Demo.behaviours_m",
        "peekOfCode": "class Overdose_Checker(py_trees.behaviour.Behaviour):\n    def __init__(self, name='Overdose_Checker'):\n        super(Overdose_Checker, self).__init__(name)\n        p = pd.read_excel('ODEMSA_Protocols_weighted.xlsx', engine=\"openpyxl\")\n        self.key = p['Protocol'][7]\n    def update(self):\n        #blackboard = Blackboard()\n        global blackboard\n        if blackboard.protocol_flag[self.key][0]:\n            return py_trees.common.Status.SUCCESS",
        "detail": "Demo.behaviours_m",
        "documentation": {}
    },
    {
        "label": "Chestpain",
        "kind": 6,
        "importPath": "Demo.behaviours_m",
        "description": "Demo.behaviours_m",
        "peekOfCode": "class Chestpain(py_trees.behaviour.Behaviour):\n    def __init__(self, name='Chestpain'):\n        super(Chestpain, self).__init__(name)\n        p = pd.read_excel('ODEMSA_Protocols_weighted.xlsx', engine=\"openpyxl\")\n        self.key = p['Protocol'][0]\n    def update(self):\n        #blackboard = Blackboard()\n        global blackboard\n        self.posi = blackboard.protocol_flag[self.key][1]\n        if self.posi == 0:",
        "detail": "Demo.behaviours_m",
        "documentation": {}
    },
    {
        "label": "Abdopain",
        "kind": 6,
        "importPath": "Demo.behaviours_m",
        "description": "Demo.behaviours_m",
        "peekOfCode": "class Abdopain(py_trees.behaviour.Behaviour):\n    def __init__(self, name='Abdopain'):\n        super(Abdopain, self).__init__(name)\n        p = pd.read_excel('ODEMSA_Protocols_weighted.xlsx', engine=\"openpyxl\")\n        self.key = p['Protocol'][1]\n    def update(self):\n        #blackboard = Blackboard()\n        global blackboard\n        self.posi = blackboard.protocol_flag[self.key][1]\n        if self.posi == 0:",
        "detail": "Demo.behaviours_m",
        "documentation": {}
    },
    {
        "label": "Behavioral",
        "kind": 6,
        "importPath": "Demo.behaviours_m",
        "description": "Demo.behaviours_m",
        "peekOfCode": "class Behavioral(py_trees.behaviour.Behaviour):\n    def __init__(self, name='Behavioral_Checker'):\n        super(Behavioral, self).__init__(name)\n        p = pd.read_excel('ODEMSA_Protocols_weighted.xlsx', engine=\"openpyxl\")\n        self.key = p['Protocol'][2]\n    def update(self):\n        #blackboard = Blackboard()\n        global blackboard\n        self.posi = blackboard.protocol_flag[self.key][1]\n        if self.posi == 0:",
        "detail": "Demo.behaviours_m",
        "documentation": {}
    },
    {
        "label": "Seizure",
        "kind": 6,
        "importPath": "Demo.behaviours_m",
        "description": "Demo.behaviours_m",
        "peekOfCode": "class Seizure(py_trees.behaviour.Behaviour):\n    def __init__(self, name='Seizure'):\n        super(Seizure, self).__init__(name)\n        p = pd.read_excel('ODEMSA_Protocols_weighted.xlsx', engine=\"openpyxl\")\n        self.key = p['Protocol'][3]\n    def update(self):\n        #blackboard = Blackboard()\n        global blackboard\n        self.posi = blackboard.protocol_flag[self.key][1]\n        if self.posi == 0:",
        "detail": "Demo.behaviours_m",
        "documentation": {}
    },
    {
        "label": "resp",
        "kind": 6,
        "importPath": "Demo.behaviours_m",
        "description": "Demo.behaviours_m",
        "peekOfCode": "class resp(py_trees.behaviour.Behaviour):\n    '''\n    Acute respiratory distress syndrome:\n    sob: shortness of breath\n    rapid breathing: tachypnea\n    low bp: hypotension\n    confusion: confusion\n    '''\n    def __init__(self, name='resp'):\n        super(resp, self).__init__(name)",
        "detail": "Demo.behaviours_m",
        "documentation": {}
    },
    {
        "label": "AMS",
        "kind": 6,
        "importPath": "Demo.behaviours_m",
        "description": "Demo.behaviours_m",
        "peekOfCode": "class AMS(py_trees.behaviour.Behaviour):\n    def __init__(self, name='AMS'):\n        super(AMS, self).__init__(name)\n        p = pd.read_excel('ODEMSA_Protocols_weighted.xlsx', engine=\"openpyxl\")\n        self.key = p['Protocol'][5]\n    def update(self):\n        #blackboard = Blackboard()\n        global blackboard\n        self.posi = blackboard.protocol_flag[self.key][1]\n        if self.posi == 0:",
        "detail": "Demo.behaviours_m",
        "documentation": {}
    },
    {
        "label": "Diab",
        "kind": 6,
        "importPath": "Demo.behaviours_m",
        "description": "Demo.behaviours_m",
        "peekOfCode": "class Diab(py_trees.behaviour.Behaviour):\n    def __init__(self, name='Diab'):\n        super(Diab, self).__init__(name)\n        p = pd.read_excel('ODEMSA_Protocols_weighted.xlsx', engine=\"openpyxl\")\n        self.key = p['Protocol'][6]\n    def update(self):\n        #blackboard = Blackboard()\n        global blackboard\n        self.posi = blackboard.protocol_flag[self.key][1]\n        if self.posi == 0:",
        "detail": "Demo.behaviours_m",
        "documentation": {}
    },
    {
        "label": "Overdose",
        "kind": 6,
        "importPath": "Demo.behaviours_m",
        "description": "Demo.behaviours_m",
        "peekOfCode": "class Overdose(py_trees.behaviour.Behaviour):\n    def __init__(self, name='Overdose'):\n        super(Overdose, self).__init__(name)\n        p = pd.read_excel('ODEMSA_Protocols_weighted.xlsx', engine=\"openpyxl\")\n        self.key = p['Protocol'][7]\n    def update(self):\n        #blackboard = Blackboard()\n        global blackboard\n        self.posi = blackboard.protocol_flag[self.key][1]\n        if self.posi == 0:",
        "detail": "Demo.behaviours_m",
        "documentation": {}
    },
    {
        "label": "softmax",
        "kind": 2,
        "importPath": "Demo.behaviours_m",
        "description": "Demo.behaviours_m",
        "peekOfCode": "def softmax(x):\n    return np.exp(x) / np.sum(np.exp(x), axis=0)\n# dummy leaves\nclass dummy(py_trees.behaviour.Behaviour):\n    def __init__(self, name):\n        super(dummy, self).__init__(name)\nclass PROTOCOLi_Check(py_trees.behaviour.Behaviour):\n    def __init__(self, name='PROTOCOLi Check'):\n        super(PROTOCOLi_Check, self).__init__(name)\nclass PROTOCOLi_Action(py_trees.behaviour.Behaviour):",
        "detail": "Demo.behaviours_m",
        "documentation": {}
    },
    {
        "label": "blackboard",
        "kind": 5,
        "importPath": "Demo.behaviours_m",
        "description": "Demo.behaviours_m",
        "peekOfCode": "blackboard = Blackboard()\n# behaviors in framework\nclass InformationGathering(py_trees.behaviour.Behaviour):\n    def __init__(self, name='Information Extraction',\n                 slist=\"concept_list(s&s)_revised.csv\",\n                 vlist=\"Vital_List.csv\",\n                 exlist=\"CLfromVt.csv\",\n                 intlist=\"concept_list(interventions).csv\"):\n        super(InformationGathering, self).__init__(name)\n        self.slist = slist",
        "detail": "Demo.behaviours_m",
        "documentation": {}
    },
    {
        "label": "CP_C",
        "kind": 5,
        "importPath": "Demo.behaviours_m",
        "description": "Demo.behaviours_m",
        "peekOfCode": "CP_C = Chestpain_Checker()\nCP_A = Chestpain()\nCP = py_trees.composites.Sequence(\"Chestpain\", children=[CP_C, CP_A])\nclass Abdopain(py_trees.behaviour.Behaviour):\n    def __init__(self, name='Abdopain'):\n        super(Abdopain, self).__init__(name)\n        p = pd.read_excel('ODEMSA_Protocols_weighted.xlsx', engine=\"openpyxl\")\n        self.key = p['Protocol'][1]\n    def update(self):\n        #blackboard = Blackboard()",
        "detail": "Demo.behaviours_m",
        "documentation": {}
    },
    {
        "label": "CP_A",
        "kind": 5,
        "importPath": "Demo.behaviours_m",
        "description": "Demo.behaviours_m",
        "peekOfCode": "CP_A = Chestpain()\nCP = py_trees.composites.Sequence(\"Chestpain\", children=[CP_C, CP_A])\nclass Abdopain(py_trees.behaviour.Behaviour):\n    def __init__(self, name='Abdopain'):\n        super(Abdopain, self).__init__(name)\n        p = pd.read_excel('ODEMSA_Protocols_weighted.xlsx', engine=\"openpyxl\")\n        self.key = p['Protocol'][1]\n    def update(self):\n        #blackboard = Blackboard()\n        global blackboard",
        "detail": "Demo.behaviours_m",
        "documentation": {}
    },
    {
        "label": "CP",
        "kind": 5,
        "importPath": "Demo.behaviours_m",
        "description": "Demo.behaviours_m",
        "peekOfCode": "CP = py_trees.composites.Sequence(\"Chestpain\", children=[CP_C, CP_A])\nclass Abdopain(py_trees.behaviour.Behaviour):\n    def __init__(self, name='Abdopain'):\n        super(Abdopain, self).__init__(name)\n        p = pd.read_excel('ODEMSA_Protocols_weighted.xlsx', engine=\"openpyxl\")\n        self.key = p['Protocol'][1]\n    def update(self):\n        #blackboard = Blackboard()\n        global blackboard\n        self.posi = blackboard.protocol_flag[self.key][1]",
        "detail": "Demo.behaviours_m",
        "documentation": {}
    },
    {
        "label": "AP_C",
        "kind": 5,
        "importPath": "Demo.behaviours_m",
        "description": "Demo.behaviours_m",
        "peekOfCode": "AP_C = Abdopain_Checker()\nAP_A = Abdopain()\nAP = py_trees.composites.Sequence(\"Abdopain\", children=[AP_C, AP_A])\nclass Behavioral(py_trees.behaviour.Behaviour):\n    def __init__(self, name='Behavioral_Checker'):\n        super(Behavioral, self).__init__(name)\n        p = pd.read_excel('ODEMSA_Protocols_weighted.xlsx', engine=\"openpyxl\")\n        self.key = p['Protocol'][2]\n    def update(self):\n        #blackboard = Blackboard()",
        "detail": "Demo.behaviours_m",
        "documentation": {}
    },
    {
        "label": "AP_A",
        "kind": 5,
        "importPath": "Demo.behaviours_m",
        "description": "Demo.behaviours_m",
        "peekOfCode": "AP_A = Abdopain()\nAP = py_trees.composites.Sequence(\"Abdopain\", children=[AP_C, AP_A])\nclass Behavioral(py_trees.behaviour.Behaviour):\n    def __init__(self, name='Behavioral_Checker'):\n        super(Behavioral, self).__init__(name)\n        p = pd.read_excel('ODEMSA_Protocols_weighted.xlsx', engine=\"openpyxl\")\n        self.key = p['Protocol'][2]\n    def update(self):\n        #blackboard = Blackboard()\n        global blackboard",
        "detail": "Demo.behaviours_m",
        "documentation": {}
    },
    {
        "label": "AP",
        "kind": 5,
        "importPath": "Demo.behaviours_m",
        "description": "Demo.behaviours_m",
        "peekOfCode": "AP = py_trees.composites.Sequence(\"Abdopain\", children=[AP_C, AP_A])\nclass Behavioral(py_trees.behaviour.Behaviour):\n    def __init__(self, name='Behavioral_Checker'):\n        super(Behavioral, self).__init__(name)\n        p = pd.read_excel('ODEMSA_Protocols_weighted.xlsx', engine=\"openpyxl\")\n        self.key = p['Protocol'][2]\n    def update(self):\n        #blackboard = Blackboard()\n        global blackboard\n        self.posi = blackboard.protocol_flag[self.key][1]",
        "detail": "Demo.behaviours_m",
        "documentation": {}
    },
    {
        "label": "BE_C",
        "kind": 5,
        "importPath": "Demo.behaviours_m",
        "description": "Demo.behaviours_m",
        "peekOfCode": "BE_C = Behavioral_Checker()\nBE_A = Behavioral()\nBE = py_trees.composites.Sequence(\"Behavioral\", children=[BE_C, BE_A])\nclass Seizure(py_trees.behaviour.Behaviour):\n    def __init__(self, name='Seizure'):\n        super(Seizure, self).__init__(name)\n        p = pd.read_excel('ODEMSA_Protocols_weighted.xlsx', engine=\"openpyxl\")\n        self.key = p['Protocol'][3]\n    def update(self):\n        #blackboard = Blackboard()",
        "detail": "Demo.behaviours_m",
        "documentation": {}
    },
    {
        "label": "BE_A",
        "kind": 5,
        "importPath": "Demo.behaviours_m",
        "description": "Demo.behaviours_m",
        "peekOfCode": "BE_A = Behavioral()\nBE = py_trees.composites.Sequence(\"Behavioral\", children=[BE_C, BE_A])\nclass Seizure(py_trees.behaviour.Behaviour):\n    def __init__(self, name='Seizure'):\n        super(Seizure, self).__init__(name)\n        p = pd.read_excel('ODEMSA_Protocols_weighted.xlsx', engine=\"openpyxl\")\n        self.key = p['Protocol'][3]\n    def update(self):\n        #blackboard = Blackboard()\n        global blackboard",
        "detail": "Demo.behaviours_m",
        "documentation": {}
    },
    {
        "label": "BE",
        "kind": 5,
        "importPath": "Demo.behaviours_m",
        "description": "Demo.behaviours_m",
        "peekOfCode": "BE = py_trees.composites.Sequence(\"Behavioral\", children=[BE_C, BE_A])\nclass Seizure(py_trees.behaviour.Behaviour):\n    def __init__(self, name='Seizure'):\n        super(Seizure, self).__init__(name)\n        p = pd.read_excel('ODEMSA_Protocols_weighted.xlsx', engine=\"openpyxl\")\n        self.key = p['Protocol'][3]\n    def update(self):\n        #blackboard = Blackboard()\n        global blackboard\n        self.posi = blackboard.protocol_flag[self.key][1]",
        "detail": "Demo.behaviours_m",
        "documentation": {}
    },
    {
        "label": "SZ_C",
        "kind": 5,
        "importPath": "Demo.behaviours_m",
        "description": "Demo.behaviours_m",
        "peekOfCode": "SZ_C = Seizure_Checker()\nSZ_A = Seizure()\nSZ = py_trees.composites.Sequence(\"Seizure\", children=[SZ_C, SZ_A])\nclass resp(py_trees.behaviour.Behaviour):\n    '''\n    Acute respiratory distress syndrome:\n    sob: shortness of breath\n    rapid breathing: tachypnea\n    low bp: hypotension\n    confusion: confusion",
        "detail": "Demo.behaviours_m",
        "documentation": {}
    },
    {
        "label": "SZ_A",
        "kind": 5,
        "importPath": "Demo.behaviours_m",
        "description": "Demo.behaviours_m",
        "peekOfCode": "SZ_A = Seizure()\nSZ = py_trees.composites.Sequence(\"Seizure\", children=[SZ_C, SZ_A])\nclass resp(py_trees.behaviour.Behaviour):\n    '''\n    Acute respiratory distress syndrome:\n    sob: shortness of breath\n    rapid breathing: tachypnea\n    low bp: hypotension\n    confusion: confusion\n    '''",
        "detail": "Demo.behaviours_m",
        "documentation": {}
    },
    {
        "label": "SZ",
        "kind": 5,
        "importPath": "Demo.behaviours_m",
        "description": "Demo.behaviours_m",
        "peekOfCode": "SZ = py_trees.composites.Sequence(\"Seizure\", children=[SZ_C, SZ_A])\nclass resp(py_trees.behaviour.Behaviour):\n    '''\n    Acute respiratory distress syndrome:\n    sob: shortness of breath\n    rapid breathing: tachypnea\n    low bp: hypotension\n    confusion: confusion\n    '''\n    def __init__(self, name='resp'):",
        "detail": "Demo.behaviours_m",
        "documentation": {}
    },
    {
        "label": "RE_C",
        "kind": 5,
        "importPath": "Demo.behaviours_m",
        "description": "Demo.behaviours_m",
        "peekOfCode": "RE_C = resp_Checker()\nRE_A = resp()\nRE = py_trees.composites.Sequence(\"resp\", children=[RE_C, RE_A])\nclass AMS(py_trees.behaviour.Behaviour):\n    def __init__(self, name='AMS'):\n        super(AMS, self).__init__(name)\n        p = pd.read_excel('ODEMSA_Protocols_weighted.xlsx', engine=\"openpyxl\")\n        self.key = p['Protocol'][5]\n    def update(self):\n        #blackboard = Blackboard()",
        "detail": "Demo.behaviours_m",
        "documentation": {}
    },
    {
        "label": "RE_A",
        "kind": 5,
        "importPath": "Demo.behaviours_m",
        "description": "Demo.behaviours_m",
        "peekOfCode": "RE_A = resp()\nRE = py_trees.composites.Sequence(\"resp\", children=[RE_C, RE_A])\nclass AMS(py_trees.behaviour.Behaviour):\n    def __init__(self, name='AMS'):\n        super(AMS, self).__init__(name)\n        p = pd.read_excel('ODEMSA_Protocols_weighted.xlsx', engine=\"openpyxl\")\n        self.key = p['Protocol'][5]\n    def update(self):\n        #blackboard = Blackboard()\n        global blackboard",
        "detail": "Demo.behaviours_m",
        "documentation": {}
    },
    {
        "label": "RE",
        "kind": 5,
        "importPath": "Demo.behaviours_m",
        "description": "Demo.behaviours_m",
        "peekOfCode": "RE = py_trees.composites.Sequence(\"resp\", children=[RE_C, RE_A])\nclass AMS(py_trees.behaviour.Behaviour):\n    def __init__(self, name='AMS'):\n        super(AMS, self).__init__(name)\n        p = pd.read_excel('ODEMSA_Protocols_weighted.xlsx', engine=\"openpyxl\")\n        self.key = p['Protocol'][5]\n    def update(self):\n        #blackboard = Blackboard()\n        global blackboard\n        self.posi = blackboard.protocol_flag[self.key][1]",
        "detail": "Demo.behaviours_m",
        "documentation": {}
    },
    {
        "label": "AMS_C",
        "kind": 5,
        "importPath": "Demo.behaviours_m",
        "description": "Demo.behaviours_m",
        "peekOfCode": "AMS_C = AMS_Checker()\nAMS_A = AMS()\nAMS = py_trees.composites.Sequence(\"resp\", children=[AMS_C, AMS_A])\nclass Diab(py_trees.behaviour.Behaviour):\n    def __init__(self, name='Diab'):\n        super(Diab, self).__init__(name)\n        p = pd.read_excel('ODEMSA_Protocols_weighted.xlsx', engine=\"openpyxl\")\n        self.key = p['Protocol'][6]\n    def update(self):\n        #blackboard = Blackboard()",
        "detail": "Demo.behaviours_m",
        "documentation": {}
    },
    {
        "label": "AMS_A",
        "kind": 5,
        "importPath": "Demo.behaviours_m",
        "description": "Demo.behaviours_m",
        "peekOfCode": "AMS_A = AMS()\nAMS = py_trees.composites.Sequence(\"resp\", children=[AMS_C, AMS_A])\nclass Diab(py_trees.behaviour.Behaviour):\n    def __init__(self, name='Diab'):\n        super(Diab, self).__init__(name)\n        p = pd.read_excel('ODEMSA_Protocols_weighted.xlsx', engine=\"openpyxl\")\n        self.key = p['Protocol'][6]\n    def update(self):\n        #blackboard = Blackboard()\n        global blackboard",
        "detail": "Demo.behaviours_m",
        "documentation": {}
    },
    {
        "label": "AMS",
        "kind": 5,
        "importPath": "Demo.behaviours_m",
        "description": "Demo.behaviours_m",
        "peekOfCode": "AMS = py_trees.composites.Sequence(\"resp\", children=[AMS_C, AMS_A])\nclass Diab(py_trees.behaviour.Behaviour):\n    def __init__(self, name='Diab'):\n        super(Diab, self).__init__(name)\n        p = pd.read_excel('ODEMSA_Protocols_weighted.xlsx', engine=\"openpyxl\")\n        self.key = p['Protocol'][6]\n    def update(self):\n        #blackboard = Blackboard()\n        global blackboard\n        self.posi = blackboard.protocol_flag[self.key][1]",
        "detail": "Demo.behaviours_m",
        "documentation": {}
    },
    {
        "label": "DI_C",
        "kind": 5,
        "importPath": "Demo.behaviours_m",
        "description": "Demo.behaviours_m",
        "peekOfCode": "DI_C = Diab_Checker()\nDI_A = Diab()\nDI = py_trees.composites.Sequence(\"Diab\", children=[DI_C, DI_A])\nclass Overdose(py_trees.behaviour.Behaviour):\n    def __init__(self, name='Overdose'):\n        super(Overdose, self).__init__(name)\n        p = pd.read_excel('ODEMSA_Protocols_weighted.xlsx', engine=\"openpyxl\")\n        self.key = p['Protocol'][7]\n    def update(self):\n        #blackboard = Blackboard()",
        "detail": "Demo.behaviours_m",
        "documentation": {}
    },
    {
        "label": "DI_A",
        "kind": 5,
        "importPath": "Demo.behaviours_m",
        "description": "Demo.behaviours_m",
        "peekOfCode": "DI_A = Diab()\nDI = py_trees.composites.Sequence(\"Diab\", children=[DI_C, DI_A])\nclass Overdose(py_trees.behaviour.Behaviour):\n    def __init__(self, name='Overdose'):\n        super(Overdose, self).__init__(name)\n        p = pd.read_excel('ODEMSA_Protocols_weighted.xlsx', engine=\"openpyxl\")\n        self.key = p['Protocol'][7]\n    def update(self):\n        #blackboard = Blackboard()\n        global blackboard",
        "detail": "Demo.behaviours_m",
        "documentation": {}
    },
    {
        "label": "DI",
        "kind": 5,
        "importPath": "Demo.behaviours_m",
        "description": "Demo.behaviours_m",
        "peekOfCode": "DI = py_trees.composites.Sequence(\"Diab\", children=[DI_C, DI_A])\nclass Overdose(py_trees.behaviour.Behaviour):\n    def __init__(self, name='Overdose'):\n        super(Overdose, self).__init__(name)\n        p = pd.read_excel('ODEMSA_Protocols_weighted.xlsx', engine=\"openpyxl\")\n        self.key = p['Protocol'][7]\n    def update(self):\n        #blackboard = Blackboard()\n        global blackboard\n        self.posi = blackboard.protocol_flag[self.key][1]",
        "detail": "Demo.behaviours_m",
        "documentation": {}
    },
    {
        "label": "OV_C",
        "kind": 5,
        "importPath": "Demo.behaviours_m",
        "description": "Demo.behaviours_m",
        "peekOfCode": "OV_C = Overdose_Checker()\nOV_A = Overdose()\nOV = py_trees.composites.Sequence(\"Diab\", children=[OV_C, OV_A])\nprotocols = py_trees.composites.Parallel('protocols',\n                                         children=[CP, OV, DI, AMS, RE, SZ, BE, AP])",
        "detail": "Demo.behaviours_m",
        "documentation": {}
    },
    {
        "label": "OV_A",
        "kind": 5,
        "importPath": "Demo.behaviours_m",
        "description": "Demo.behaviours_m",
        "peekOfCode": "OV_A = Overdose()\nOV = py_trees.composites.Sequence(\"Diab\", children=[OV_C, OV_A])\nprotocols = py_trees.composites.Parallel('protocols',\n                                         children=[CP, OV, DI, AMS, RE, SZ, BE, AP])",
        "detail": "Demo.behaviours_m",
        "documentation": {}
    },
    {
        "label": "OV",
        "kind": 5,
        "importPath": "Demo.behaviours_m",
        "description": "Demo.behaviours_m",
        "peekOfCode": "OV = py_trees.composites.Sequence(\"Diab\", children=[OV_C, OV_A])\nprotocols = py_trees.composites.Parallel('protocols',\n                                         children=[CP, OV, DI, AMS, RE, SZ, BE, AP])",
        "detail": "Demo.behaviours_m",
        "documentation": {}
    },
    {
        "label": "protocols",
        "kind": 5,
        "importPath": "Demo.behaviours_m",
        "description": "Demo.behaviours_m",
        "peekOfCode": "protocols = py_trees.composites.Parallel('protocols',\n                                         children=[CP, OV, DI, AMS, RE, SZ, BE, AP])",
        "detail": "Demo.behaviours_m",
        "documentation": {}
    },
    {
        "label": "EKGdic",
        "kind": 5,
        "importPath": "Demo.bt_parameters",
        "description": "Demo.bt_parameters",
        "peekOfCode": "EKGdic = {\n     '':'',\n     'AV_Block_1st_Deg':'AV_Block-1st_Degree',\n     'AV_Block_1st_Degree':'AV_Block-1st_Degree',\n     'AV_Block_2nd_Degree_Type_1':'AV_Block_2nd_Degree_Type_1',\n     'AV_Block_2nd_Degree_Type_2':'AV_Block_2nd_Degree_Type_2',\n     'AV_Block_3rd_Degree':'AV_Block_3rd_Degree',\n     'Asystole':'Asystole',\n     'Artifact':'Artifact',\n     'Atrial_Fibrill':'Atrial_Fibrillation',",
        "detail": "Demo.bt_parameters",
        "documentation": {}
    },
    {
        "label": "pool",
        "kind": 5,
        "importPath": "Demo.bt_parameters",
        "description": "Demo.bt_parameters",
        "peekOfCode": "pool = set(['Medical - Abdominal Pain',\n            'Medical - Altered Mental Status',\n            'Medical - Seizure',\n            'Medical - Respiratory Distress/Asthma/COPD/Croup/Reactive Airway',\n            'General - Behavioral/Patient Restraint',\n            'Medical - Overdose/Poisoning - Opioid',\n            'Medical - Diabetic - Hypoglycemia',\n            'Medical - Chest Pain - Cardiac Suspected'])\nFN2H = {\n '': 1,",
        "detail": "Demo.bt_parameters",
        "documentation": {}
    },
    {
        "label": "FN2H",
        "kind": 5,
        "importPath": "Demo.bt_parameters",
        "description": "Demo.bt_parameters",
        "peekOfCode": "FN2H = {\n '': 1,\n 'aspirin': 2,\n 'albuterol': 2,\n 'bag valve mask ventilation': 4,\n 'capnography': 2,\n 'cardiac monitor': 2,\n 'cpap': 4,\n 'dexamethasone': 2,\n 'dextrose': 4,",
        "detail": "Demo.bt_parameters",
        "documentation": {}
    },
    {
        "label": "FP2H",
        "kind": 5,
        "importPath": "Demo.bt_parameters",
        "description": "Demo.bt_parameters",
        "peekOfCode": "FP2H = {\n '': 1,\n 'aspirin': 2,\n 'bag valve mask ventilation': 2,\n 'cardiac monitor': 1,\n 'cpap': 3,\n 'dexamethasone': 2,\n 'dextrose': 2,\n 'midazolam': 4,\n 'narcan': 2,",
        "detail": "Demo.bt_parameters",
        "documentation": {}
    },
    {
        "label": "TranscriptItem",
        "kind": 6,
        "importPath": "Demo.classes",
        "description": "Demo.classes",
        "peekOfCode": "class TranscriptItem:\n      def __init__(self, transcript, isFinal, confidence, transcriptionDuration):\n            self.transcript = transcript\n            self.isFinal = isFinal\n            self.confidence = confidence\n            self.transcriptionDuration = transcriptionDuration\n# ------------ For Feedback ------------\nclass FeedbackObj:\n    def __init__(self, intervention, protocol, protocol_confidence, concept):\n        super(FeedbackObj, self).__init__()",
        "detail": "Demo.classes",
        "documentation": {}
    },
    {
        "label": "FeedbackObj",
        "kind": 6,
        "importPath": "Demo.classes",
        "description": "Demo.classes",
        "peekOfCode": "class FeedbackObj:\n    def __init__(self, intervention, protocol, protocol_confidence, concept):\n        super(FeedbackObj, self).__init__()\n        self.intervention = intervention\n        self.protocol = protocol\n        self.protocol_confidence = protocol_confidence\n        self.concept = concept\n# ============== Custom Speech to NLP Queue Item Class ==============\nclass SpeechNLPItem:\n        def __init__(self, transcript, isFinal, confidence, numPrinted, origin):",
        "detail": "Demo.classes",
        "documentation": {}
    },
    {
        "label": "SpeechNLPItem",
        "kind": 6,
        "importPath": "Demo.classes",
        "description": "Demo.classes",
        "peekOfCode": "class SpeechNLPItem:\n        def __init__(self, transcript, isFinal, confidence, numPrinted, origin):\n            self.transcript = transcript\n            self.isFinal = isFinal\n            self.confidence = confidence\n            self.origin = origin\n            self.numPrinted = numPrinted\n#======================= To be replaced with better designed classes:\nclass QueueItem:\n    def __init__(self, transcript, isFinal, confidence):",
        "detail": "Demo.classes",
        "documentation": {}
    },
    {
        "label": "QueueItem",
        "kind": 6,
        "importPath": "Demo.classes",
        "description": "Demo.classes",
        "peekOfCode": "class QueueItem:\n    def __init__(self, transcript, isFinal, confidence):\n            self.transcript = transcript\n            self.isFinal = isFinal\n            self.confidence = 0\n# # Custom object for signalling\n# class GUISignal(QObject):\n#     signal = pyqtSignal(list)",
        "detail": "Demo.classes",
        "documentation": {}
    },
    {
        "label": "FeedbackObj",
        "kind": 6,
        "importPath": "Demo.clientTCPtest",
        "description": "Demo.clientTCPtest",
        "peekOfCode": "class FeedbackObj:\n    def __init__(self, intervention, protocol, concept):\n        super(FeedbackObj, self).__init__()\n        self.intervention = intervention\n        self.protocol = protocol\n        self.concept = concept\nwith socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n    s.connect((HOST, PORT))\n    while(True):\n        data = s.recv(1024)",
        "detail": "Demo.clientTCPtest",
        "documentation": {}
    },
    {
        "label": "HOST",
        "kind": 5,
        "importPath": "Demo.clientTCPtest",
        "description": "Demo.clientTCPtest",
        "peekOfCode": "HOST = \"127.0.0.1\" # The server's hostname or IP address\nPORT = 7088  # The port used by the server\nclass FeedbackObj:\n    def __init__(self, intervention, protocol, concept):\n        super(FeedbackObj, self).__init__()\n        self.intervention = intervention\n        self.protocol = protocol\n        self.concept = concept\nwith socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n    s.connect((HOST, PORT))",
        "detail": "Demo.clientTCPtest",
        "documentation": {}
    },
    {
        "label": "PORT",
        "kind": 5,
        "importPath": "Demo.clientTCPtest",
        "description": "Demo.clientTCPtest",
        "peekOfCode": "PORT = 7088  # The port used by the server\nclass FeedbackObj:\n    def __init__(self, intervention, protocol, concept):\n        super(FeedbackObj, self).__init__()\n        self.intervention = intervention\n        self.protocol = protocol\n        self.concept = concept\nwith socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n    s.connect((HOST, PORT))\n    while(True):",
        "detail": "Demo.clientTCPtest",
        "documentation": {}
    },
    {
        "label": "thresholding_algo",
        "kind": 2,
        "importPath": "Demo.cpr_calculation",
        "description": "Demo.cpr_calculation",
        "peekOfCode": "def thresholding_algo(y, lag, threshold, influence):\n    signals = np.zeros(len(y))\n    filteredY = np.array(y)\n    avgFilter = [0]*len(y)\n    stdFilter = [0]*len(y)\n    avgFilter[lag - 1] = np.mean(y[0:lag])\n    stdFilter[lag - 1] = np.std(y[0:lag])\n    for i in range(lag, len(y)):\n        if abs(y[i] - avgFilter[i-1]) > threshold * stdFilter [i-1]:\n            if y[i] > avgFilter[i-1]:",
        "detail": "Demo.cpr_calculation",
        "documentation": {}
    },
    {
        "label": "robust_peaks_detection_zscore",
        "kind": 2,
        "importPath": "Demo.cpr_calculation",
        "description": "Demo.cpr_calculation",
        "peekOfCode": "def robust_peaks_detection_zscore(df,lag,threshold,influence):\n    rate = thresholding_algo(df['Value_Magnitude_XYZ'],lag,threshold,influence)\n    indices = np.where(rate['signals'] == 1)[0]\n    robust_peaks_time = df.iloc[indices]['EPOCH_Time_ms']\n    robust_peaks_value = df.iloc[indices]['Value_Magnitude_XYZ']\n    indices = np.where(rate['signals'] == -1)[0]\n    robust_valleys_time = df.iloc[indices]['EPOCH_Time_ms']\n    robust_valleys_value = df.iloc[indices]['Value_Magnitude_XYZ']\n    # # #Plotting\n    # fig = plt.figure()",
        "detail": "Demo.cpr_calculation",
        "documentation": {}
    },
    {
        "label": "find_peaks_valleys",
        "kind": 2,
        "importPath": "Demo.cpr_calculation",
        "description": "Demo.cpr_calculation",
        "peekOfCode": "def find_peaks_valleys(df,height,distance,prominence):\n    # print(\"find_peaks\",df)\n    peaks = find_peaks(df['Value_Magnitude_XYZ'], height = height,  distance = distance,prominence=prominence)\n    height = peaks[1]['peak_heights'] #list of the heights of the peaks\n    peak_pos = df.iloc[peaks[0]] #list of the peaks positions\n    # #Finding the minima\n    y2 = df['Value_Magnitude_XYZ']*-1\n    minima = find_peaks(y2,height = -5, distance = 1)\n    min_pos = df.iloc[minima[0]] #list of the minima positions\n    min_height = y2.iloc[minima[0]] #list of the mirrored minima heights",
        "detail": "Demo.cpr_calculation",
        "documentation": {}
    },
    {
        "label": "find_cpr_rate",
        "kind": 2,
        "importPath": "Demo.cpr_calculation",
        "description": "Demo.cpr_calculation",
        "peekOfCode": "def find_cpr_rate(peaks):\n    time_diff_between_peaks=np.diff(peaks['EPOCH_Time_ms'])\n    is_not_empty=len(time_diff_between_peaks) > 0\n    if is_not_empty:\n        avg_time_btwn_peaks_in_seconds_scipy = np.average(time_diff_between_peaks)/1000\n        # print (\"Average time between peaks in seconds (scipy): \", str(avg_time_btwn_peaks_in_seconds_scipy))\n        # print(\"CPR Rate Per Minute (scipy): \", (1/avg_time_btwn_peaks_in_seconds_scipy)*60)\n        return [(avg_time_btwn_peaks_in_seconds_scipy), ((1/avg_time_btwn_peaks_in_seconds_scipy)*60)]\n    return [0,0]\n\"\"\" ",
        "detail": "Demo.cpr_calculation",
        "documentation": {}
    },
    {
        "label": "preprocess_data",
        "kind": 2,
        "importPath": "Demo.cpr_calculation",
        "description": "Demo.cpr_calculation",
        "peekOfCode": "def preprocess_data(df):\n    magnitude_xyz_df = np.sqrt(np.square(df[['Value_X_Axis','Value_Y_Axis','Value_Z_Axis']]).sum(axis=1))\n    df['Value_Magnitude_XYZ'] = magnitude_xyz_df\n    return df\ndef vid_streaming_Cpr(y_vals, image_times):\n    mean=np.convolve(y_vals, np.ones(50)/50, mode='valid')\n    mean=np.pad(mean,(len(y_vals)-len(mean),0),'edge')\n    #normalize by removing mean \n    wrist_data_norm=y_vals-mean\n    #detect peaks for hand detection",
        "detail": "Demo.cpr_calculation",
        "documentation": {}
    },
    {
        "label": "vid_streaming_Cpr",
        "kind": 2,
        "importPath": "Demo.cpr_calculation",
        "description": "Demo.cpr_calculation",
        "peekOfCode": "def vid_streaming_Cpr(y_vals, image_times):\n    mean=np.convolve(y_vals, np.ones(50)/50, mode='valid')\n    mean=np.pad(mean,(len(y_vals)-len(mean),0),'edge')\n    #normalize by removing mean \n    wrist_data_norm=y_vals-mean\n    #detect peaks for hand detection\n    peaks, _ = find_peaks(wrist_data_norm, height=0.005)\n    peak_times = np.take(image_times, peaks)\n    #find time difference between peaks and calculate cpr rate\n    time_diff_between_peaks=np.diff(peak_times)",
        "detail": "Demo.cpr_calculation",
        "documentation": {}
    },
    {
        "label": "MPThread",
        "kind": 6,
        "importPath": "Demo.mediapipe_thread",
        "description": "Demo.mediapipe_thread",
        "peekOfCode": "class MPThread(QThread):\n    changePixmap = pyqtSignal(QImage)\n    frame_buffer = []\n    def read_frames(self, path_to_video_file):\n        \"\"\"method to read all frames in video regardless (to be run in a parallel thread to processing code)\"\"\"\n        global frame_buffer\n        cap = cv2.VideoCapture(path_to_video_file)\n        done = False\n        while not done:\n            ret, image = cap.read()",
        "detail": "Demo.mediapipe_thread",
        "documentation": {}
    },
    {
        "label": "VIDEO_WIDTH",
        "kind": 5,
        "importPath": "Demo.mediapipe_thread",
        "description": "Demo.mediapipe_thread",
        "peekOfCode": "VIDEO_WIDTH = 841\nVIDEO_HEIGHT = 511\nclass MPThread(QThread):\n    changePixmap = pyqtSignal(QImage)\n    frame_buffer = []\n    def read_frames(self, path_to_video_file):\n        \"\"\"method to read all frames in video regardless (to be run in a parallel thread to processing code)\"\"\"\n        global frame_buffer\n        cap = cv2.VideoCapture(path_to_video_file)\n        done = False",
        "detail": "Demo.mediapipe_thread",
        "documentation": {}
    },
    {
        "label": "VIDEO_HEIGHT",
        "kind": 5,
        "importPath": "Demo.mediapipe_thread",
        "description": "Demo.mediapipe_thread",
        "peekOfCode": "VIDEO_HEIGHT = 511\nclass MPThread(QThread):\n    changePixmap = pyqtSignal(QImage)\n    frame_buffer = []\n    def read_frames(self, path_to_video_file):\n        \"\"\"method to read all frames in video regardless (to be run in a parallel thread to processing code)\"\"\"\n        global frame_buffer\n        cap = cv2.VideoCapture(path_to_video_file)\n        done = False\n        while not done:",
        "detail": "Demo.mediapipe_thread",
        "documentation": {}
    },
    {
        "label": "datacollection",
        "kind": 5,
        "importPath": "Demo.pipeline_config",
        "description": "Demo.pipeline_config",
        "peekOfCode": "datacollection = True\nvideostream = True\naudiostream = True\nsmartwatchStream = True\nconceptExtractionStream = True\nprotocolStream = True\ninterventionStream = True\ntranscriptStream = True\n# ------------------- Experiment Control ------------------- #\nendtoendspv = False #speech+protocol+vision",
        "detail": "Demo.pipeline_config",
        "documentation": {}
    },
    {
        "label": "videostream",
        "kind": 5,
        "importPath": "Demo.pipeline_config",
        "description": "Demo.pipeline_config",
        "peekOfCode": "videostream = True\naudiostream = True\nsmartwatchStream = True\nconceptExtractionStream = True\nprotocolStream = True\ninterventionStream = True\ntranscriptStream = True\n# ------------------- Experiment Control ------------------- #\nendtoendspv = False #speech+protocol+vision\n# ------------------- Speech to Text Control ------------------- #",
        "detail": "Demo.pipeline_config",
        "documentation": {}
    },
    {
        "label": "audiostream",
        "kind": 5,
        "importPath": "Demo.pipeline_config",
        "description": "Demo.pipeline_config",
        "peekOfCode": "audiostream = True\nsmartwatchStream = True\nconceptExtractionStream = True\nprotocolStream = True\ninterventionStream = True\ntranscriptStream = True\n# ------------------- Experiment Control ------------------- #\nendtoendspv = False #speech+protocol+vision\n# ------------------- Speech to Text Control ------------------- #\nspeech_model = 'conformer' # 'conformer'",
        "detail": "Demo.pipeline_config",
        "documentation": {}
    },
    {
        "label": "smartwatchStream",
        "kind": 5,
        "importPath": "Demo.pipeline_config",
        "description": "Demo.pipeline_config",
        "peekOfCode": "smartwatchStream = True\nconceptExtractionStream = True\nprotocolStream = True\ninterventionStream = True\ntranscriptStream = True\n# ------------------- Experiment Control ------------------- #\nendtoendspv = False #speech+protocol+vision\n# ------------------- Speech to Text Control ------------------- #\nspeech_model = 'conformer' # 'conformer'\nspeech_model = 'whisper' # 'conformer'",
        "detail": "Demo.pipeline_config",
        "documentation": {}
    },
    {
        "label": "conceptExtractionStream",
        "kind": 5,
        "importPath": "Demo.pipeline_config",
        "description": "Demo.pipeline_config",
        "peekOfCode": "conceptExtractionStream = True\nprotocolStream = True\ninterventionStream = True\ntranscriptStream = True\n# ------------------- Experiment Control ------------------- #\nendtoendspv = False #speech+protocol+vision\n# ------------------- Speech to Text Control ------------------- #\nspeech_model = 'conformer' # 'conformer'\nspeech_model = 'whisper' # 'conformer'\n# -- Audio Recording Option -----",
        "detail": "Demo.pipeline_config",
        "documentation": {}
    },
    {
        "label": "protocolStream",
        "kind": 5,
        "importPath": "Demo.pipeline_config",
        "description": "Demo.pipeline_config",
        "peekOfCode": "protocolStream = True\ninterventionStream = True\ntranscriptStream = True\n# ------------------- Experiment Control ------------------- #\nendtoendspv = False #speech+protocol+vision\n# ------------------- Speech to Text Control ------------------- #\nspeech_model = 'conformer' # 'conformer'\nspeech_model = 'whisper' # 'conformer'\n# -- Audio Recording Option -----\nhardcoded = True",
        "detail": "Demo.pipeline_config",
        "documentation": {}
    },
    {
        "label": "interventionStream",
        "kind": 5,
        "importPath": "Demo.pipeline_config",
        "description": "Demo.pipeline_config",
        "peekOfCode": "interventionStream = True\ntranscriptStream = True\n# ------------------- Experiment Control ------------------- #\nendtoendspv = False #speech+protocol+vision\n# ------------------- Speech to Text Control ------------------- #\nspeech_model = 'conformer' # 'conformer'\nspeech_model = 'whisper' # 'conformer'\n# -- Audio Recording Option -----\nhardcoded = True\nrecording_name = '000_190105'",
        "detail": "Demo.pipeline_config",
        "documentation": {}
    },
    {
        "label": "transcriptStream",
        "kind": 5,
        "importPath": "Demo.pipeline_config",
        "description": "Demo.pipeline_config",
        "peekOfCode": "transcriptStream = True\n# ------------------- Experiment Control ------------------- #\nendtoendspv = False #speech+protocol+vision\n# ------------------- Speech to Text Control ------------------- #\nspeech_model = 'conformer' # 'conformer'\nspeech_model = 'whisper' # 'conformer'\n# -- Audio Recording Option -----\nhardcoded = True\nrecording_name = '000_190105'\nvideo_name = 'scenario_1'",
        "detail": "Demo.pipeline_config",
        "documentation": {}
    },
    {
        "label": "endtoendspv",
        "kind": 5,
        "importPath": "Demo.pipeline_config",
        "description": "Demo.pipeline_config",
        "peekOfCode": "endtoendspv = False #speech+protocol+vision\n# ------------------- Speech to Text Control ------------------- #\nspeech_model = 'conformer' # 'conformer'\nspeech_model = 'whisper' # 'conformer'\n# -- Audio Recording Option -----\nhardcoded = True\nrecording_name = '000_190105'\nvideo_name = 'scenario_1'\n# -- Whisper configuration ---\nwhisper_model_size = \"base-finetuned\"",
        "detail": "Demo.pipeline_config",
        "documentation": {}
    },
    {
        "label": "speech_model",
        "kind": 5,
        "importPath": "Demo.pipeline_config",
        "description": "Demo.pipeline_config",
        "peekOfCode": "speech_model = 'conformer' # 'conformer'\nspeech_model = 'whisper' # 'conformer'\n# -- Audio Recording Option -----\nhardcoded = True\nrecording_name = '000_190105'\nvideo_name = 'scenario_1'\n# -- Whisper configuration ---\nwhisper_model_size = \"base-finetuned\"\nwhisper_model_size = \"base.en\"\n# -- Whisper configuration ---",
        "detail": "Demo.pipeline_config",
        "documentation": {}
    },
    {
        "label": "speech_model",
        "kind": 5,
        "importPath": "Demo.pipeline_config",
        "description": "Demo.pipeline_config",
        "peekOfCode": "speech_model = 'whisper' # 'conformer'\n# -- Audio Recording Option -----\nhardcoded = True\nrecording_name = '000_190105'\nvideo_name = 'scenario_1'\n# -- Whisper configuration ---\nwhisper_model_size = \"base-finetuned\"\nwhisper_model_size = \"base.en\"\n# -- Whisper configuration ---\nwhisper_model_sizes = [",
        "detail": "Demo.pipeline_config",
        "documentation": {}
    },
    {
        "label": "hardcoded",
        "kind": 5,
        "importPath": "Demo.pipeline_config",
        "description": "Demo.pipeline_config",
        "peekOfCode": "hardcoded = True\nrecording_name = '000_190105'\nvideo_name = 'scenario_1'\n# -- Whisper configuration ---\nwhisper_model_size = \"base-finetuned\"\nwhisper_model_size = \"base.en\"\n# -- Whisper configuration ---\nwhisper_model_sizes = [\n    \"base-wo-emsassist\",\n    \"base-wo-synth-v1\",",
        "detail": "Demo.pipeline_config",
        "documentation": {}
    },
    {
        "label": "recording_name",
        "kind": 5,
        "importPath": "Demo.pipeline_config",
        "description": "Demo.pipeline_config",
        "peekOfCode": "recording_name = '000_190105'\nvideo_name = 'scenario_1'\n# -- Whisper configuration ---\nwhisper_model_size = \"base-finetuned\"\nwhisper_model_size = \"base.en\"\n# -- Whisper configuration ---\nwhisper_model_sizes = [\n    \"base-wo-emsassist\",\n    \"base-wo-synth-v1\",\n    \"tiny-wo-synth-v1\",",
        "detail": "Demo.pipeline_config",
        "documentation": {}
    },
    {
        "label": "video_name",
        "kind": 5,
        "importPath": "Demo.pipeline_config",
        "description": "Demo.pipeline_config",
        "peekOfCode": "video_name = 'scenario_1'\n# -- Whisper configuration ---\nwhisper_model_size = \"base-finetuned\"\nwhisper_model_size = \"base.en\"\n# -- Whisper configuration ---\nwhisper_model_sizes = [\n    \"base-wo-emsassist\",\n    \"base-wo-synth-v1\",\n    \"tiny-wo-synth-v1\",\n    \"base-finetuned-v6\",",
        "detail": "Demo.pipeline_config",
        "documentation": {}
    },
    {
        "label": "whisper_model_size",
        "kind": 5,
        "importPath": "Demo.pipeline_config",
        "description": "Demo.pipeline_config",
        "peekOfCode": "whisper_model_size = \"base-finetuned\"\nwhisper_model_size = \"base.en\"\n# -- Whisper configuration ---\nwhisper_model_sizes = [\n    \"base-wo-emsassist\",\n    \"base-wo-synth-v1\",\n    \"tiny-wo-synth-v1\",\n    \"base-finetuned-v6\",\n    \"base-finetuned-v5\",\n    \"base-finetuned-v4\",",
        "detail": "Demo.pipeline_config",
        "documentation": {}
    },
    {
        "label": "whisper_model_size",
        "kind": 5,
        "importPath": "Demo.pipeline_config",
        "description": "Demo.pipeline_config",
        "peekOfCode": "whisper_model_size = \"base.en\"\n# -- Whisper configuration ---\nwhisper_model_sizes = [\n    \"base-wo-emsassist\",\n    \"base-wo-synth-v1\",\n    \"tiny-wo-synth-v1\",\n    \"base-finetuned-v6\",\n    \"base-finetuned-v5\",\n    \"base-finetuned-v4\",\n    \"base-finetuned-v3\",",
        "detail": "Demo.pipeline_config",
        "documentation": {}
    },
    {
        "label": "whisper_model_sizes",
        "kind": 5,
        "importPath": "Demo.pipeline_config",
        "description": "Demo.pipeline_config",
        "peekOfCode": "whisper_model_sizes = [\n    \"base-wo-emsassist\",\n    \"base-wo-synth-v1\",\n    \"tiny-wo-synth-v1\",\n    \"base-finetuned-v6\",\n    \"base-finetuned-v5\",\n    \"base-finetuned-v4\",\n    \"base-finetuned-v3\",\n    \"base-finetuned-v2\",\n    \"tiny-finetuned-v5\",",
        "detail": "Demo.pipeline_config",
        "documentation": {}
    },
    {
        "label": "PATH_TO_WHISPER_CPP_FOLDER",
        "kind": 5,
        "importPath": "Demo.pipeline_config",
        "description": "Demo.pipeline_config",
        "peekOfCode": "PATH_TO_WHISPER_CPP_FOLDER = \"/home/cogems_nist/Desktop/CogEMS_NIST/whisper.cpp\"\nnum_threads = 8\nstep = 2000\nlength = 30000\nkeep_ms = 200 #audio to keep from previous step in ms\n# -- EMS Conformer configuration ------\n# conformer_model_type = 'all_14_model.tflite' \n# -- EMS Agent configuration ------\nprotocol_model_device = 'cuda' # 'cuda' or 'cpu'\nprotocol_model_type = 'DKEC-TinyClinicalBERT' #DKEC-TinyClinicalBERT, EMSAssist",
        "detail": "Demo.pipeline_config",
        "documentation": {}
    },
    {
        "label": "num_threads",
        "kind": 5,
        "importPath": "Demo.pipeline_config",
        "description": "Demo.pipeline_config",
        "peekOfCode": "num_threads = 8\nstep = 2000\nlength = 30000\nkeep_ms = 200 #audio to keep from previous step in ms\n# -- EMS Conformer configuration ------\n# conformer_model_type = 'all_14_model.tflite' \n# -- EMS Agent configuration ------\nprotocol_model_device = 'cuda' # 'cuda' or 'cpu'\nprotocol_model_type = 'DKEC-TinyClinicalBERT' #DKEC-TinyClinicalBERT, EMSAssist\n# protocol_model_type = 'EMSAssist' #DKEC-TinyClinicalBERT, EMSAssist",
        "detail": "Demo.pipeline_config",
        "documentation": {}
    },
    {
        "label": "step",
        "kind": 5,
        "importPath": "Demo.pipeline_config",
        "description": "Demo.pipeline_config",
        "peekOfCode": "step = 2000\nlength = 30000\nkeep_ms = 200 #audio to keep from previous step in ms\n# -- EMS Conformer configuration ------\n# conformer_model_type = 'all_14_model.tflite' \n# -- EMS Agent configuration ------\nprotocol_model_device = 'cuda' # 'cuda' or 'cpu'\nprotocol_model_type = 'DKEC-TinyClinicalBERT' #DKEC-TinyClinicalBERT, EMSAssist\n# protocol_model_type = 'EMSAssist' #DKEC-TinyClinicalBERT, EMSAssist\n# -- EMS Vision configuration ------",
        "detail": "Demo.pipeline_config",
        "documentation": {}
    },
    {
        "label": "length",
        "kind": 5,
        "importPath": "Demo.pipeline_config",
        "description": "Demo.pipeline_config",
        "peekOfCode": "length = 30000\nkeep_ms = 200 #audio to keep from previous step in ms\n# -- EMS Conformer configuration ------\n# conformer_model_type = 'all_14_model.tflite' \n# -- EMS Agent configuration ------\nprotocol_model_device = 'cuda' # 'cuda' or 'cpu'\nprotocol_model_type = 'DKEC-TinyClinicalBERT' #DKEC-TinyClinicalBERT, EMSAssist\n# protocol_model_type = 'EMSAssist' #DKEC-TinyClinicalBERT, EMSAssist\n# -- EMS Vision configuration ------\nvision_model_type = 'openai/clip-vit-base-patch32' ",
        "detail": "Demo.pipeline_config",
        "documentation": {}
    },
    {
        "label": "keep_ms",
        "kind": 5,
        "importPath": "Demo.pipeline_config",
        "description": "Demo.pipeline_config",
        "peekOfCode": "keep_ms = 200 #audio to keep from previous step in ms\n# -- EMS Conformer configuration ------\n# conformer_model_type = 'all_14_model.tflite' \n# -- EMS Agent configuration ------\nprotocol_model_device = 'cuda' # 'cuda' or 'cpu'\nprotocol_model_type = 'DKEC-TinyClinicalBERT' #DKEC-TinyClinicalBERT, EMSAssist\n# protocol_model_type = 'EMSAssist' #DKEC-TinyClinicalBERT, EMSAssist\n# -- EMS Vision configuration ------\nvision_model_type = 'openai/clip-vit-base-patch32' \n# --- End to End evaluation testing configs --------",
        "detail": "Demo.pipeline_config",
        "documentation": {}
    },
    {
        "label": "protocol_model_device",
        "kind": 5,
        "importPath": "Demo.pipeline_config",
        "description": "Demo.pipeline_config",
        "peekOfCode": "protocol_model_device = 'cuda' # 'cuda' or 'cpu'\nprotocol_model_type = 'DKEC-TinyClinicalBERT' #DKEC-TinyClinicalBERT, EMSAssist\n# protocol_model_type = 'EMSAssist' #DKEC-TinyClinicalBERT, EMSAssist\n# -- EMS Vision configuration ------\nvision_model_type = 'openai/clip-vit-base-patch32' \n# --- End to End evaluation testing configs --------\nif not endtoendspv:\n    recordings_to_test = [\n        '000_190105',\n        '001_190105',",
        "detail": "Demo.pipeline_config",
        "documentation": {}
    },
    {
        "label": "protocol_model_type",
        "kind": 5,
        "importPath": "Demo.pipeline_config",
        "description": "Demo.pipeline_config",
        "peekOfCode": "protocol_model_type = 'DKEC-TinyClinicalBERT' #DKEC-TinyClinicalBERT, EMSAssist\n# protocol_model_type = 'EMSAssist' #DKEC-TinyClinicalBERT, EMSAssist\n# -- EMS Vision configuration ------\nvision_model_type = 'openai/clip-vit-base-patch32' \n# --- End to End evaluation testing configs --------\nif not endtoendspv:\n    recordings_to_test = [\n        '000_190105',\n        '001_190105',\n        '002_190105',",
        "detail": "Demo.pipeline_config",
        "documentation": {}
    },
    {
        "label": "vision_model_type",
        "kind": 5,
        "importPath": "Demo.pipeline_config",
        "description": "Demo.pipeline_config",
        "peekOfCode": "vision_model_type = 'openai/clip-vit-base-patch32' \n# --- End to End evaluation testing configs --------\nif not endtoendspv:\n    recordings_to_test = [\n        '000_190105',\n        '001_190105',\n        '002_190105',\n        '003_190105',\n        '004_190105',\n        '005_190105',",
        "detail": "Demo.pipeline_config",
        "documentation": {}
    },
    {
        "label": "data_save",
        "kind": 5,
        "importPath": "Demo.pipeline_config",
        "description": "Demo.pipeline_config",
        "peekOfCode": "data_save = True\ncurr_segment = []\nvision_curr_segment = []\nrows_trial = []\nvision_rows_trial = []\nnum_trials = 10",
        "detail": "Demo.pipeline_config",
        "documentation": {}
    },
    {
        "label": "curr_segment",
        "kind": 5,
        "importPath": "Demo.pipeline_config",
        "description": "Demo.pipeline_config",
        "peekOfCode": "curr_segment = []\nvision_curr_segment = []\nrows_trial = []\nvision_rows_trial = []\nnum_trials = 10",
        "detail": "Demo.pipeline_config",
        "documentation": {}
    },
    {
        "label": "vision_curr_segment",
        "kind": 5,
        "importPath": "Demo.pipeline_config",
        "description": "Demo.pipeline_config",
        "peekOfCode": "vision_curr_segment = []\nrows_trial = []\nvision_rows_trial = []\nnum_trials = 10",
        "detail": "Demo.pipeline_config",
        "documentation": {}
    },
    {
        "label": "rows_trial",
        "kind": 5,
        "importPath": "Demo.pipeline_config",
        "description": "Demo.pipeline_config",
        "peekOfCode": "rows_trial = []\nvision_rows_trial = []\nnum_trials = 10",
        "detail": "Demo.pipeline_config",
        "documentation": {}
    },
    {
        "label": "vision_rows_trial",
        "kind": 5,
        "importPath": "Demo.pipeline_config",
        "description": "Demo.pipeline_config",
        "peekOfCode": "vision_rows_trial = []\nnum_trials = 10",
        "detail": "Demo.pipeline_config",
        "documentation": {}
    },
    {
        "label": "num_trials",
        "kind": 5,
        "importPath": "Demo.pipeline_config",
        "description": "Demo.pipeline_config",
        "peekOfCode": "num_trials = 10",
        "detail": "Demo.pipeline_config",
        "documentation": {}
    },
    {
        "label": "audio_stream_UDP",
        "kind": 2,
        "importPath": "Demo.pythonServer",
        "description": "Demo.pythonServer",
        "peekOfCode": "def audio_stream_UDP():\n    BUFF_SIZE = 1280 #65536\n    client_socket = socket.socket(socket.AF_INET,socket.SOCK_DGRAM)\n    # client_socket.setsockopt(socket.SOL_SOCKET,socket.SO_RCVBUF,BUFF_SIZE)\n    client_socket.bind((host_ip,port))\n    p = pyaudio.PyAudio()\n    CHUNK = 128\n    stream = p.open(format=p.get_format_from_width(2),\n                    channels=1,\n                    rate=16000,#44100,",
        "detail": "Demo.pythonServer",
        "documentation": {}
    },
    {
        "label": "host_name",
        "kind": 5,
        "importPath": "Demo.pythonServer",
        "description": "Demo.pythonServer",
        "peekOfCode": "host_name = socket.gethostname()\nhost_ip = '0.0.0.0' #'172.25.149.100'#'192.168.1.102'#  socket.gethostbyname(host_name)\nprint(host_ip)\nport = 8888\nq = queue.Queue(maxsize=12800)\ndef audio_stream_UDP():\n    BUFF_SIZE = 1280 #65536\n    client_socket = socket.socket(socket.AF_INET,socket.SOCK_DGRAM)\n    # client_socket.setsockopt(socket.SOL_SOCKET,socket.SO_RCVBUF,BUFF_SIZE)\n    client_socket.bind((host_ip,port))",
        "detail": "Demo.pythonServer",
        "documentation": {}
    },
    {
        "label": "host_ip",
        "kind": 5,
        "importPath": "Demo.pythonServer",
        "description": "Demo.pythonServer",
        "peekOfCode": "host_ip = '0.0.0.0' #'172.25.149.100'#'192.168.1.102'#  socket.gethostbyname(host_name)\nprint(host_ip)\nport = 8888\nq = queue.Queue(maxsize=12800)\ndef audio_stream_UDP():\n    BUFF_SIZE = 1280 #65536\n    client_socket = socket.socket(socket.AF_INET,socket.SOCK_DGRAM)\n    # client_socket.setsockopt(socket.SOL_SOCKET,socket.SO_RCVBUF,BUFF_SIZE)\n    client_socket.bind((host_ip,port))\n    p = pyaudio.PyAudio()",
        "detail": "Demo.pythonServer",
        "documentation": {}
    },
    {
        "label": "port",
        "kind": 5,
        "importPath": "Demo.pythonServer",
        "description": "Demo.pythonServer",
        "peekOfCode": "port = 8888\nq = queue.Queue(maxsize=12800)\ndef audio_stream_UDP():\n    BUFF_SIZE = 1280 #65536\n    client_socket = socket.socket(socket.AF_INET,socket.SOCK_DGRAM)\n    # client_socket.setsockopt(socket.SOL_SOCKET,socket.SO_RCVBUF,BUFF_SIZE)\n    client_socket.bind((host_ip,port))\n    p = pyaudio.PyAudio()\n    CHUNK = 128\n    stream = p.open(format=p.get_format_from_width(2),",
        "detail": "Demo.pythonServer",
        "documentation": {}
    },
    {
        "label": "q",
        "kind": 5,
        "importPath": "Demo.pythonServer",
        "description": "Demo.pythonServer",
        "peekOfCode": "q = queue.Queue(maxsize=12800)\ndef audio_stream_UDP():\n    BUFF_SIZE = 1280 #65536\n    client_socket = socket.socket(socket.AF_INET,socket.SOCK_DGRAM)\n    # client_socket.setsockopt(socket.SOL_SOCKET,socket.SO_RCVBUF,BUFF_SIZE)\n    client_socket.bind((host_ip,port))\n    p = pyaudio.PyAudio()\n    CHUNK = 128\n    stream = p.open(format=p.get_format_from_width(2),\n                    channels=1,",
        "detail": "Demo.pythonServer",
        "documentation": {}
    },
    {
        "label": "t1",
        "kind": 5,
        "importPath": "Demo.pythonServer",
        "description": "Demo.pythonServer",
        "peekOfCode": "t1 = threading.Thread(target=audio_stream_UDP, args=())\nt1.start()",
        "detail": "Demo.pythonServer",
        "documentation": {}
    },
    {
        "label": "rank",
        "kind": 2,
        "importPath": "Demo.ranking_func",
        "description": "Demo.ranking_func",
        "peekOfCode": "def rank(scores):\n    srt = sorted(scores,key=lambda x: x[1], reverse=True)\n    return [i[0] for i in srt],[i[1] for i in srt]\ndef nDCG(rank,gt,group,num):\n    from numpy import log2\n    for i in group:\n        if gt in group[i]:\n            break\n    ct = len(group[i]) - 1\n    idcg = 1.",
        "detail": "Demo.ranking_func",
        "documentation": {}
    },
    {
        "label": "nDCG",
        "kind": 2,
        "importPath": "Demo.ranking_func",
        "description": "Demo.ranking_func",
        "peekOfCode": "def nDCG(rank,gt,group,num):\n    from numpy import log2\n    for i in group:\n        if gt in group[i]:\n            break\n    ct = len(group[i]) - 1\n    idcg = 1.\n    idx = 1\n    num -= 1\n    while (ct&num):",
        "detail": "Demo.ranking_func",
        "documentation": {}
    },
    {
        "label": "search_term",
        "kind": 2,
        "importPath": "Demo.search_term",
        "description": "Demo.search_term",
        "peekOfCode": "def search_term(string, page_limit = 2, check = False):\n    apikey = '6119ca85-39ff-4649-aa79-6c1b1d281a02'\n    version = '2018AA'\n    string = string\n    uri = \"https://uts-ws.nlm.nih.gov\"\n    content_endpoint = \"/rest/search/\"+version\n    ##get at ticket granting ticket for the session\n    AuthClient = Authentication(apikey)\n    tgt = AuthClient.gettgt()\n    pageNumber=0",
        "detail": "Demo.search_term",
        "documentation": {}
    },
    {
        "label": "HOST",
        "kind": 5,
        "importPath": "Demo.serverTest",
        "description": "Demo.serverTest",
        "peekOfCode": "HOST = \"127.0.0.1\" # The server's hostname or IP address\nTCP_PORT = 7088\n#initialize tcp connection\nsock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)  \nsock.bind((\"0.0.0.0\", TCP_PORT))  \nsock.listen(5)  \nprint(\"Waiting for android...\")\nconnection,address = sock.accept()  \nprint(\"Connected to android: \",address)\n# while(True):",
        "detail": "Demo.serverTest",
        "documentation": {}
    },
    {
        "label": "TCP_PORT",
        "kind": 5,
        "importPath": "Demo.serverTest",
        "description": "Demo.serverTest",
        "peekOfCode": "TCP_PORT = 7088\n#initialize tcp connection\nsock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)  \nsock.bind((\"0.0.0.0\", TCP_PORT))  \nsock.listen(5)  \nprint(\"Waiting for android...\")\nconnection,address = sock.accept()  \nprint(\"Connected to android: \",address)\n# while(True):\n#     example = FeedbackObj('intervention', 'protocol', 'concept')",
        "detail": "Demo.serverTest",
        "documentation": {}
    },
    {
        "label": "sock",
        "kind": 5,
        "importPath": "Demo.serverTest",
        "description": "Demo.serverTest",
        "peekOfCode": "sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)  \nsock.bind((\"0.0.0.0\", TCP_PORT))  \nsock.listen(5)  \nprint(\"Waiting for android...\")\nconnection,address = sock.accept()  \nprint(\"Connected to android: \",address)\n# while(True):\n#     example = FeedbackObj('intervention', 'protocol', 'concept')\n#     sendMessage(example, connection)\n#     time.sleep(1)",
        "detail": "Demo.serverTest",
        "documentation": {}
    },
    {
        "label": "connection,address",
        "kind": 5,
        "importPath": "Demo.serverTest",
        "description": "Demo.serverTest",
        "peekOfCode": "connection,address = sock.accept()  \nprint(\"Connected to android: \",address)\n# while(True):\n#     example = FeedbackObj('intervention', 'protocol', 'concept')\n#     sendMessage(example, connection)\n#     time.sleep(1)\ncount = 0\nwhile True:\n    # print(\"data string to send from feedback: \", data_string)   \n    data_string = b\"Concept: (Wheezing, True, breath sounds, 0.76)\" + b'\\0' #b\"Hello from cogEMS! \" +str.encode(str(count)) + b'\\0'",
        "detail": "Demo.serverTest",
        "documentation": {}
    },
    {
        "label": "count",
        "kind": 5,
        "importPath": "Demo.serverTest",
        "description": "Demo.serverTest",
        "peekOfCode": "count = 0\nwhile True:\n    # print(\"data string to send from feedback: \", data_string)   \n    data_string = b\"Concept: (Wheezing, True, breath sounds, 0.76)\" + b'\\0' #b\"Hello from cogEMS! \" +str.encode(str(count)) + b'\\0'\n    data_string2 = b\"Protocol: general - cardiac arrest: 0.919859\" + b'\\0' #b\"Hello from cogEMS! \" +str.encode(str(count)) + b'\\0'\n    data_string3 = b\"Intervention: (Transport, 1.0) | (Cardiac monitor, 0.57)\" + b'\\0' #b\"Hello from cogEMS! \" +str.encode(str(count)) + b'\\0'\n    num_bytes = len(data_string)\n    # connection.send(num_bytes.to_bytes())\n    # sent = connection.send(getsizeof(data_string)) #b\"hello from server\"\n    sent = connection.send(data_string) #b\"hello from server\"",
        "detail": "Demo.serverTest",
        "documentation": {}
    },
    {
        "label": "Thread_Watch",
        "kind": 6,
        "importPath": "Demo.smartwatch_streaming",
        "description": "Demo.smartwatch_streaming",
        "peekOfCode": "class Thread_Watch(QThread):\n    def __init__(self, var, bool):\n        self.data_path_str = var + \"smartwatchdata/\"\n        self.smartwatchStreamBool = bool\n        super().__init__()\n    changeActivityRec = pyqtSignal(str)\n    def run(self):\n        curr_date = datetime.datetime.now()\n        dt_string = curr_date.strftime(\"%d-%m-%Y-%H-%M-%S\")\n        if self.smartwatchStreamBool == True:",
        "detail": "Demo.smartwatch_streaming",
        "documentation": {}
    },
    {
        "label": "RAAdata",
        "kind": 6,
        "importPath": "Demo.text_clf_utils",
        "description": "Demo.text_clf_utils",
        "peekOfCode": "class RAAdata(object):\n    def __init__(self,text,vital,inter):\n        self.text = text\n        self.vital = vital\n        self.inter = inter\ndef load_RAA_data(path, cv = True): \n    df = pd.read_excel(path)\n    interset = set()\n    interdict = dict()\n    narratives = df['Narrative']",
        "detail": "Demo.text_clf_utils",
        "documentation": {}
    },
    {
        "label": "load_RAA_data",
        "kind": 2,
        "importPath": "Demo.text_clf_utils",
        "description": "Demo.text_clf_utils",
        "peekOfCode": "def load_RAA_data(path, cv = True): \n    df = pd.read_excel(path)\n    interset = set()\n    interdict = dict()\n    narratives = df['Narrative']\n    narratives = [i for i in narratives]\n    inters = df['Interventions']\n    vitals = df['Vitals']\n    vitals = [i for i in vitals]\n    interventions = []",
        "detail": "Demo.text_clf_utils",
        "documentation": {}
    },
    {
        "label": "fullmatch",
        "kind": 2,
        "importPath": "Demo.text_clf_utils",
        "description": "Demo.text_clf_utils",
        "peekOfCode": "def fullmatch(regex, string, flags=0):\n    \"\"\"Emulate python-3.4 re.fullmatch().\"\"\"\n    return re.match(\"(?:\" + regex + r\")\\Z\", string, flags=flags)\n# preprocess utils\ndef cleanHtml(sentence):\n    cleanr = re.compile('<.*?>')\n    cleantext = re.sub(cleanr, ' ', str(sentence))\n    return cleantext\ndef cleanPunc(sentence):\n    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)",
        "detail": "Demo.text_clf_utils",
        "documentation": {}
    },
    {
        "label": "cleanHtml",
        "kind": 2,
        "importPath": "Demo.text_clf_utils",
        "description": "Demo.text_clf_utils",
        "peekOfCode": "def cleanHtml(sentence):\n    cleanr = re.compile('<.*?>')\n    cleantext = re.sub(cleanr, ' ', str(sentence))\n    return cleantext\ndef cleanPunc(sentence):\n    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n    cleaned = cleaned.strip()\n    cleaned = cleaned.replace(\"\\n\",\" \")\n    return cleaned",
        "detail": "Demo.text_clf_utils",
        "documentation": {}
    },
    {
        "label": "cleanPunc",
        "kind": 2,
        "importPath": "Demo.text_clf_utils",
        "description": "Demo.text_clf_utils",
        "peekOfCode": "def cleanPunc(sentence):\n    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n    cleaned = cleaned.strip()\n    cleaned = cleaned.replace(\"\\n\",\" \")\n    return cleaned\ndef keepAlpha(sentence):\n    alpha_sent = \"\"\n    for word in sentence.split():\n        alpha_word = re.sub('[^a-z A-Z]+', ' ', word)",
        "detail": "Demo.text_clf_utils",
        "documentation": {}
    },
    {
        "label": "keepAlpha",
        "kind": 2,
        "importPath": "Demo.text_clf_utils",
        "description": "Demo.text_clf_utils",
        "peekOfCode": "def keepAlpha(sentence):\n    alpha_sent = \"\"\n    for word in sentence.split():\n        alpha_word = re.sub('[^a-z A-Z]+', ' ', word)\n        alpha_sent += alpha_word\n        alpha_sent += \" \"\n    alpha_sent = alpha_sent.strip()\n    return alpha_sent\nstemmer = SnowballStemmer(\"english\")\ndef stemming(sentence):",
        "detail": "Demo.text_clf_utils",
        "documentation": {}
    },
    {
        "label": "stemming",
        "kind": 2,
        "importPath": "Demo.text_clf_utils",
        "description": "Demo.text_clf_utils",
        "peekOfCode": "def stemming(sentence):\n    stemSentence = \"\"\n    for word in sentence.split():\n        stem = stemmer.stem(word)\n        stemSentence += stem\n        stemSentence += \" \"\n    stemSentence = stemSentence.strip()\n    return stemSentence\ndef weighted_precision_recall_f1_util (y_test, y_pre, weight = None):\n    tp, fp, fn = [0. for _ in range(len(y_pre[0]))], [0. for _ in range(len(y_pre[0]))], \\",
        "detail": "Demo.text_clf_utils",
        "documentation": {}
    },
    {
        "label": "weighted_precision_recall_f1_uti",
        "kind": 2,
        "importPath": "Demo.text_clf_utils",
        "description": "Demo.text_clf_utils",
        "peekOfCode": "def weighted_precision_recall_f1_util (y_test, y_pre, weight = None):\n    tp, fp, fn = [0. for _ in range(len(y_pre[0]))], [0. for _ in range(len(y_pre[0]))], \\\n    [0. for _ in range(len(y_pre[0]))]\n    for idx in range(len(y_pre)):\n        for i in range(len(y_pre[idx])):\n            if y_pre[idx][i] == 1 and y_test[idx][i] == 1: tp[i] += 1\n            elif y_pre[idx][i] == 1 and y_test[idx][i] == 0: fp[i] += 1\n            elif y_pre[idx][i] == 0 and y_test[idx][i] == 1: fn[i] += 1\n    precision = [tp[i] / (tp[i] + fp[i]) if tp[i] > 0 or fp[i] > 0 else 0. for i in range(len(tp))]\n    recall = [tp[i] / (tp[i] + fn[i]) if tp[i] > 0 or fn[i] > 0 else 0. for i in range(len(tp))]",
        "detail": "Demo.text_clf_utils",
        "documentation": {}
    },
    {
        "label": "weighted_precisio",
        "kind": 2,
        "importPath": "Demo.text_clf_utils",
        "description": "Demo.text_clf_utils",
        "peekOfCode": "def weighted_precision (y_test, y_pre, weight = None):\n    precision, _, _ = weighted_precision_recall_f1_util (y_test, y_pre, weight)\n    return precision\ndef weighted_recall (y_test, y_pre, weight = None):\n    _, recall, _ = weighted_precision_recall_f1_util (y_test, y_pre, weight)\n    return recall\ndef weighted_f1 (y_test, y_pre, weight = None):\n    _, _, f1 = weighted_precision_recall_f1_util (y_test, y_pre, weight)\n    return f1\ndef show_results(scores):",
        "detail": "Demo.text_clf_utils",
        "documentation": {}
    },
    {
        "label": "weighted_recal",
        "kind": 2,
        "importPath": "Demo.text_clf_utils",
        "description": "Demo.text_clf_utils",
        "peekOfCode": "def weighted_recall (y_test, y_pre, weight = None):\n    _, recall, _ = weighted_precision_recall_f1_util (y_test, y_pre, weight)\n    return recall\ndef weighted_f1 (y_test, y_pre, weight = None):\n    _, _, f1 = weighted_precision_recall_f1_util (y_test, y_pre, weight)\n    return f1\ndef show_results(scores):\n    metrics = ['test_precision_weighted','test_recall_weighted', 'test_f1_weighted',\\\n            'test_precision_micro', 'test_recall_micro', 'test_f1_micro']\n    for metric in metrics:",
        "detail": "Demo.text_clf_utils",
        "documentation": {}
    },
    {
        "label": "weighted_f",
        "kind": 2,
        "importPath": "Demo.text_clf_utils",
        "description": "Demo.text_clf_utils",
        "peekOfCode": "def weighted_f1 (y_test, y_pre, weight = None):\n    _, _, f1 = weighted_precision_recall_f1_util (y_test, y_pre, weight)\n    return f1\ndef show_results(scores):\n    metrics = ['test_precision_weighted','test_recall_weighted', 'test_f1_weighted',\\\n            'test_precision_micro', 'test_recall_micro', 'test_f1_micro']\n    for metric in metrics:\n        print(metric + ':' + '%.2f' % np.average(scores[metric]))\ndef risk_factor(gt, probs, preds):\n    risk = []",
        "detail": "Demo.text_clf_utils",
        "documentation": {}
    },
    {
        "label": "show_results",
        "kind": 2,
        "importPath": "Demo.text_clf_utils",
        "description": "Demo.text_clf_utils",
        "peekOfCode": "def show_results(scores):\n    metrics = ['test_precision_weighted','test_recall_weighted', 'test_f1_weighted',\\\n            'test_precision_micro', 'test_recall_micro', 'test_f1_micro']\n    for metric in metrics:\n        print(metric + ':' + '%.2f' % np.average(scores[metric]))\ndef risk_factor(gt, probs, preds):\n    risk = []\n    for idx,case in enumerate(probs):\n        r = 0\n        for i,prob in enumerate(case):",
        "detail": "Demo.text_clf_utils",
        "documentation": {}
    },
    {
        "label": "risk_factor",
        "kind": 2,
        "importPath": "Demo.text_clf_utils",
        "description": "Demo.text_clf_utils",
        "peekOfCode": "def risk_factor(gt, probs, preds):\n    risk = []\n    for idx,case in enumerate(probs):\n        r = 0\n        for i,prob in enumerate(case):\n            if preds[idx][i] == 1 and gt[idx][i] == 0:\n                r += prob * int2fp_score[num2int[i]] / sum(gt[idx])\n            if preds[idx][i] == 0 and gt[idx][i] == 1:\n                r += prob * int2fn_score[num2int[i]] / sum(gt[idx])\n        risk.append(r)",
        "detail": "Demo.text_clf_utils",
        "documentation": {}
    },
    {
        "label": "trans_prob",
        "kind": 2,
        "importPath": "Demo.text_clf_utils",
        "description": "Demo.text_clf_utils",
        "peekOfCode": "def trans_prob(probs):\n    transed_prob = [[0.] * len(probs) for _ in range(len(probs[0]))]\n    for idx, res in enumerate(probs):\n        for i, p in enumerate(res):\n            if len(p) < 2: transed_prob[i][idx] = 1. - p[0]\n            else: transed_prob[i][idx] = p[1]\n    return transed_prob\ndef show_test_results(gt, res, prob, class_weight):\n    print (\"precision_micro\" + ':' + '%.2f' % sklearn.metrics.precision_score(gt, res, average = 'micro'))\n    print (\"recall_micro\" + ':' + '%.2f' % sklearn.metrics.recall_score(gt, res, average = 'micro'))",
        "detail": "Demo.text_clf_utils",
        "documentation": {}
    },
    {
        "label": "show_test_results",
        "kind": 2,
        "importPath": "Demo.text_clf_utils",
        "description": "Demo.text_clf_utils",
        "peekOfCode": "def show_test_results(gt, res, prob, class_weight):\n    print (\"precision_micro\" + ':' + '%.2f' % sklearn.metrics.precision_score(gt, res, average = 'micro'))\n    print (\"recall_micro\" + ':' + '%.2f' % sklearn.metrics.recall_score(gt, res, average = 'micro'))\n    print (\"f1_micro\" + ':' + '%.2f' % sklearn.metrics.f1_score(gt, res, average = 'micro'))\n    print (\"precision_weighted\" + ':' + '%.2f' % weighted_precision(gt, res, class_weight))\n    print (\"recall_weighted\" + ':' + '%.2f' % weighted_recall(gt, res, class_weight))\n    print (\"f1_weighted\" + ':' + '%.2f' % weighted_f1(gt, res, class_weight))\n    print (\"risk_factor\" + ':' + '%.4f' % risk_factor(gt, prob, res))\ndef filtering(res, prob, threshold):\n    for idx, case in enumerate(res):",
        "detail": "Demo.text_clf_utils",
        "documentation": {}
    },
    {
        "label": "filtering",
        "kind": 2,
        "importPath": "Demo.text_clf_utils",
        "description": "Demo.text_clf_utils",
        "peekOfCode": "def filtering(res, prob, threshold):\n    for idx, case in enumerate(res):\n        for i in range(len(case)):\n            if prob[idx][i] < threshold:\n                res[idx][i] = 0\n                prob[idx][i] = 0.\n    return res, prob",
        "detail": "Demo.text_clf_utils",
        "documentation": {}
    },
    {
        "label": "stemmer",
        "kind": 5,
        "importPath": "Demo.text_clf_utils",
        "description": "Demo.text_clf_utils",
        "peekOfCode": "stemmer = SnowballStemmer(\"english\")\ndef stemming(sentence):\n    stemSentence = \"\"\n    for word in sentence.split():\n        stem = stemmer.stem(word)\n        stemSentence += stem\n        stemSentence += \" \"\n    stemSentence = stemSentence.strip()\n    return stemSentence\ndef weighted_precision_recall_f1_util (y_test, y_pre, weight = None):",
        "detail": "Demo.text_clf_utils",
        "documentation": {}
    },
    {
        "label": "robust_peaks_detection_zscore",
        "kind": 2,
        "importPath": "Demo.udp_server",
        "description": "Demo.udp_server",
        "peekOfCode": "def robust_peaks_detection_zscore(df,lag,threshold,influence):\n    rate = thresholding_algo(df['Value_Magnitude_XYZ'],lag,threshold,influence)\n    indices = np.where(rate['signals'] == 1)[0]\n    robust_peaks_time = df.iloc[indices]['EPOCH_Time_ms']\n    robust_peaks_value = df.iloc[indices]['Value_Magnitude_XYZ']\n    indices = np.where(rate['signals'] == -1)[0]\n    robust_valleys_time = df.iloc[indices]['EPOCH_Time_ms']\n    robust_valleys_value = df.iloc[indices]['Value_Magnitude_XYZ']\n    # # #Plotting\n    # fig = plt.figure()",
        "detail": "Demo.udp_server",
        "documentation": {}
    },
    {
        "label": "find_peaks_valleys_cwt",
        "kind": 2,
        "importPath": "Demo.udp_server",
        "description": "Demo.udp_server",
        "peekOfCode": "def find_peaks_valleys_cwt(df):\n    peaks = find_peaks_cwt(df['Value_Magnitude_XYZ'],np.arange(100,2000))\n    print(peaks)\n#     height = peaks[1][peak_heights] #list of the heights of the peaks\n#     peak_pos = data_frame.iloc[peaks[0]] #list of the peaks positions\n#     # #Finding the minima\n#     y2 = df[Value_Magnitude_XYZ]*-1\n#     minima = find_peaks(y2,height = -5, distance = 1)\n#     min_pos = data_frame.iloc[minima[0]] #list of the minima positions\n#     min_height = y2.iloc[minima[0]] #list of the mirrored minima heights",
        "detail": "Demo.udp_server",
        "documentation": {}
    },
    {
        "label": "find_peaks_valleys",
        "kind": 2,
        "importPath": "Demo.udp_server",
        "description": "Demo.udp_server",
        "peekOfCode": "def find_peaks_valleys(df,height,distance,prominence):\n    peaks = find_peaks(df['Value_Magnitude_XYZ'], height = height,  distance = distance,prominence=prominence)\n    height = peaks[1]['peak_heights'] #list of the heights of the peaks\n    peak_pos = data_frame.iloc[peaks[0]] #list of the peaks positions\n    # #Finding the minima\n    y2 = df['Value_Magnitude_XYZ']*-1\n    minima = find_peaks(y2,height = -5, distance = 1)\n    min_pos = data_frame.iloc[minima[0]] #list of the minima positions\n    min_height = y2.iloc[minima[0]] #list of the mirrored minima heights\n    return [peak_pos,min_pos,height,min_height]",
        "detail": "Demo.udp_server",
        "documentation": {}
    },
    {
        "label": "find_cpr_rate",
        "kind": 2,
        "importPath": "Demo.udp_server",
        "description": "Demo.udp_server",
        "peekOfCode": "def find_cpr_rate(peaks):\n    time_diff_between_peaks=np.diff(peaks['EPOCH_Time_ms'])\n    is_not_empty=len(time_diff_between_peaks) > 0\n    if is_not_empty:\n        avg_time_btwn_peaks_in_seconds_scipy = np.average(time_diff_between_peaks)/1000\n        # # printing difference list\n        print (\"Average time between peaks in seconds (scipy): \", str(avg_time_btwn_peaks_in_seconds_scipy))\n        print(\"CPR Rate Per Minute (scipy): \", (1/avg_time_btwn_peaks_in_seconds_scipy)*60)\nlocalIP     = \"127.0.0.1\"\nlocalPort   = 20002",
        "detail": "Demo.udp_server",
        "documentation": {}
    },
    {
        "label": "UDPServerSocket",
        "kind": 5,
        "importPath": "Demo.udp_server",
        "description": "Demo.udp_server",
        "peekOfCode": "UDPServerSocket = socket.socket(family=socket.AF_INET, type=socket.SOCK_DGRAM)\n# Bind to address and ip\nUDPServerSocket.bind((localIP, localPort))\nprint(\"UDP server up and listening\")\nbuffer = collections.deque([])\n# Listen for incoming datagrams\nwhile(True):\n    bytesAddressPair = UDPServerSocket.recvfrom(bufferSize)\n    message = bytesAddressPair[0]\n    address = bytesAddressPair[1]",
        "detail": "Demo.udp_server",
        "documentation": {}
    },
    {
        "label": "buffer",
        "kind": 5,
        "importPath": "Demo.udp_server",
        "description": "Demo.udp_server",
        "peekOfCode": "buffer = collections.deque([])\n# Listen for incoming datagrams\nwhile(True):\n    bytesAddressPair = UDPServerSocket.recvfrom(bufferSize)\n    message = bytesAddressPair[0]\n    address = bytesAddressPair[1]\n    # clientMsg = Message from Client:{}.format(message.decode())\n    clientIP  = \"Client IP Address:{}\".format(address)\n    buffer.append(message.decode())\n    current_buffer_size = len(buffer)",
        "detail": "Demo.udp_server",
        "documentation": {}
    },
    {
        "label": "ThreadPhoneVid",
        "kind": 6,
        "importPath": "Demo.video_stream_phone",
        "description": "Demo.video_stream_phone",
        "peekOfCode": "class ThreadPhoneVid(QThread):\n    def __init__(self, var, bool):\n        self.data_path_str = var + \"phonevideodata_fordepth/\"\n        self.videoStreamBool = bool\n        super().__init__()\n    def run(self):\n        recording_enabled = True\n        frame_index = 0\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)  \n        sock.bind(('0.0.0.0', TCP_PORT))  ",
        "detail": "Demo.video_stream_phone",
        "documentation": {}
    },
    {
        "label": "TCP_IP",
        "kind": 5,
        "importPath": "Demo.video_stream_phone",
        "description": "Demo.video_stream_phone",
        "peekOfCode": "TCP_IP = '127.0.0.1'\nTCP_PORT = 9009\nseq_all=0\ndropped_imgs=0\ntotal_imgs=0\nclass ThreadPhoneVid(QThread):\n    def __init__(self, var, bool):\n        self.data_path_str = var + \"phonevideodata_fordepth/\"\n        self.videoStreamBool = bool\n        super().__init__()",
        "detail": "Demo.video_stream_phone",
        "documentation": {}
    },
    {
        "label": "TCP_PORT",
        "kind": 5,
        "importPath": "Demo.video_stream_phone",
        "description": "Demo.video_stream_phone",
        "peekOfCode": "TCP_PORT = 9009\nseq_all=0\ndropped_imgs=0\ntotal_imgs=0\nclass ThreadPhoneVid(QThread):\n    def __init__(self, var, bool):\n        self.data_path_str = var + \"phonevideodata_fordepth/\"\n        self.videoStreamBool = bool\n        super().__init__()\n    def run(self):",
        "detail": "Demo.video_stream_phone",
        "documentation": {}
    },
    {
        "label": "Thread",
        "kind": 6,
        "importPath": "Demo.video_streaming",
        "description": "Demo.video_streaming",
        "peekOfCode": "class Thread(QThread):\n    def __init__(self, var, bool):\n        self.data_path_str = var + \"videodata/\"\n        self.videoStreamBool = bool\n        super().__init__()\n    changePixmap = pyqtSignal(QImage)\n    changeVisInfo = pyqtSignal(str)\n    def run(self):\n        recording_enabled = True\n        frame_index = 0",
        "detail": "Demo.video_streaming",
        "documentation": {}
    },
    {
        "label": "process_image",
        "kind": 2,
        "importPath": "Demo.video_streaming",
        "description": "Demo.video_streaming",
        "peekOfCode": "def process_image(image):\n    \"\"\"\n    Adds annotations to image for the models you have selected, \n    For now, it just depict results from hand detection\n    \"\"\"\n    global mp_hands #, mp_face_mesh\n    hand_detection_results = None\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    # hand detection\n    with mp_hands.Hands(",
        "detail": "Demo.video_streaming",
        "documentation": {}
    },
    {
        "label": "mp_drawing",
        "kind": 5,
        "importPath": "Demo.video_streaming",
        "description": "Demo.video_streaming",
        "peekOfCode": "mp_drawing = mp.solutions.drawing_utils\nmp_drawing_styles = mp.solutions.drawing_styles\nmp_hands = mp.solutions.hands\nmp_face_mesh = mp.solutions.face_mesh\n# mp_pose = mp.solutions.pose\n#functions and variables for getting video stream from Android camera on AR glasses\nTCP_IP = '127.0.0.1'\nTCP_PORT = 8899\nseq_all=0\ndropped_imgs=0",
        "detail": "Demo.video_streaming",
        "documentation": {}
    },
    {
        "label": "mp_drawing_styles",
        "kind": 5,
        "importPath": "Demo.video_streaming",
        "description": "Demo.video_streaming",
        "peekOfCode": "mp_drawing_styles = mp.solutions.drawing_styles\nmp_hands = mp.solutions.hands\nmp_face_mesh = mp.solutions.face_mesh\n# mp_pose = mp.solutions.pose\n#functions and variables for getting video stream from Android camera on AR glasses\nTCP_IP = '127.0.0.1'\nTCP_PORT = 8899\nseq_all=0\ndropped_imgs=0\ntotal_imgs=0",
        "detail": "Demo.video_streaming",
        "documentation": {}
    },
    {
        "label": "mp_hands",
        "kind": 5,
        "importPath": "Demo.video_streaming",
        "description": "Demo.video_streaming",
        "peekOfCode": "mp_hands = mp.solutions.hands\nmp_face_mesh = mp.solutions.face_mesh\n# mp_pose = mp.solutions.pose\n#functions and variables for getting video stream from Android camera on AR glasses\nTCP_IP = '127.0.0.1'\nTCP_PORT = 8899\nseq_all=0\ndropped_imgs=0\ntotal_imgs=0\ndef process_image(image):",
        "detail": "Demo.video_streaming",
        "documentation": {}
    },
    {
        "label": "mp_face_mesh",
        "kind": 5,
        "importPath": "Demo.video_streaming",
        "description": "Demo.video_streaming",
        "peekOfCode": "mp_face_mesh = mp.solutions.face_mesh\n# mp_pose = mp.solutions.pose\n#functions and variables for getting video stream from Android camera on AR glasses\nTCP_IP = '127.0.0.1'\nTCP_PORT = 8899\nseq_all=0\ndropped_imgs=0\ntotal_imgs=0\ndef process_image(image):\n    \"\"\"",
        "detail": "Demo.video_streaming",
        "documentation": {}
    },
    {
        "label": "TCP_IP",
        "kind": 5,
        "importPath": "Demo.video_streaming",
        "description": "Demo.video_streaming",
        "peekOfCode": "TCP_IP = '127.0.0.1'\nTCP_PORT = 8899\nseq_all=0\ndropped_imgs=0\ntotal_imgs=0\ndef process_image(image):\n    \"\"\"\n    Adds annotations to image for the models you have selected, \n    For now, it just depict results from hand detection\n    \"\"\"",
        "detail": "Demo.video_streaming",
        "documentation": {}
    },
    {
        "label": "TCP_PORT",
        "kind": 5,
        "importPath": "Demo.video_streaming",
        "description": "Demo.video_streaming",
        "peekOfCode": "TCP_PORT = 8899\nseq_all=0\ndropped_imgs=0\ntotal_imgs=0\ndef process_image(image):\n    \"\"\"\n    Adds annotations to image for the models you have selected, \n    For now, it just depict results from hand detection\n    \"\"\"\n    global mp_hands #, mp_face_mesh",
        "detail": "Demo.video_streaming",
        "documentation": {}
    },
    {
        "label": "hasNumbers",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "def hasNumbers(inputString):\n    return any(char.isdigit() for char in inputString)\ndef hasStopwords(inputString):\n    stops = loadStopWords()\n    tokens = inputString.split(\" \")\n    for item in stops:\n        if item in tokens:\n            return True\n    return False\ndef loadData(filepath, remove_stopwords=False):",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "hasStopwords",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "def hasStopwords(inputString):\n    stops = loadStopWords()\n    tokens = inputString.split(\" \")\n    for item in stops:\n        if item in tokens:\n            return True\n    return False\ndef loadData(filepath, remove_stopwords=False):\n##    dataList = list()\n    dataList = [[] for x in range(11891)]",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "loadData",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "def loadData(filepath, remove_stopwords=False):\n##    dataList = list()\n    dataList = [[] for x in range(11891)]\n    rfile = open(filepath, \"r\")\n    for line in rfile:\n        line2 = line.lower().strip()\n        line2 = re.sub(\"[^a-zA-Z0-9_]\",\" \", line2)\n        innerList = line2.split()\n        if remove_stopwords:\n            stops = loadStopWords()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "loadStringData",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "def loadStringData(filepath):\n##    dataList = list()\n##    dataList = [[] for x in range(11891)]\n    dataString = \"\"\n    rfile = open(filepath, \"r\")\n    for line in rfile:\n        line2 = line.lower().strip()\n        line2 = re.sub(\"[^a-zA-Z0-9_]\",\" \", line2)\n        dataString += \" \"+ line2\n    rfile.close()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "loadStringData2",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "def loadStringData2(filepath):\n##    dataList = list()\n##    dataList = [[] for x in range(11891)]\n    dataString = \"\"\n    rfile = open(filepath, \"r\")\n    for line in rfile:\n        line = line.lower()\n        dataString += line\n    rfile.close()\n    return dataString",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "loadStopWords",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "def loadStopWords():\n    filePath = \"/Users/sarahmasudpreum/Desktop/PythonTCode/general/stopwords.txt\"\n    rfile = open(filePath, \"r\")\n    stopWords = list()\n    for line in rfile:\n        stopWords.append(line.strip())\n    rfile.close()\n    return stopWords\ndef loadOriginalConceptListPhrase(filepath):\n    conceptList = list()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "loadOriginalConceptListPhrase",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "def loadOriginalConceptListPhrase(filepath):\n    conceptList = list()\n    rfile = open(filepath, \"r\")\n    for line in rfile:\n        item = line.strip()\n        if item not in conceptList:\n            conceptList.append(item)\n    rfile.close()\n    return conceptList\ndef loadOriginalConceptList(filepath):",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "loadOriginalConceptList",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "def loadOriginalConceptList(filepath):\n    conceptList = list()\n    rfile = open(filepath, \"r\")\n    for line in rfile:\n        concept = line.split(\"\\t\")[0].lower().strip()\n        items = concept.split(\" \")\n        for item in items: \n            if item not in conceptList:\n                conceptList.append(item)\n    stops = loadStopWords()##set(stopwords.words(\"english\"))",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "loadSimpleListPhras",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "def loadSimpleListPhrase (filePath):\n    rfile = open(filePath, \"r\")\n    itemList = list()\n##    table = str.maketrans({key: None for key in string.punctuation})\n    for line in rfile:\n##        line = re.sub(\"[^a-zA-Z0-9_]\",\" \", line)\n        item = line.lower().strip()\n        if item not in itemList:\n            itemList.append(item)\n    rfile.close()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "computeFrequenc",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "def computeFrequency (gtConceptList, frequencyList, wordList,fileName=\"\"):\n    wordFrequencyList = list(repeat(0,len(gtConceptList)))\n    for item in wordList:\n        for i in range (0,len(gtConceptList)):\n            gTerm = str(gtConceptList[i]).strip()\n            item = item.strip()\n##            if len(gTerm.split(\" \")) > 1:\n            if item.find(gTerm) is not -1: #or gTerm.find(item) is not -1:\n                wordFrequencyList[i] = frequencyList[i]\n    # filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/candidatePhrases/\"",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "computeF",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "def computeFP (filePath, conceptList, gtConceptList):\n##    filteredConceptList = [x.strip() for x in conceptList if x not in gtConceptList]\n    tpConceptList = list()\n    for item in conceptList:\n        for i in range (0,len(gtConceptList)):\n            gTerm = str(gtConceptList[i]).strip()\n            item = item.strip()\n##            if len(gTerm.split(\" \")) > 1:\n            if item.find(gTerm) is not -1: #or gTerm.find(item) is not -1:\n                tpConceptList.append(item)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "computePrecisio",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "def computePrecision (TPlist, FPList):\n    t = sum(TPlist) + sum(FPList)\n    p = float(0)\n    if t>0:\n        p = sum(TPlist)/float( t )\n    return round( p,4 )\ndef computeRecall (TPlist, denom):\n    p = sum(TPlist)/float(denom)\n    return round( p,4 )\ndef score(seq1, seq2, model):",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "computeRecal",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "def computeRecall (TPlist, denom):\n    p = sum(TPlist)/float(denom)\n    return round( p,4 )\ndef score(seq1, seq2, model):\n\t'''prepares inputs for scoring'''\n\tseq1_word_list = word_tokenize(seq1.strip().lower())\n\tseq2_word_list = word_tokenize(seq2.strip().lower())\n\treturn sim_score(seq1_word_list, seq2_word_list,model)\ndef sim_score(wordlist1, wordlist2,model):\n\t'''calculates average maximum similarity between two phrase inputs'''",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "score",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "def score(seq1, seq2, model):\n\t'''prepares inputs for scoring'''\n\tseq1_word_list = word_tokenize(seq1.strip().lower())\n\tseq2_word_list = word_tokenize(seq2.strip().lower())\n\treturn sim_score(seq1_word_list, seq2_word_list,model)\ndef sim_score(wordlist1, wordlist2,model):\n\t'''calculates average maximum similarity between two phrase inputs'''\n\tmaxes = []\n\tfor word in wordlist1:\n\t\tcur_max = 0",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "sim_score",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "def sim_score(wordlist1, wordlist2,model):\n\t'''calculates average maximum similarity between two phrase inputs'''\n\tmaxes = []\n\tfor word in wordlist1:\n\t\tcur_max = 0\n\t\tfor word2 in wordlist2:\n\t\t\tif word == word2: #case where words are identical\n\t\t\t\tsim = 1\n\t\t\t\tcur_max = sim\n\t\t\telif word in model and word2 in model:\t",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "sim_sum_score",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "def sim_sum_score(wordlist1, wordlist2,model):\n    '''calculates similarity between two phrase inputs'''\n    x1 = np.zeros(100)\n    x2 = np.zeros(100)\n    for word in wordlist1:\n        if word in model:\n            x1 = x1 + model[word]\n    if len(wordlist1) > 0:\n        x1 = x1/len(wordlist1)\n    for word in wordlist2:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "sim_avg_score",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "def sim_avg_score(wordlist1, wordlist2,model):\n\tavgdist = []\n\tfor word in wordlist1:\n            cur_dist = 0\n            for word2 in wordlist2:\n                if word == word2:\n                    sim = 1\n                    cur_dist += sim\n                elif word in model and word2 in model:\t\n                    sim = model.similarity(word, word2) #calculate cosine similarity score for words",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "model_most_similar_phrases",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "def model_most_similar_phrases(model, inputSpace, seedConcepts, topn):\n    candidateSet = set()\n    for sc in seedConcepts:\n        scoreList = list(repeat(0,len(inputSpace)))\n        for i in range(0,len(inputSpace)):\n            ip = inputSpace[i]\n            scoreList[i] = score(ip, sc, model)\n        finalList = list(zip(inputSpace, scoreList))\n        finalList.sort(key=lambda x: x[1])\n        finalList = finalList[-topn:]",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "\tseq1_word_list",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "\tseq1_word_list = word_tokenize(seq1.strip().lower())\n\tseq2_word_list = word_tokenize(seq2.strip().lower())\n\treturn sim_score(seq1_word_list, seq2_word_list,model)\ndef sim_score(wordlist1, wordlist2,model):\n\t'''calculates average maximum similarity between two phrase inputs'''\n\tmaxes = []\n\tfor word in wordlist1:\n\t\tcur_max = 0\n\t\tfor word2 in wordlist2:\n\t\t\tif word == word2: #case where words are identical",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "\tseq2_word_list",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "\tseq2_word_list = word_tokenize(seq2.strip().lower())\n\treturn sim_score(seq1_word_list, seq2_word_list,model)\ndef sim_score(wordlist1, wordlist2,model):\n\t'''calculates average maximum similarity between two phrase inputs'''\n\tmaxes = []\n\tfor word in wordlist1:\n\t\tcur_max = 0\n\t\tfor word2 in wordlist2:\n\t\t\tif word == word2: #case where words are identical\n\t\t\t\tsim = 1",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "\tmaxes",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "\tmaxes = []\n\tfor word in wordlist1:\n\t\tcur_max = 0\n\t\tfor word2 in wordlist2:\n\t\t\tif word == word2: #case where words are identical\n\t\t\t\tsim = 1\n\t\t\t\tcur_max = sim\n\t\t\telif word in model and word2 in model:\t\n\t\t\t\tsim = model.similarity(word, word2) #calculate cosine similarity score for words\n\t\t\t\tif sim > cur_max:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "\t\tcur_max",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "\t\tcur_max = 0\n\t\tfor word2 in wordlist2:\n\t\t\tif word == word2: #case where words are identical\n\t\t\t\tsim = 1\n\t\t\t\tcur_max = sim\n\t\t\telif word in model and word2 in model:\t\n\t\t\t\tsim = model.similarity(word, word2) #calculate cosine similarity score for words\n\t\t\t\tif sim > cur_max:\n\t\t\t\t\tcur_max = sim\n\t\tif cur_max != 0:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "\t\t\t\tsim",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "\t\t\t\tsim = 1\n\t\t\t\tcur_max = sim\n\t\t\telif word in model and word2 in model:\t\n\t\t\t\tsim = model.similarity(word, word2) #calculate cosine similarity score for words\n\t\t\t\tif sim > cur_max:\n\t\t\t\t\tcur_max = sim\n\t\tif cur_max != 0:\n\t\t\tmaxes.append(cur_max)\n\tif sum(maxes) == 0:\n\t    return 0",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "\t\t\t\tcur_max",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "\t\t\t\tcur_max = sim\n\t\t\telif word in model and word2 in model:\t\n\t\t\t\tsim = model.similarity(word, word2) #calculate cosine similarity score for words\n\t\t\t\tif sim > cur_max:\n\t\t\t\t\tcur_max = sim\n\t\tif cur_max != 0:\n\t\t\tmaxes.append(cur_max)\n\tif sum(maxes) == 0:\n\t    return 0\n\treturn float(sum(maxes)) / len(maxes)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "\t\t\t\tsim",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "\t\t\t\tsim = model.similarity(word, word2) #calculate cosine similarity score for words\n\t\t\t\tif sim > cur_max:\n\t\t\t\t\tcur_max = sim\n\t\tif cur_max != 0:\n\t\t\tmaxes.append(cur_max)\n\tif sum(maxes) == 0:\n\t    return 0\n\treturn float(sum(maxes)) / len(maxes)\ndef sim_sum_score(wordlist1, wordlist2,model):\n    '''calculates similarity between two phrase inputs'''",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "\t\t\t\t\tcur_max",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "\t\t\t\t\tcur_max = sim\n\t\tif cur_max != 0:\n\t\t\tmaxes.append(cur_max)\n\tif sum(maxes) == 0:\n\t    return 0\n\treturn float(sum(maxes)) / len(maxes)\ndef sim_sum_score(wordlist1, wordlist2,model):\n    '''calculates similarity between two phrase inputs'''\n    x1 = np.zeros(100)\n    x2 = np.zeros(100)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "\tavgdist",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "\tavgdist = []\n\tfor word in wordlist1:\n            cur_dist = 0\n            for word2 in wordlist2:\n                if word == word2:\n                    sim = 1\n                    cur_dist += sim\n                elif word in model and word2 in model:\t\n                    sim = model.similarity(word, word2) #calculate cosine similarity score for words\n                    cur_dist += sim",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "datafileRootPath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "datafileRootPath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/wordEmbeddingModel/Data/\"\ndataFileName = \"ForumallDataIn1.txt\"\n# filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/trainingData/DataLines.txt\" \nfilePath = datafileRootPath + dataFileName\ndataString = loadStringData2(filePath)\n# dataString = re.sub(\"[^a-zA-Z0-9]\",\" \", dataString)\n##        tokens = line.split(\" \")\n##        tokens = [w for w in tokens if w.find(\" \") is -1 and not w.isdigit() and not w in stops]\n##        line = \" \".join(tokens)\n# sent_tokenize_list = sent_tokenize(dataString)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "dataFileName",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "dataFileName = \"ForumallDataIn1.txt\"\n# filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/trainingData/DataLines.txt\" \nfilePath = datafileRootPath + dataFileName\ndataString = loadStringData2(filePath)\n# dataString = re.sub(\"[^a-zA-Z0-9]\",\" \", dataString)\n##        tokens = line.split(\" \")\n##        tokens = [w for w in tokens if w.find(\" \") is -1 and not w.isdigit() and not w in stops]\n##        line = \" \".join(tokens)\n# sent_tokenize_list = sent_tokenize(dataString)\n# tokens = nltk.wordpunct_tokenize(dataString) #word_tokenize",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "filePath = datafileRootPath + dataFileName\ndataString = loadStringData2(filePath)\n# dataString = re.sub(\"[^a-zA-Z0-9]\",\" \", dataString)\n##        tokens = line.split(\" \")\n##        tokens = [w for w in tokens if w.find(\" \") is -1 and not w.isdigit() and not w in stops]\n##        line = \" \".join(tokens)\n# sent_tokenize_list = sent_tokenize(dataString)\n# tokens = nltk.wordpunct_tokenize(dataString) #word_tokenize\ntokens = word_tokenize(dataString) #word_tokenize\ntokens = [w for w in tokens if not w.isdigit()]",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "dataString",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "dataString = loadStringData2(filePath)\n# dataString = re.sub(\"[^a-zA-Z0-9]\",\" \", dataString)\n##        tokens = line.split(\" \")\n##        tokens = [w for w in tokens if w.find(\" \") is -1 and not w.isdigit() and not w in stops]\n##        line = \" \".join(tokens)\n# sent_tokenize_list = sent_tokenize(dataString)\n# tokens = nltk.wordpunct_tokenize(dataString) #word_tokenize\ntokens = word_tokenize(dataString) #word_tokenize\ntokens = [w for w in tokens if not w.isdigit()]\nstops = loadStopWords()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "tokens = word_tokenize(dataString) #word_tokenize\ntokens = [w for w in tokens if not w.isdigit()]\nstops = loadStopWords()\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/newConceptList_Sept2018.txt\"\nconceptList = loadOriginalConceptList(filePath)\nconceptList = [w for w in conceptList if w.find(\" \") is -1]\nfinder = BigramCollocationFinder.from_words(tokens)\nbigram_measures = nltk.collocations.BigramAssocMeasures()\n##sorted(finder.nbest(bigram_measures.raw_freq, 100))\nscored = finder.score_ngrams(bigram_measures.raw_freq)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "tokens = [w for w in tokens if not w.isdigit()]\nstops = loadStopWords()\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/newConceptList_Sept2018.txt\"\nconceptList = loadOriginalConceptList(filePath)\nconceptList = [w for w in conceptList if w.find(\" \") is -1]\nfinder = BigramCollocationFinder.from_words(tokens)\nbigram_measures = nltk.collocations.BigramAssocMeasures()\n##sorted(finder.nbest(bigram_measures.raw_freq, 100))\nscored = finder.score_ngrams(bigram_measures.raw_freq)\nbigramSet = set(sorted(bigram for bigram, score in scored))  ",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "stops",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "stops = loadStopWords()\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/newConceptList_Sept2018.txt\"\nconceptList = loadOriginalConceptList(filePath)\nconceptList = [w for w in conceptList if w.find(\" \") is -1]\nfinder = BigramCollocationFinder.from_words(tokens)\nbigram_measures = nltk.collocations.BigramAssocMeasures()\n##sorted(finder.nbest(bigram_measures.raw_freq, 100))\nscored = finder.score_ngrams(bigram_measures.raw_freq)\nbigramSet = set(sorted(bigram for bigram, score in scored))  \nbigramList = list(bigramSet)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/newConceptList_Sept2018.txt\"\nconceptList = loadOriginalConceptList(filePath)\nconceptList = [w for w in conceptList if w.find(\" \") is -1]\nfinder = BigramCollocationFinder.from_words(tokens)\nbigram_measures = nltk.collocations.BigramAssocMeasures()\n##sorted(finder.nbest(bigram_measures.raw_freq, 100))\nscored = finder.score_ngrams(bigram_measures.raw_freq)\nbigramSet = set(sorted(bigram for bigram, score in scored))  \nbigramList = list(bigramSet)\nlen(bigramList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "conceptList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "conceptList = loadOriginalConceptList(filePath)\nconceptList = [w for w in conceptList if w.find(\" \") is -1]\nfinder = BigramCollocationFinder.from_words(tokens)\nbigram_measures = nltk.collocations.BigramAssocMeasures()\n##sorted(finder.nbest(bigram_measures.raw_freq, 100))\nscored = finder.score_ngrams(bigram_measures.raw_freq)\nbigramSet = set(sorted(bigram for bigram, score in scored))  \nbigramList = list(bigramSet)\nlen(bigramList)\nfiltBigramList = list()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "conceptList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "conceptList = [w for w in conceptList if w.find(\" \") is -1]\nfinder = BigramCollocationFinder.from_words(tokens)\nbigram_measures = nltk.collocations.BigramAssocMeasures()\n##sorted(finder.nbest(bigram_measures.raw_freq, 100))\nscored = finder.score_ngrams(bigram_measures.raw_freq)\nbigramSet = set(sorted(bigram for bigram, score in scored))  \nbigramList = list(bigramSet)\nlen(bigramList)\nfiltBigramList = list()\nfiltBigramList= [u+\" \"+v for u,v in bigramList if not u in stops and not v in stops and (u in conceptList or v in conceptList)]",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "finder",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "finder = BigramCollocationFinder.from_words(tokens)\nbigram_measures = nltk.collocations.BigramAssocMeasures()\n##sorted(finder.nbest(bigram_measures.raw_freq, 100))\nscored = finder.score_ngrams(bigram_measures.raw_freq)\nbigramSet = set(sorted(bigram for bigram, score in scored))  \nbigramList = list(bigramSet)\nlen(bigramList)\nfiltBigramList = list()\nfiltBigramList= [u+\" \"+v for u,v in bigramList if not u in stops and not v in stops and (u in conceptList or v in conceptList)]\nlen(filtBigramList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "bigram_measures",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "bigram_measures = nltk.collocations.BigramAssocMeasures()\n##sorted(finder.nbest(bigram_measures.raw_freq, 100))\nscored = finder.score_ngrams(bigram_measures.raw_freq)\nbigramSet = set(sorted(bigram for bigram, score in scored))  \nbigramList = list(bigramSet)\nlen(bigramList)\nfiltBigramList = list()\nfiltBigramList= [u+\" \"+v for u,v in bigramList if not u in stops and not v in stops and (u in conceptList or v in conceptList)]\nlen(filtBigramList)\nfiltBigramList= [w for w in filtBigramList if len(set('[~!@#$%^&*()_-+{}<>\":,.;?/|\\']+$').intersection(w)) == 0 ]",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "scored",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "scored = finder.score_ngrams(bigram_measures.raw_freq)\nbigramSet = set(sorted(bigram for bigram, score in scored))  \nbigramList = list(bigramSet)\nlen(bigramList)\nfiltBigramList = list()\nfiltBigramList= [u+\" \"+v for u,v in bigramList if not u in stops and not v in stops and (u in conceptList or v in conceptList)]\nlen(filtBigramList)\nfiltBigramList= [w for w in filtBigramList if len(set('[~!@#$%^&*()_-+{}<>\":,.;?/|\\']+$').intersection(w)) == 0 ]\n# filtBigramList= [w for w in filtBigramList if w.find(\"patient\") is -1 and w.find(\" pt\") is -1]\nlen(filtBigramList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "bigramSet",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "bigramSet = set(sorted(bigram for bigram, score in scored))  \nbigramList = list(bigramSet)\nlen(bigramList)\nfiltBigramList = list()\nfiltBigramList= [u+\" \"+v for u,v in bigramList if not u in stops and not v in stops and (u in conceptList or v in conceptList)]\nlen(filtBigramList)\nfiltBigramList= [w for w in filtBigramList if len(set('[~!@#$%^&*()_-+{}<>\":,.;?/|\\']+$').intersection(w)) == 0 ]\n# filtBigramList= [w for w in filtBigramList if w.find(\"patient\") is -1 and w.find(\" pt\") is -1]\nlen(filtBigramList)\nfiltBigramList = list(set(filtBigramList))",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "bigramList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "bigramList = list(bigramSet)\nlen(bigramList)\nfiltBigramList = list()\nfiltBigramList= [u+\" \"+v for u,v in bigramList if not u in stops and not v in stops and (u in conceptList or v in conceptList)]\nlen(filtBigramList)\nfiltBigramList= [w for w in filtBigramList if len(set('[~!@#$%^&*()_-+{}<>\":,.;?/|\\']+$').intersection(w)) == 0 ]\n# filtBigramList= [w for w in filtBigramList if w.find(\"patient\") is -1 and w.find(\" pt\") is -1]\nlen(filtBigramList)\nfiltBigramList = list(set(filtBigramList))\nfinder = TrigramCollocationFinder.from_words(tokens)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "filtBigramList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "filtBigramList = list()\nfiltBigramList= [u+\" \"+v for u,v in bigramList if not u in stops and not v in stops and (u in conceptList or v in conceptList)]\nlen(filtBigramList)\nfiltBigramList= [w for w in filtBigramList if len(set('[~!@#$%^&*()_-+{}<>\":,.;?/|\\']+$').intersection(w)) == 0 ]\n# filtBigramList= [w for w in filtBigramList if w.find(\"patient\") is -1 and w.find(\" pt\") is -1]\nlen(filtBigramList)\nfiltBigramList = list(set(filtBigramList))\nfinder = TrigramCollocationFinder.from_words(tokens)\ntrigram_measures = nltk.collocations.TrigramAssocMeasures()\nscored = finder.score_ngrams(trigram_measures.raw_freq)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "filtBigramList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "filtBigramList = list(set(filtBigramList))\nfinder = TrigramCollocationFinder.from_words(tokens)\ntrigram_measures = nltk.collocations.TrigramAssocMeasures()\nscored = finder.score_ngrams(trigram_measures.raw_freq)\ntrigramSet = set(sorted(trigram for trigram, score in scored))  \ntrigramList = list(trigramSet)\nlen(trigramList)\nfiltTrigramList = list()\nfiltTrigramList= [u+\" \"+v+\" \"+w for u,v,w in trigramList if not u in stops and not v in stops and not w in stops and (u in conceptList or v in conceptList or w in conceptList)]\nlen(filtTrigramList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "finder",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "finder = TrigramCollocationFinder.from_words(tokens)\ntrigram_measures = nltk.collocations.TrigramAssocMeasures()\nscored = finder.score_ngrams(trigram_measures.raw_freq)\ntrigramSet = set(sorted(trigram for trigram, score in scored))  \ntrigramList = list(trigramSet)\nlen(trigramList)\nfiltTrigramList = list()\nfiltTrigramList= [u+\" \"+v+\" \"+w for u,v,w in trigramList if not u in stops and not v in stops and not w in stops and (u in conceptList or v in conceptList or w in conceptList)]\nlen(filtTrigramList)\nfiltTrigramList= [w for w in filtTrigramList if len(set('[~!@#$%^&*()_-+{}<>\":,.;?/|\\']+$').intersection(w)) == 0 ]",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "trigram_measures",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "trigram_measures = nltk.collocations.TrigramAssocMeasures()\nscored = finder.score_ngrams(trigram_measures.raw_freq)\ntrigramSet = set(sorted(trigram for trigram, score in scored))  \ntrigramList = list(trigramSet)\nlen(trigramList)\nfiltTrigramList = list()\nfiltTrigramList= [u+\" \"+v+\" \"+w for u,v,w in trigramList if not u in stops and not v in stops and not w in stops and (u in conceptList or v in conceptList or w in conceptList)]\nlen(filtTrigramList)\nfiltTrigramList= [w for w in filtTrigramList if len(set('[~!@#$%^&*()_-+{}<>\":,.;?/|\\']+$').intersection(w)) == 0 ]\n# filtTrigramList = [w for w in filtTrigramList if w.find(\"patient\") is -1 and w.find(\" pt\") is -1] ",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "scored",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "scored = finder.score_ngrams(trigram_measures.raw_freq)\ntrigramSet = set(sorted(trigram for trigram, score in scored))  \ntrigramList = list(trigramSet)\nlen(trigramList)\nfiltTrigramList = list()\nfiltTrigramList= [u+\" \"+v+\" \"+w for u,v,w in trigramList if not u in stops and not v in stops and not w in stops and (u in conceptList or v in conceptList or w in conceptList)]\nlen(filtTrigramList)\nfiltTrigramList= [w for w in filtTrigramList if len(set('[~!@#$%^&*()_-+{}<>\":,.;?/|\\']+$').intersection(w)) == 0 ]\n# filtTrigramList = [w for w in filtTrigramList if w.find(\"patient\") is -1 and w.find(\" pt\") is -1] \n##filtTrigramList= [u+\" \"+v+\" \"+w for u,v,w in trigramList if not u in stops and not w in stops and (u in conceptList or v in conceptList or w in conceptList)]",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "trigramSet",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "trigramSet = set(sorted(trigram for trigram, score in scored))  \ntrigramList = list(trigramSet)\nlen(trigramList)\nfiltTrigramList = list()\nfiltTrigramList= [u+\" \"+v+\" \"+w for u,v,w in trigramList if not u in stops and not v in stops and not w in stops and (u in conceptList or v in conceptList or w in conceptList)]\nlen(filtTrigramList)\nfiltTrigramList= [w for w in filtTrigramList if len(set('[~!@#$%^&*()_-+{}<>\":,.;?/|\\']+$').intersection(w)) == 0 ]\n# filtTrigramList = [w for w in filtTrigramList if w.find(\"patient\") is -1 and w.find(\" pt\") is -1] \n##filtTrigramList= [u+\" \"+v+\" \"+w for u,v,w in trigramList if not u in stops and not w in stops and (u in conceptList or v in conceptList or w in conceptList)]\nfiltTrigramList = list(set(filtTrigramList))",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "trigramList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "trigramList = list(trigramSet)\nlen(trigramList)\nfiltTrigramList = list()\nfiltTrigramList= [u+\" \"+v+\" \"+w for u,v,w in trigramList if not u in stops and not v in stops and not w in stops and (u in conceptList or v in conceptList or w in conceptList)]\nlen(filtTrigramList)\nfiltTrigramList= [w for w in filtTrigramList if len(set('[~!@#$%^&*()_-+{}<>\":,.;?/|\\']+$').intersection(w)) == 0 ]\n# filtTrigramList = [w for w in filtTrigramList if w.find(\"patient\") is -1 and w.find(\" pt\") is -1] \n##filtTrigramList= [u+\" \"+v+\" \"+w for u,v,w in trigramList if not u in stops and not w in stops and (u in conceptList or v in conceptList or w in conceptList)]\nfiltTrigramList = list(set(filtTrigramList))\nlen(filtTrigramList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "filtTrigramList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "filtTrigramList = list()\nfiltTrigramList= [u+\" \"+v+\" \"+w for u,v,w in trigramList if not u in stops and not v in stops and not w in stops and (u in conceptList or v in conceptList or w in conceptList)]\nlen(filtTrigramList)\nfiltTrigramList= [w for w in filtTrigramList if len(set('[~!@#$%^&*()_-+{}<>\":,.;?/|\\']+$').intersection(w)) == 0 ]\n# filtTrigramList = [w for w in filtTrigramList if w.find(\"patient\") is -1 and w.find(\" pt\") is -1] \n##filtTrigramList= [u+\" \"+v+\" \"+w for u,v,w in trigramList if not u in stops and not w in stops and (u in conceptList or v in conceptList or w in conceptList)]\nfiltTrigramList = list(set(filtTrigramList))\nlen(filtTrigramList)\n##filtTrigramList = set()\n##for trigram in trigramList:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "filtTrigramList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "filtTrigramList = list(set(filtTrigramList))\nlen(filtTrigramList)\n##filtTrigramList = set()\n##for trigram in trigramList:\n##    if not trigram[0] in stops and not trigram[2] in stops:\n##        print(trigram[0]+\" \"+trigram[1]+\" \"+trigram[2] )\n##        if trigram[0] in conceptList or trigram[1] in conceptList or trigram[2] in conceptList:\n##            filtTrigramList.update(trigram[0]+\" \"+trigram[1]+\" \"+trigram[2])\n##            \n##len(filtTrigramList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "##filtTrigramList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "##filtTrigramList = set()\n##for trigram in trigramList:\n##    if not trigram[0] in stops and not trigram[2] in stops:\n##        print(trigram[0]+\" \"+trigram[1]+\" \"+trigram[2] )\n##        if trigram[0] in conceptList or trigram[1] in conceptList or trigram[2] in conceptList:\n##            filtTrigramList.update(trigram[0]+\" \"+trigram[1]+\" \"+trigram[2])\n##            \n##len(filtTrigramList)\n##load GT annotation\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/FormattedTags/allInOne.txt\"",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/FormattedTags/allInOne.txt\"\ngtConceptList = loadSimpleListPhrase(filePath)\nfrequencyList = list(repeat(0,len(gtConceptList)))\n##load protocol concept list\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/newConceptList_Sept2018.txt\"\nconceptList = loadOriginalConceptListPhrase(filePath)\n##load metaMap extended concept list\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/version2/MetaMapOutputsConciseP.txt\" ## MetaMapOutputsConciseW MetaMapOutputs_FilteredST_ConciseW\nrfile1 = open(filePath,\"r\")\nmmWordList = list()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "gtConceptList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "gtConceptList = loadSimpleListPhrase(filePath)\nfrequencyList = list(repeat(0,len(gtConceptList)))\n##load protocol concept list\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/newConceptList_Sept2018.txt\"\nconceptList = loadOriginalConceptListPhrase(filePath)\n##load metaMap extended concept list\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/version2/MetaMapOutputsConciseP.txt\" ## MetaMapOutputsConciseW MetaMapOutputs_FilteredST_ConciseW\nrfile1 = open(filePath,\"r\")\nmmWordList = list()\nfor line in rfile1:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "frequencyList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "frequencyList = list(repeat(0,len(gtConceptList)))\n##load protocol concept list\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/newConceptList_Sept2018.txt\"\nconceptList = loadOriginalConceptListPhrase(filePath)\n##load metaMap extended concept list\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/version2/MetaMapOutputsConciseP.txt\" ## MetaMapOutputsConciseW MetaMapOutputs_FilteredST_ConciseW\nrfile1 = open(filePath,\"r\")\nmmWordList = list()\nfor line in rfile1:\n    word = line.strip()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/newConceptList_Sept2018.txt\"\nconceptList = loadOriginalConceptListPhrase(filePath)\n##load metaMap extended concept list\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/version2/MetaMapOutputsConciseP.txt\" ## MetaMapOutputsConciseW MetaMapOutputs_FilteredST_ConciseW\nrfile1 = open(filePath,\"r\")\nmmWordList = list()\nfor line in rfile1:\n    word = line.strip()\n    if not word.isdigit():\n        if word not in mmWordList:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "conceptList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "conceptList = loadOriginalConceptListPhrase(filePath)\n##load metaMap extended concept list\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/version2/MetaMapOutputsConciseP.txt\" ## MetaMapOutputsConciseW MetaMapOutputs_FilteredST_ConciseW\nrfile1 = open(filePath,\"r\")\nmmWordList = list()\nfor line in rfile1:\n    word = line.strip()\n    if not word.isdigit():\n        if word not in mmWordList:\n            mmWordList.append(word)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/version2/MetaMapOutputsConciseP.txt\" ## MetaMapOutputsConciseW MetaMapOutputs_FilteredST_ConciseW\nrfile1 = open(filePath,\"r\")\nmmWordList = list()\nfor line in rfile1:\n    word = line.strip()\n    if not word.isdigit():\n        if word not in mmWordList:\n            mmWordList.append(word)\nrfile1.close()\n##count ground truth in testData",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "rfile1",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "rfile1 = open(filePath,\"r\")\nmmWordList = list()\nfor line in rfile1:\n    word = line.strip()\n    if not word.isdigit():\n        if word not in mmWordList:\n            mmWordList.append(word)\nrfile1.close()\n##count ground truth in testData\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData.txt\"",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "mmWordList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "mmWordList = list()\nfor line in rfile1:\n    word = line.strip()\n    if not word.isdigit():\n        if word not in mmWordList:\n            mmWordList.append(word)\nrfile1.close()\n##count ground truth in testData\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData.txt\"\nrfile = open(filePath, \"r\")",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData.txt\"\nrfile = open(filePath, \"r\")\nfor line in rfile:\n    for i in range (0,len(gtConceptList)):\n        line = line.lower().strip()\n        if line.find(gtConceptList[i]) is not -1:\n            frequencyList[i] += 1            \nrfile.close()\n##create Output file\ndenom = sum(frequencyList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "rfile",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "rfile = open(filePath, \"r\")\nfor line in rfile:\n    for i in range (0,len(gtConceptList)):\n        line = line.lower().strip()\n        if line.find(gtConceptList[i]) is not -1:\n            frequencyList[i] += 1            \nrfile.close()\n##create Output file\ndenom = sum(frequencyList)\n###calculate phrase similarity",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "denom",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "denom = sum(frequencyList)\n###calculate phrase similarity\n# filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/trainingData/DataLines.txt\" \nfilePath = datafileRootPath + dataFileName\nsentenceList = loadData(filePath, True)\nmin_count = 1\nvectorSize = 150\nwindowSize = 10\ncbow_mean = 0\ndownsampling = 1e-3",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "filePath = datafileRootPath + dataFileName\nsentenceList = loadData(filePath, True)\nmin_count = 1\nvectorSize = 150\nwindowSize = 10\ncbow_mean = 0\ndownsampling = 1e-3\n# model = gensim.models.Word2Vec(sentenceList, sg=0, min_count=min_count, size=vectorSize, window=windowSize, sample = downsampling)\ncandidateList = list()\ninputSpace = set()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "sentenceList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "sentenceList = loadData(filePath, True)\nmin_count = 1\nvectorSize = 150\nwindowSize = 10\ncbow_mean = 0\ndownsampling = 1e-3\n# model = gensim.models.Word2Vec(sentenceList, sg=0, min_count=min_count, size=vectorSize, window=windowSize, sample = downsampling)\ncandidateList = list()\ninputSpace = set()\ninputSpace.update(filtTrigramList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "min_count",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "min_count = 1\nvectorSize = 150\nwindowSize = 10\ncbow_mean = 0\ndownsampling = 1e-3\n# model = gensim.models.Word2Vec(sentenceList, sg=0, min_count=min_count, size=vectorSize, window=windowSize, sample = downsampling)\ncandidateList = list()\ninputSpace = set()\ninputSpace.update(filtTrigramList)\ninputSpace.update(filtBigramList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "vectorSize",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "vectorSize = 150\nwindowSize = 10\ncbow_mean = 0\ndownsampling = 1e-3\n# model = gensim.models.Word2Vec(sentenceList, sg=0, min_count=min_count, size=vectorSize, window=windowSize, sample = downsampling)\ncandidateList = list()\ninputSpace = set()\ninputSpace.update(filtTrigramList)\ninputSpace.update(filtBigramList)\ninputSpace = list(inputSpace)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "windowSize",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "windowSize = 10\ncbow_mean = 0\ndownsampling = 1e-3\n# model = gensim.models.Word2Vec(sentenceList, sg=0, min_count=min_count, size=vectorSize, window=windowSize, sample = downsampling)\ncandidateList = list()\ninputSpace = set()\ninputSpace.update(filtTrigramList)\ninputSpace.update(filtBigramList)\ninputSpace = list(inputSpace)\n##inputSpace = inputSpace[5500:5600]",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "cbow_mean",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "cbow_mean = 0\ndownsampling = 1e-3\n# model = gensim.models.Word2Vec(sentenceList, sg=0, min_count=min_count, size=vectorSize, window=windowSize, sample = downsampling)\ncandidateList = list()\ninputSpace = set()\ninputSpace.update(filtTrigramList)\ninputSpace.update(filtBigramList)\ninputSpace = list(inputSpace)\n##inputSpace = inputSpace[5500:5600]\n##conceptList = conceptList[100:110]",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "downsampling",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "downsampling = 1e-3\n# model = gensim.models.Word2Vec(sentenceList, sg=0, min_count=min_count, size=vectorSize, window=windowSize, sample = downsampling)\ncandidateList = list()\ninputSpace = set()\ninputSpace.update(filtTrigramList)\ninputSpace.update(filtBigramList)\ninputSpace = list(inputSpace)\n##inputSpace = inputSpace[5500:5600]\n##conceptList = conceptList[100:110]\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/RecallPrecision/\"",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "candidateList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "candidateList = list()\ninputSpace = set()\ninputSpace.update(filtTrigramList)\ninputSpace.update(filtBigramList)\ninputSpace = list(inputSpace)\n##inputSpace = inputSpace[5500:5600]\n##conceptList = conceptList[100:110]\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/RecallPrecision/\"\nfilePath = filePath  +\"Results_wordEmbedding_Phrases.txt\"\n# filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/version2/PhraseLevel/conceptPhrase/Results_W2V_CBOW_DomainKnowledge_MM_modFP_newStopWords_MaxAvgDist.txt\"",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "inputSpace",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "inputSpace = set()\ninputSpace.update(filtTrigramList)\ninputSpace.update(filtBigramList)\ninputSpace = list(inputSpace)\n##inputSpace = inputSpace[5500:5600]\n##conceptList = conceptList[100:110]\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/RecallPrecision/\"\nfilePath = filePath  +\"Results_wordEmbedding_Phrases.txt\"\n# filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/version2/PhraseLevel/conceptPhrase/Results_W2V_CBOW_DomainKnowledge_MM_modFP_newStopWords_MaxAvgDist.txt\"\nwfile = open (filePath, \"a\")",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "inputSpace",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "inputSpace = list(inputSpace)\n##inputSpace = inputSpace[5500:5600]\n##conceptList = conceptList[100:110]\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/RecallPrecision/\"\nfilePath = filePath  +\"Results_wordEmbedding_Phrases.txt\"\n# filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/version2/PhraseLevel/conceptPhrase/Results_W2V_CBOW_DomainKnowledge_MM_modFP_newStopWords_MaxAvgDist.txt\"\nwfile = open (filePath, \"a\")\n# wfile.write(\"cbow_mean\\tvectorSize\\twindowLength\\ttopN\\thfListSize\\tRecall\\tPrecision\\n\")\n##wfile.write(\"cbow_mean\\tvectorSize\\twindowLength\\ttopN\\tRecall\\tPrecision\\n\")\n################################################################################################################################################",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "##inputSpace",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "##inputSpace = inputSpace[5500:5600]\n##conceptList = conceptList[100:110]\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/RecallPrecision/\"\nfilePath = filePath  +\"Results_wordEmbedding_Phrases.txt\"\n# filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/version2/PhraseLevel/conceptPhrase/Results_W2V_CBOW_DomainKnowledge_MM_modFP_newStopWords_MaxAvgDist.txt\"\nwfile = open (filePath, \"a\")\n# wfile.write(\"cbow_mean\\tvectorSize\\twindowLength\\ttopN\\thfListSize\\tRecall\\tPrecision\\n\")\n##wfile.write(\"cbow_mean\\tvectorSize\\twindowLength\\ttopN\\tRecall\\tPrecision\\n\")\n################################################################################################################################################\n##candidateList = model_most_similar_phrases(model, inputSpace, conceptList, topn=200)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "##conceptList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "##conceptList = conceptList[100:110]\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/RecallPrecision/\"\nfilePath = filePath  +\"Results_wordEmbedding_Phrases.txt\"\n# filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/version2/PhraseLevel/conceptPhrase/Results_W2V_CBOW_DomainKnowledge_MM_modFP_newStopWords_MaxAvgDist.txt\"\nwfile = open (filePath, \"a\")\n# wfile.write(\"cbow_mean\\tvectorSize\\twindowLength\\ttopN\\thfListSize\\tRecall\\tPrecision\\n\")\n##wfile.write(\"cbow_mean\\tvectorSize\\twindowLength\\ttopN\\tRecall\\tPrecision\\n\")\n################################################################################################################################################\n##candidateList = model_most_similar_phrases(model, inputSpace, conceptList, topn=200)\n##w2vList.update(candidateList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/RecallPrecision/\"\nfilePath = filePath  +\"Results_wordEmbedding_Phrases.txt\"\n# filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/version2/PhraseLevel/conceptPhrase/Results_W2V_CBOW_DomainKnowledge_MM_modFP_newStopWords_MaxAvgDist.txt\"\nwfile = open (filePath, \"a\")\n# wfile.write(\"cbow_mean\\tvectorSize\\twindowLength\\ttopN\\thfListSize\\tRecall\\tPrecision\\n\")\n##wfile.write(\"cbow_mean\\tvectorSize\\twindowLength\\ttopN\\tRecall\\tPrecision\\n\")\n################################################################################################################################################\n##candidateList = model_most_similar_phrases(model, inputSpace, conceptList, topn=200)\n##w2vList.update(candidateList)\n##w2vList = list(w2vList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "filePath = filePath  +\"Results_wordEmbedding_Phrases.txt\"\n# filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/version2/PhraseLevel/conceptPhrase/Results_W2V_CBOW_DomainKnowledge_MM_modFP_newStopWords_MaxAvgDist.txt\"\nwfile = open (filePath, \"a\")\n# wfile.write(\"cbow_mean\\tvectorSize\\twindowLength\\ttopN\\thfListSize\\tRecall\\tPrecision\\n\")\n##wfile.write(\"cbow_mean\\tvectorSize\\twindowLength\\ttopN\\tRecall\\tPrecision\\n\")\n################################################################################################################################################\n##candidateList = model_most_similar_phrases(model, inputSpace, conceptList, topn=200)\n##w2vList.update(candidateList)\n##w2vList = list(w2vList)\n##for item in w2vList:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "wfile",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "wfile = open (filePath, \"a\")\n# wfile.write(\"cbow_mean\\tvectorSize\\twindowLength\\ttopN\\thfListSize\\tRecall\\tPrecision\\n\")\n##wfile.write(\"cbow_mean\\tvectorSize\\twindowLength\\ttopN\\tRecall\\tPrecision\\n\")\n################################################################################################################################################\n##candidateList = model_most_similar_phrases(model, inputSpace, conceptList, topn=200)\n##w2vList.update(candidateList)\n##w2vList = list(w2vList)\n##for item in w2vList:\n####    print(item)\n##    wfile.write(item+\"\\n\")",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "##candidateList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "##candidateList = model_most_similar_phrases(model, inputSpace, conceptList, topn=200)\n##w2vList.update(candidateList)\n##w2vList = list(w2vList)\n##for item in w2vList:\n####    print(item)\n##    wfile.write(item+\"\\n\")\n##wfile.close()\n################################################################################################################################################\nfor cbow_mean in [0]:#[1,0]:\n    for vectorSize in [150]:#[50, 100, 150, 200, 250, 300]:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "##w2vList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "peekOfCode": "##w2vList = list(w2vList)\n##for item in w2vList:\n####    print(item)\n##    wfile.write(item+\"\\n\")\n##wfile.close()\n################################################################################################################################################\nfor cbow_mean in [0]:#[1,0]:\n    for vectorSize in [150]:#[50, 100, 150, 200, 250, 300]:\n        for windowSize in [10]: #, 15, 20\n            for TOPN in [50]:#[100, 150, 200]:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.nGramFinder_v3_Oct2018",
        "documentation": {}
    },
    {
        "label": "hasNumbers",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "def hasNumbers(inputString):\n    return any(char.isdigit() for char in inputString)\ndef isNumber(inputString):\n    try:\n        float(inputString)\n        return True\n    except ValueError:\n        return False\ndef hasStopwords(inputString):\n    stops = loadStopWords()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "isNumber",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "def isNumber(inputString):\n    try:\n        float(inputString)\n        return True\n    except ValueError:\n        return False\ndef hasStopwords(inputString):\n    stops = loadStopWords()\n    tokens = inputString.split(\" \")\n    for item in stops:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "hasStopwords",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "def hasStopwords(inputString):\n    stops = loadStopWords()\n    tokens = inputString.split(\" \")\n    for item in stops:\n        if item in tokens:\n            return True\n    return False\ndef loadData(filepath, remove_stopwords=False):\n##    dataList = list()\n    dataList = [[] for x in range(11891)]",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "loadData",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "def loadData(filepath, remove_stopwords=False):\n##    dataList = list()\n    dataList = [[] for x in range(11891)]\n    rfile = open(filepath, \"r\")\n    for line in rfile:\n        line2 = line.lower().strip()\n        line2 = re.sub(\"[^a-zA-Z0-9_]\",\" \", line2)\n        innerList = line2.split()\n        if remove_stopwords:\n            stops = loadStopWords()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "loadStringData",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "def loadStringData(filepath):\n##    dataList = list()\n##    dataList = [[] for x in range(11891)]\n    dataString = \"\"\n    rfile = open(filepath, \"r\")\n    for line in rfile:\n        line2 = line.lower().strip()\n        line2 = re.sub(\"[^a-zA-Z0-9_]\",\" \", line2)\n        dataString += \" \"+ line2\n    rfile.close()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "loadStringData2",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "def loadStringData2(filepath):\n##    dataList = list()\n##    dataList = [[] for x in range(11891)]\n    dataString = \"\"\n    rfile = open(filepath, \"r\")\n    for line in rfile:\n        line = line.lower()\n        dataString += line\n    rfile.close()\n    return dataString",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "loadStopWords",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "def loadStopWords():\n    filePath = \"/Users/sarahmasudpreum/Desktop/PythonTCode/general/stopwords.txt\"\n    rfile = open(filePath, \"r\")\n    stopWords = list()\n    for line in rfile:\n        stopWords.append(line.strip())\n    rfile.close()\n    return stopWords\ndef loadOriginalConceptListPhrase(filepath):\n    conceptList = list()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "loadOriginalConceptListPhrase",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "def loadOriginalConceptListPhrase(filepath):\n    conceptList = list()\n    rfile = open(filepath, \"r\")\n    for line in rfile:\n        item = line.lower().strip()\n        if item not in conceptList:\n            conceptList.append(item)\n    rfile.close()\n    return conceptList\ndef loadExtendedConceptListPhrase(filepath):",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "loadExtendedConceptListPhrase",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "def loadExtendedConceptListPhrase(filepath):\n    conceptList = list()\n    rfile = open(filepath, \"r\")\n    for line in rfile:\n        tokens = line.lower().strip().split(\"\\t\")\n        for token in tokens:\n            if token not in conceptList:\n                conceptList.append(token)\n    rfile.close()\n    return conceptList",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "loadOriginalConceptList",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "def loadOriginalConceptList(filepath):\n    conceptList = list()\n    rfile = open(filepath, \"r\")\n    for line in rfile:\n        concept = line.split(\"\\t\")[0].lower().strip()\n        items = concept.split(\" \")\n        for item in items:\n            if item not in conceptList:\n                conceptList.append(item)\n    stops = loadStopWords()##set(stopwords.words(\"english\"))",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "loadSimpleListPhras",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "def loadSimpleListPhrase (filePath):\n    rfile = open(filePath, \"r\")\n    itemList = list()\n##    table = str.maketrans({key: None for key in string.punctuation})\n    for line in rfile:\n##        line = re.sub(\"[^a-zA-Z0-9_]\",\" \", line)\n        item = line.lower().strip()\n        if item not in itemList:\n            itemList.append(item)\n    rfile.close()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "computeFrequenc",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "def computeFrequency (gtConceptList, frequencyList, wordList,fileName=\"\"):\n    wordFrequencyList = list(repeat(0,len(gtConceptList)))\n    for item in wordList:\n        for i in range (0,len(gtConceptList)):\n            gTerm = str(gtConceptList[i]).strip()\n            item = item.strip()\n##            if len(gTerm.split(\" \")) > 1:\n            if len(gTerm) > 1 and item.find(gTerm) is not -1: #or gTerm.find(item) is not -1:\n                wordFrequencyList[i] = frequencyList[i]\n            if len(gTerm) is 1:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "computeF",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "def computeFP (filePath, conceptList, gtConceptList):\n    tpConceptList = list()\n    for item in conceptList:\n        for i in range (0,len(gtConceptList)):\n            gTerm = str(gtConceptList[i]).strip()\n            item = item.strip()\n            if len(gTerm) > 1 and item.find(gTerm) is not -1: #or gTerm.find(item) is not -1:\n                tpConceptList.append(item)\n            if len(gTerm) is 1:\n                tokens = item.split(\" \")",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "computePrecisio",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "def computePrecision (TPlist, FPList):\n    # print(TPlist)\n    # print(FPList)\n    t = sum(TPlist) + sum(FPList)\n    p = float(0)\n    if t>0:\n        p = sum(TPlist)/float( t )\n    return round( p,4 )\ndef computeRecall (TPlist, denom):\n    p = sum(TPlist)/float(denom)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "computeRecal",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "def computeRecall (TPlist, denom):\n    p = sum(TPlist)/float(denom)\n    return round( p,4 )\ndef score(seq1, seq2, model):\n        '''prepares inputs for scoring'''\n        seq1_word_list = word_tokenize(seq1.strip().lower())\n        seq2_word_list = word_tokenize(seq2.strip().lower())\n        return sim_score(seq1_word_list, seq2_word_list,model)\ndef sim_score(wordlist1, wordlist2,model):\n        '''calculates average maximum similarity between two phrase inputs'''",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "score",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "def score(seq1, seq2, model):\n        '''prepares inputs for scoring'''\n        seq1_word_list = word_tokenize(seq1.strip().lower())\n        seq2_word_list = word_tokenize(seq2.strip().lower())\n        return sim_score(seq1_word_list, seq2_word_list,model)\ndef sim_score(wordlist1, wordlist2,model):\n        '''calculates average maximum similarity between two phrase inputs'''\n        maxes = []\n        for word in wordlist1:\n                cur_max = 0",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "sim_score",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "def sim_score(wordlist1, wordlist2,model):\n        '''calculates average maximum similarity between two phrase inputs'''\n        maxes = []\n        for word in wordlist1:\n                cur_max = 0\n                for word2 in wordlist2:\n                        if word == word2: #case where words are identical\n                                sim = 1\n                                cur_max = sim\n                        elif word in model and word2 in model:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "sim_sum_score",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "def sim_sum_score(wordlist1, wordlist2,model):\n    '''calculates similarity between two phrase inputs'''\n    x1 = np.zeros(100)\n    x2 = np.zeros(100)\n    for word in wordlist1:\n        if word in model:\n            x1 = x1 + model[word]\n    if len(wordlist1) > 0:\n        x1 = x1/len(wordlist1)\n    for word in wordlist2:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "sim_avg_score",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "def sim_avg_score(wordlist1, wordlist2,model):\n        avgdist = []\n        for word in wordlist1:\n            cur_dist = 0\n            for word2 in wordlist2:\n                if word == word2:\n                    sim = 1\n                    cur_dist += sim\n                elif word in model and word2 in model:\n                    sim = model.similarity(word, word2) #calculate cosine similarity score for words",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "model_most_similar_phrases",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "def model_most_similar_phrases(model, inputSpace, seedConcept, topn):\n    candidateSet = set()\n    scoreList = list(repeat(0,len(inputSpace)))\n    for i in range(0,len(inputSpace)):\n        ip = inputSpace[i]\n        scoreList[i] = score(ip, seedConcept, model)\n    finalList = list(zip(inputSpace, scoreList))\n    finalList.sort(key=lambda x: x[1])\n    finalList = finalList[-topn:]\n    # print(finalList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "createFileName",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "def createFileName(modelName, n=25):\n    filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/PhraseResults/withUnigram/\" #withUnigram withoutUnigram\n    if \"RAA\" in modelName:\n        filePath1 = filePath + str(n)+ \"candidateConcepts_RAA.txt\"\n        filePath2 = filePath + str(n)+ \"candidateConceptsWDistance_RAA.txt\"\n    elif \"Forum\" in modelName:\n        filePath1 = filePath + str(n)+ \"candidateConcepts_Forum.txt\"\n        filePath2 = filePath + str(n)+ \"candidateConceptsWDistance_Forum.txt\"\n    elif \"I2B2\" in modelName:\n        filePath1 = filePath + str(n)+ \"candidateConcepts_I2B2.txt\"",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "datafileRootPath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "datafileRootPath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/wordEmbeddingModel/Data/\"\ndataFileName = \"MedEMSallDataIn1.txt\" #EMSallDataIn1 MedallDataIn1 MedEMSallDataIn1\n# filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/trainingData/DataLines.txt\"\nfilePath = datafileRootPath + dataFileName\ndataString = loadStringData2(filePath)\n# dataString = re.sub(\"[^a-zA-Z0-9]\",\" \", dataString)\n##        tokens = line.split(\" \")\n##        tokens = [w for w in tokens if w.find(\" \") is -1 and not w.isdigit() and not w in stops]\n##        line = \" \".join(tokens)\nsent_tokenize_list = sent_tokenize(dataString)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "dataFileName",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "dataFileName = \"MedEMSallDataIn1.txt\" #EMSallDataIn1 MedallDataIn1 MedEMSallDataIn1\n# filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/trainingData/DataLines.txt\"\nfilePath = datafileRootPath + dataFileName\ndataString = loadStringData2(filePath)\n# dataString = re.sub(\"[^a-zA-Z0-9]\",\" \", dataString)\n##        tokens = line.split(\" \")\n##        tokens = [w for w in tokens if w.find(\" \") is -1 and not w.isdigit() and not w in stops]\n##        line = \" \".join(tokens)\nsent_tokenize_list = sent_tokenize(dataString)\nprint(str(len(sent_tokenize_list)))",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "filePath = datafileRootPath + dataFileName\ndataString = loadStringData2(filePath)\n# dataString = re.sub(\"[^a-zA-Z0-9]\",\" \", dataString)\n##        tokens = line.split(\" \")\n##        tokens = [w for w in tokens if w.find(\" \") is -1 and not w.isdigit() and not w in stops]\n##        line = \" \".join(tokens)\nsent_tokenize_list = sent_tokenize(dataString)\nprint(str(len(sent_tokenize_list)))\n# tokens = nltk.wordpunct_tokenize(dataString) #word_tokenize\ntokens = word_tokenize(dataString) #word_tokenize",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "dataString",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "dataString = loadStringData2(filePath)\n# dataString = re.sub(\"[^a-zA-Z0-9]\",\" \", dataString)\n##        tokens = line.split(\" \")\n##        tokens = [w for w in tokens if w.find(\" \") is -1 and not w.isdigit() and not w in stops]\n##        line = \" \".join(tokens)\nsent_tokenize_list = sent_tokenize(dataString)\nprint(str(len(sent_tokenize_list)))\n# tokens = nltk.wordpunct_tokenize(dataString) #word_tokenize\ntokens = word_tokenize(dataString) #word_tokenize\nprint(str(len(tokens)))",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "sent_tokenize_list",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "sent_tokenize_list = sent_tokenize(dataString)\nprint(str(len(sent_tokenize_list)))\n# tokens = nltk.wordpunct_tokenize(dataString) #word_tokenize\ntokens = word_tokenize(dataString) #word_tokenize\nprint(str(len(tokens)))\ntokens = [w for w in tokens if not w.isdigit()]\nunigramList = list(set(tokens))\nstops = loadStopWords()\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/conceptList.txt\" # newConceptList_Sept2018 conceptList\nconceptList = loadOriginalConceptListPhrase(filePath)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "tokens = word_tokenize(dataString) #word_tokenize\nprint(str(len(tokens)))\ntokens = [w for w in tokens if not w.isdigit()]\nunigramList = list(set(tokens))\nstops = loadStopWords()\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/conceptList.txt\" # newConceptList_Sept2018 conceptList\nconceptList = loadOriginalConceptListPhrase(filePath)\nconceptList = [w for w in conceptList if w.find(\" \") is -1]\nfinder = BigramCollocationFinder.from_words(tokens)\nbigram_measures = nltk.collocations.BigramAssocMeasures()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "tokens = [w for w in tokens if not w.isdigit()]\nunigramList = list(set(tokens))\nstops = loadStopWords()\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/conceptList.txt\" # newConceptList_Sept2018 conceptList\nconceptList = loadOriginalConceptListPhrase(filePath)\nconceptList = [w for w in conceptList if w.find(\" \") is -1]\nfinder = BigramCollocationFinder.from_words(tokens)\nbigram_measures = nltk.collocations.BigramAssocMeasures()\n##sorted(finder.nbest(bigram_measures.raw_freq, 100))\nscored = finder.score_ngrams(bigram_measures.raw_freq)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "unigramList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "unigramList = list(set(tokens))\nstops = loadStopWords()\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/conceptList.txt\" # newConceptList_Sept2018 conceptList\nconceptList = loadOriginalConceptListPhrase(filePath)\nconceptList = [w for w in conceptList if w.find(\" \") is -1]\nfinder = BigramCollocationFinder.from_words(tokens)\nbigram_measures = nltk.collocations.BigramAssocMeasures()\n##sorted(finder.nbest(bigram_measures.raw_freq, 100))\nscored = finder.score_ngrams(bigram_measures.raw_freq)\nbigramSet = set(sorted(bigram for bigram, score in scored))",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "stops",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "stops = loadStopWords()\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/conceptList.txt\" # newConceptList_Sept2018 conceptList\nconceptList = loadOriginalConceptListPhrase(filePath)\nconceptList = [w for w in conceptList if w.find(\" \") is -1]\nfinder = BigramCollocationFinder.from_words(tokens)\nbigram_measures = nltk.collocations.BigramAssocMeasures()\n##sorted(finder.nbest(bigram_measures.raw_freq, 100))\nscored = finder.score_ngrams(bigram_measures.raw_freq)\nbigramSet = set(sorted(bigram for bigram, score in scored))\nbigramList = list(bigramSet)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/conceptList.txt\" # newConceptList_Sept2018 conceptList\nconceptList = loadOriginalConceptListPhrase(filePath)\nconceptList = [w for w in conceptList if w.find(\" \") is -1]\nfinder = BigramCollocationFinder.from_words(tokens)\nbigram_measures = nltk.collocations.BigramAssocMeasures()\n##sorted(finder.nbest(bigram_measures.raw_freq, 100))\nscored = finder.score_ngrams(bigram_measures.raw_freq)\nbigramSet = set(sorted(bigram for bigram, score in scored))\nbigramList = list(bigramSet)\nlen(bigramList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "conceptList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "conceptList = loadOriginalConceptListPhrase(filePath)\nconceptList = [w for w in conceptList if w.find(\" \") is -1]\nfinder = BigramCollocationFinder.from_words(tokens)\nbigram_measures = nltk.collocations.BigramAssocMeasures()\n##sorted(finder.nbest(bigram_measures.raw_freq, 100))\nscored = finder.score_ngrams(bigram_measures.raw_freq)\nbigramSet = set(sorted(bigram for bigram, score in scored))\nbigramList = list(bigramSet)\nlen(bigramList)\nfiltBigramList = list()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "conceptList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "conceptList = [w for w in conceptList if w.find(\" \") is -1]\nfinder = BigramCollocationFinder.from_words(tokens)\nbigram_measures = nltk.collocations.BigramAssocMeasures()\n##sorted(finder.nbest(bigram_measures.raw_freq, 100))\nscored = finder.score_ngrams(bigram_measures.raw_freq)\nbigramSet = set(sorted(bigram for bigram, score in scored))\nbigramList = list(bigramSet)\nlen(bigramList)\nfiltBigramList = list()\nfiltBigramList= [u+\" \"+v for u,v in bigramList if not u in stops and not v in stops and (u in conceptList or v in conceptList)]",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "finder",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "finder = BigramCollocationFinder.from_words(tokens)\nbigram_measures = nltk.collocations.BigramAssocMeasures()\n##sorted(finder.nbest(bigram_measures.raw_freq, 100))\nscored = finder.score_ngrams(bigram_measures.raw_freq)\nbigramSet = set(sorted(bigram for bigram, score in scored))\nbigramList = list(bigramSet)\nlen(bigramList)\nfiltBigramList = list()\nfiltBigramList= [u+\" \"+v for u,v in bigramList if not u in stops and not v in stops and (u in conceptList or v in conceptList)]\nlen(filtBigramList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "bigram_measures",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "bigram_measures = nltk.collocations.BigramAssocMeasures()\n##sorted(finder.nbest(bigram_measures.raw_freq, 100))\nscored = finder.score_ngrams(bigram_measures.raw_freq)\nbigramSet = set(sorted(bigram for bigram, score in scored))\nbigramList = list(bigramSet)\nlen(bigramList)\nfiltBigramList = list()\nfiltBigramList= [u+\" \"+v for u,v in bigramList if not u in stops and not v in stops and (u in conceptList or v in conceptList)]\nlen(filtBigramList)\nfiltBigramList= [w for w in filtBigramList if len(set('[~!@#$%^&*()_-+{}<>\":,.;?/|\\']+$').intersection(w)) == 0 ]",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "scored",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "scored = finder.score_ngrams(bigram_measures.raw_freq)\nbigramSet = set(sorted(bigram for bigram, score in scored))\nbigramList = list(bigramSet)\nlen(bigramList)\nfiltBigramList = list()\nfiltBigramList= [u+\" \"+v for u,v in bigramList if not u in stops and not v in stops and (u in conceptList or v in conceptList)]\nlen(filtBigramList)\nfiltBigramList= [w for w in filtBigramList if len(set('[~!@#$%^&*()_-+{}<>\":,.;?/|\\']+$').intersection(w)) == 0 ]\n# filtBigramList= [w for w in filtBigramList if w.find(\"patient\") is -1 and w.find(\" pt\") is -1]\nlen(filtBigramList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "bigramSet",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "bigramSet = set(sorted(bigram for bigram, score in scored))\nbigramList = list(bigramSet)\nlen(bigramList)\nfiltBigramList = list()\nfiltBigramList= [u+\" \"+v for u,v in bigramList if not u in stops and not v in stops and (u in conceptList or v in conceptList)]\nlen(filtBigramList)\nfiltBigramList= [w for w in filtBigramList if len(set('[~!@#$%^&*()_-+{}<>\":,.;?/|\\']+$').intersection(w)) == 0 ]\n# filtBigramList= [w for w in filtBigramList if w.find(\"patient\") is -1 and w.find(\" pt\") is -1]\nlen(filtBigramList)\nfiltBigramList = list(set(filtBigramList))",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "bigramList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "bigramList = list(bigramSet)\nlen(bigramList)\nfiltBigramList = list()\nfiltBigramList= [u+\" \"+v for u,v in bigramList if not u in stops and not v in stops and (u in conceptList or v in conceptList)]\nlen(filtBigramList)\nfiltBigramList= [w for w in filtBigramList if len(set('[~!@#$%^&*()_-+{}<>\":,.;?/|\\']+$').intersection(w)) == 0 ]\n# filtBigramList= [w for w in filtBigramList if w.find(\"patient\") is -1 and w.find(\" pt\") is -1]\nlen(filtBigramList)\nfiltBigramList = list(set(filtBigramList))\nfinder = TrigramCollocationFinder.from_words(tokens)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "filtBigramList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "filtBigramList = list()\nfiltBigramList= [u+\" \"+v for u,v in bigramList if not u in stops and not v in stops and (u in conceptList or v in conceptList)]\nlen(filtBigramList)\nfiltBigramList= [w for w in filtBigramList if len(set('[~!@#$%^&*()_-+{}<>\":,.;?/|\\']+$').intersection(w)) == 0 ]\n# filtBigramList= [w for w in filtBigramList if w.find(\"patient\") is -1 and w.find(\" pt\") is -1]\nlen(filtBigramList)\nfiltBigramList = list(set(filtBigramList))\nfinder = TrigramCollocationFinder.from_words(tokens)\ntrigram_measures = nltk.collocations.TrigramAssocMeasures()\nscored = finder.score_ngrams(trigram_measures.raw_freq)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "filtBigramList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "filtBigramList = list(set(filtBigramList))\nfinder = TrigramCollocationFinder.from_words(tokens)\ntrigram_measures = nltk.collocations.TrigramAssocMeasures()\nscored = finder.score_ngrams(trigram_measures.raw_freq)\ntrigramSet = set(sorted(trigram for trigram, score in scored))\ntrigramList = list(trigramSet)\nlen(trigramList)\nfiltTrigramList = list()\nfiltTrigramList= [u+\" \"+v+\" \"+w for u,v,w in trigramList if not u in stops and not v in stops and not w in stops and (u in conceptList or v in conceptList or w in conceptList)]\nlen(filtTrigramList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "finder",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "finder = TrigramCollocationFinder.from_words(tokens)\ntrigram_measures = nltk.collocations.TrigramAssocMeasures()\nscored = finder.score_ngrams(trigram_measures.raw_freq)\ntrigramSet = set(sorted(trigram for trigram, score in scored))\ntrigramList = list(trigramSet)\nlen(trigramList)\nfiltTrigramList = list()\nfiltTrigramList= [u+\" \"+v+\" \"+w for u,v,w in trigramList if not u in stops and not v in stops and not w in stops and (u in conceptList or v in conceptList or w in conceptList)]\nlen(filtTrigramList)\nfiltTrigramList= [w for w in filtTrigramList if len(set('[~!@#$%^&*()_-+{}<>\":,.;?/|\\']+$').intersection(w)) == 0 ]",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "trigram_measures",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "trigram_measures = nltk.collocations.TrigramAssocMeasures()\nscored = finder.score_ngrams(trigram_measures.raw_freq)\ntrigramSet = set(sorted(trigram for trigram, score in scored))\ntrigramList = list(trigramSet)\nlen(trigramList)\nfiltTrigramList = list()\nfiltTrigramList= [u+\" \"+v+\" \"+w for u,v,w in trigramList if not u in stops and not v in stops and not w in stops and (u in conceptList or v in conceptList or w in conceptList)]\nlen(filtTrigramList)\nfiltTrigramList= [w for w in filtTrigramList if len(set('[~!@#$%^&*()_-+{}<>\":,.;?/|\\']+$').intersection(w)) == 0 ]\n# filtTrigramList = [w for w in filtTrigramList if w.find(\"patient\") is -1 and w.find(\" pt\") is -1]",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "scored",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "scored = finder.score_ngrams(trigram_measures.raw_freq)\ntrigramSet = set(sorted(trigram for trigram, score in scored))\ntrigramList = list(trigramSet)\nlen(trigramList)\nfiltTrigramList = list()\nfiltTrigramList= [u+\" \"+v+\" \"+w for u,v,w in trigramList if not u in stops and not v in stops and not w in stops and (u in conceptList or v in conceptList or w in conceptList)]\nlen(filtTrigramList)\nfiltTrigramList= [w for w in filtTrigramList if len(set('[~!@#$%^&*()_-+{}<>\":,.;?/|\\']+$').intersection(w)) == 0 ]\n# filtTrigramList = [w for w in filtTrigramList if w.find(\"patient\") is -1 and w.find(\" pt\") is -1]\n##filtTrigramList= [u+\" \"+v+\" \"+w for u,v,w in trigramList if not u in stops and not w in stops and (u in conceptList or v in conceptList or w in conceptList)]",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "trigramSet",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "trigramSet = set(sorted(trigram for trigram, score in scored))\ntrigramList = list(trigramSet)\nlen(trigramList)\nfiltTrigramList = list()\nfiltTrigramList= [u+\" \"+v+\" \"+w for u,v,w in trigramList if not u in stops and not v in stops and not w in stops and (u in conceptList or v in conceptList or w in conceptList)]\nlen(filtTrigramList)\nfiltTrigramList= [w for w in filtTrigramList if len(set('[~!@#$%^&*()_-+{}<>\":,.;?/|\\']+$').intersection(w)) == 0 ]\n# filtTrigramList = [w for w in filtTrigramList if w.find(\"patient\") is -1 and w.find(\" pt\") is -1]\n##filtTrigramList= [u+\" \"+v+\" \"+w for u,v,w in trigramList if not u in stops and not w in stops and (u in conceptList or v in conceptList or w in conceptList)]\nfiltTrigramList = list(set(filtTrigramList))",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "trigramList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "trigramList = list(trigramSet)\nlen(trigramList)\nfiltTrigramList = list()\nfiltTrigramList= [u+\" \"+v+\" \"+w for u,v,w in trigramList if not u in stops and not v in stops and not w in stops and (u in conceptList or v in conceptList or w in conceptList)]\nlen(filtTrigramList)\nfiltTrigramList= [w for w in filtTrigramList if len(set('[~!@#$%^&*()_-+{}<>\":,.;?/|\\']+$').intersection(w)) == 0 ]\n# filtTrigramList = [w for w in filtTrigramList if w.find(\"patient\") is -1 and w.find(\" pt\") is -1]\n##filtTrigramList= [u+\" \"+v+\" \"+w for u,v,w in trigramList if not u in stops and not w in stops and (u in conceptList or v in conceptList or w in conceptList)]\nfiltTrigramList = list(set(filtTrigramList))\nlen(filtTrigramList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "filtTrigramList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "filtTrigramList = list()\nfiltTrigramList= [u+\" \"+v+\" \"+w for u,v,w in trigramList if not u in stops and not v in stops and not w in stops and (u in conceptList or v in conceptList or w in conceptList)]\nlen(filtTrigramList)\nfiltTrigramList= [w for w in filtTrigramList if len(set('[~!@#$%^&*()_-+{}<>\":,.;?/|\\']+$').intersection(w)) == 0 ]\n# filtTrigramList = [w for w in filtTrigramList if w.find(\"patient\") is -1 and w.find(\" pt\") is -1]\n##filtTrigramList= [u+\" \"+v+\" \"+w for u,v,w in trigramList if not u in stops and not w in stops and (u in conceptList or v in conceptList or w in conceptList)]\nfiltTrigramList = list(set(filtTrigramList))\nlen(filtTrigramList)\n##load GT annotation\n##load protocol concept list",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "filtTrigramList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "filtTrigramList = list(set(filtTrigramList))\nlen(filtTrigramList)\n##load GT annotation\n##load protocol concept list\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/conceptList.txt\" # newConceptList_Sept2018 conceptList\nconceptList = loadOriginalConceptListPhrase(filePath)\n# print(len(conceptList))\n##load metaMap extended concept list\n##count ground truth in testData\n# filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/trainingData/DataLines.txt\"",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/conceptList.txt\" # newConceptList_Sept2018 conceptList\nconceptList = loadOriginalConceptListPhrase(filePath)\n# print(len(conceptList))\n##load metaMap extended concept list\n##count ground truth in testData\n# filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/trainingData/DataLines.txt\"\n# filePath = datafileRootPath + dataFileName\n# sentenceList = loadData(filePath, True)\n# min_count = 1\n# vectorSize = 150",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "conceptList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "conceptList = loadOriginalConceptListPhrase(filePath)\n# print(len(conceptList))\n##load metaMap extended concept list\n##count ground truth in testData\n# filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/trainingData/DataLines.txt\"\n# filePath = datafileRootPath + dataFileName\n# sentenceList = loadData(filePath, True)\n# min_count = 1\n# vectorSize = 150\n# windowSize = 10",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "candidateList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "candidateList = list()\ninputSpace = set()\ninputSpace.update(filtTrigramList)\ninputSpace.update(filtBigramList)\n# inputSpace.update(unigramList)\ninputSpace = list(inputSpace)\n# model = gensim.models.KeyedVectors.load_word2vec_format('/Users/sarahmasudpreum/src/word2vecModels/GoogleNews-vectors-negative300.bin', binary=True)\n# word2vec_RAA_Data_150_10 word2vec_Forum_Data_150_10 word2vec_MIMIC3_Data_150_10 word2vec_I2B2_Data_150_10\n# word2vec_Med_Data_150_10 word2vec_EMS_Data_150_10 word2vec_MedEMS_Data_150_10\nmodel = gensim.models.Word2Vec.load('/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/wordEmbeddingModel/Model/word2vec_MedEMS_Data_150_10.model')",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "inputSpace",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "inputSpace = set()\ninputSpace.update(filtTrigramList)\ninputSpace.update(filtBigramList)\n# inputSpace.update(unigramList)\ninputSpace = list(inputSpace)\n# model = gensim.models.KeyedVectors.load_word2vec_format('/Users/sarahmasudpreum/src/word2vecModels/GoogleNews-vectors-negative300.bin', binary=True)\n# word2vec_RAA_Data_150_10 word2vec_Forum_Data_150_10 word2vec_MIMIC3_Data_150_10 word2vec_I2B2_Data_150_10\n# word2vec_Med_Data_150_10 word2vec_EMS_Data_150_10 word2vec_MedEMS_Data_150_10\nmodel = gensim.models.Word2Vec.load('/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/wordEmbeddingModel/Model/word2vec_MedEMS_Data_150_10.model')\nvl = model.wv.vocab ##model.vocab (for google) model.wv.vocab",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "inputSpace",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "inputSpace = list(inputSpace)\n# model = gensim.models.KeyedVectors.load_word2vec_format('/Users/sarahmasudpreum/src/word2vecModels/GoogleNews-vectors-negative300.bin', binary=True)\n# word2vec_RAA_Data_150_10 word2vec_Forum_Data_150_10 word2vec_MIMIC3_Data_150_10 word2vec_I2B2_Data_150_10\n# word2vec_Med_Data_150_10 word2vec_EMS_Data_150_10 word2vec_MedEMS_Data_150_10\nmodel = gensim.models.Word2Vec.load('/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/wordEmbeddingModel/Model/word2vec_MedEMS_Data_150_10.model')\nvl = model.wv.vocab ##model.vocab (for google) model.wv.vocab\nTOPN = 50\nf1, f2 = createFileName(\"all\", TOPN)\n## RAA Forum MIMIC3 I2B2 Med EMS all Google Glove\nwfile = open(f1, \"w\")",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "model = gensim.models.Word2Vec.load('/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/wordEmbeddingModel/Model/word2vec_MedEMS_Data_150_10.model')\nvl = model.wv.vocab ##model.vocab (for google) model.wv.vocab\nTOPN = 50\nf1, f2 = createFileName(\"all\", TOPN)\n## RAA Forum MIMIC3 I2B2 Med EMS all Google Glove\nwfile = open(f1, \"w\")\nwfile2 = open(f2, \"w\")\n# candidateList = list()\n##count ground truth in testData\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/FormattedTags/allInOne.txt\"",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "vl",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "vl = model.wv.vocab ##model.vocab (for google) model.wv.vocab\nTOPN = 50\nf1, f2 = createFileName(\"all\", TOPN)\n## RAA Forum MIMIC3 I2B2 Med EMS all Google Glove\nwfile = open(f1, \"w\")\nwfile2 = open(f2, \"w\")\n# candidateList = list()\n##count ground truth in testData\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/FormattedTags/allInOne.txt\"\ngtConceptList = loadSimpleListPhrase(filePath)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "TOPN",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "TOPN = 50\nf1, f2 = createFileName(\"all\", TOPN)\n## RAA Forum MIMIC3 I2B2 Med EMS all Google Glove\nwfile = open(f1, \"w\")\nwfile2 = open(f2, \"w\")\n# candidateList = list()\n##count ground truth in testData\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/FormattedTags/allInOne.txt\"\ngtConceptList = loadSimpleListPhrase(filePath)\nfrequencyList = list(repeat(0,len(gtConceptList)))",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "wfile",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "wfile = open(f1, \"w\")\nwfile2 = open(f2, \"w\")\n# candidateList = list()\n##count ground truth in testData\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/FormattedTags/allInOne.txt\"\ngtConceptList = loadSimpleListPhrase(filePath)\nfrequencyList = list(repeat(0,len(gtConceptList)))\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/version2/MetaMapOutputsConciseP.txt\" ## MetaMapOutputsConciseW MetaMapOutputs_FilteredST_ConciseW\nrfile1 = open(filePath,\"r\")\nmmWordList = list()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "wfile2",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "wfile2 = open(f2, \"w\")\n# candidateList = list()\n##count ground truth in testData\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/FormattedTags/allInOne.txt\"\ngtConceptList = loadSimpleListPhrase(filePath)\nfrequencyList = list(repeat(0,len(gtConceptList)))\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/version2/MetaMapOutputsConciseP.txt\" ## MetaMapOutputsConciseW MetaMapOutputs_FilteredST_ConciseW\nrfile1 = open(filePath,\"r\")\nmmWordList = list()\nfor line in rfile1:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/FormattedTags/allInOne.txt\"\ngtConceptList = loadSimpleListPhrase(filePath)\nfrequencyList = list(repeat(0,len(gtConceptList)))\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/version2/MetaMapOutputsConciseP.txt\" ## MetaMapOutputsConciseW MetaMapOutputs_FilteredST_ConciseW\nrfile1 = open(filePath,\"r\")\nmmWordList = list()\nfor line in rfile1:\n    phrase = line.strip()\n    if not phrase.isdigit():\n        if phrase not in mmWordList:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "gtConceptList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "gtConceptList = loadSimpleListPhrase(filePath)\nfrequencyList = list(repeat(0,len(gtConceptList)))\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/version2/MetaMapOutputsConciseP.txt\" ## MetaMapOutputsConciseW MetaMapOutputs_FilteredST_ConciseW\nrfile1 = open(filePath,\"r\")\nmmWordList = list()\nfor line in rfile1:\n    phrase = line.strip()\n    if not phrase.isdigit():\n        if phrase not in mmWordList:\n            mmWordList.append(phrase)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "frequencyList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "frequencyList = list(repeat(0,len(gtConceptList)))\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/version2/MetaMapOutputsConciseP.txt\" ## MetaMapOutputsConciseW MetaMapOutputs_FilteredST_ConciseW\nrfile1 = open(filePath,\"r\")\nmmWordList = list()\nfor line in rfile1:\n    phrase = line.strip()\n    if not phrase.isdigit():\n        if phrase not in mmWordList:\n            mmWordList.append(phrase)\nrfile1.close()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/version2/MetaMapOutputsConciseP.txt\" ## MetaMapOutputsConciseW MetaMapOutputs_FilteredST_ConciseW\nrfile1 = open(filePath,\"r\")\nmmWordList = list()\nfor line in rfile1:\n    phrase = line.strip()\n    if not phrase.isdigit():\n        if phrase not in mmWordList:\n            mmWordList.append(phrase)\nrfile1.close()\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData.txt\"",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "rfile1",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "rfile1 = open(filePath,\"r\")\nmmWordList = list()\nfor line in rfile1:\n    phrase = line.strip()\n    if not phrase.isdigit():\n        if phrase not in mmWordList:\n            mmWordList.append(phrase)\nrfile1.close()\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData.txt\"\nrfile = open(filePath, \"r\")",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "mmWordList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "mmWordList = list()\nfor line in rfile1:\n    phrase = line.strip()\n    if not phrase.isdigit():\n        if phrase not in mmWordList:\n            mmWordList.append(phrase)\nrfile1.close()\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData.txt\"\nrfile = open(filePath, \"r\")\nfor line in rfile:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData.txt\"\nrfile = open(filePath, \"r\")\nfor line in rfile:\n    for i in range (0,len(gtConceptList)):\n        line = line.lower().strip()\n        if len(gtConceptList[i]) > 1 and line.find(gtConceptList[i]) is not -1:\n            frequencyList[i] += 1\n            # print(gtConceptList[i] +\"\\t\\t\"+ line)\n        if len(gtConceptList[i]) is 1:\n            tokens = line.split(\" \")",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "rfile",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "rfile = open(filePath, \"r\")\nfor line in rfile:\n    for i in range (0,len(gtConceptList)):\n        line = line.lower().strip()\n        if len(gtConceptList[i]) > 1 and line.find(gtConceptList[i]) is not -1:\n            frequencyList[i] += 1\n            # print(gtConceptList[i] +\"\\t\\t\"+ line)\n        if len(gtConceptList[i]) is 1:\n            tokens = line.split(\" \")\n            for token in tokens:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "denom",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "denom = sum(frequencyList)\nglobalConceptList = set()\nglobalConceptList.update(conceptList)\nfor concept in conceptList:\n    candidateList = list()\n    if concept in vl:\n        candidateList, scoreTupleList = model_most_similar_phrases(model, inputSpace, concept, topn=TOPN)\n        candidateList = [w for w in candidateList if not hasNumbers(w)]\n        candidateList = [w for w in candidateList if not hasStopwords(w)]\n        # candidateList = [w for w in candidateList if w.find(\"patient\") is -1 or w.find(\" pt\") is -1 or w.find(\" t \") is -1 or w.find(\" s \") is -1]",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "globalConceptList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "globalConceptList = set()\nglobalConceptList.update(conceptList)\nfor concept in conceptList:\n    candidateList = list()\n    if concept in vl:\n        candidateList, scoreTupleList = model_most_similar_phrases(model, inputSpace, concept, topn=TOPN)\n        candidateList = [w for w in candidateList if not hasNumbers(w)]\n        candidateList = [w for w in candidateList if not hasStopwords(w)]\n        # candidateList = [w for w in candidateList if w.find(\"patient\") is -1 or w.find(\" pt\") is -1 or w.find(\" t \") is -1 or w.find(\" s \") is -1]\n        candidateList= [w for w in candidateList if len(set('[~!@#$%^&*()_-+{}<>\":,.;?/|\\']+$').intersection(w)) == 0 ]",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/PhraseResults/withUnigram/results.txt\" #withUnigram withoutUnigram\nresultWriteFile = open(filePath, \"a\")\n# resultWriteFile.write(\"\\tRecall\\tPrecision\\n\")\nresultWriteFile.write(\"EMSMed+withoutUnigram\\t\\t\\n\")\n# filePath =\"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/PhraseResults/withUnigram/25candidateConcepts_EMS.txt\"\n# # globalConceptList = list()\n# temp = loadExtendedConceptListPhrase(filePath)\n# globalConceptList = list(globalConceptList) + temp\nglobalConceptList = list(set(globalConceptList))\n# globalConceptList.append(conceptList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "resultWriteFile",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "resultWriteFile = open(filePath, \"a\")\n# resultWriteFile.write(\"\\tRecall\\tPrecision\\n\")\nresultWriteFile.write(\"EMSMed+withoutUnigram\\t\\t\\n\")\n# filePath =\"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/PhraseResults/withUnigram/25candidateConcepts_EMS.txt\"\n# # globalConceptList = list()\n# temp = loadExtendedConceptListPhrase(filePath)\n# globalConceptList = list(globalConceptList) + temp\nglobalConceptList = list(set(globalConceptList))\n# globalConceptList.append(conceptList)\nw2vFrequencyList = computeFrequency (gtConceptList, frequencyList, globalConceptList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "globalConceptList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "globalConceptList = list(set(globalConceptList))\n# globalConceptList.append(conceptList)\nw2vFrequencyList = computeFrequency (gtConceptList, frequencyList, globalConceptList)\ntestDatafilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData.txt\"\nw2vFPList = computeFP(testDatafilePath, globalConceptList, gtConceptList)\nw2vr = computeRecall(w2vFrequencyList, denom)\nw2vp = computePrecision(w2vFrequencyList, w2vFPList)\nresultWriteFile.write(\"withOutMetaMap\\t\"+str(w2vr)+\"\\t\"+str(w2vp)+\"\\n\")\nprint(str(w2vr)+\"\\t\"+str(w2vp)+\"\\n\")\nprint(str(len(globalConceptList)))",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "w2vFrequencyList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "w2vFrequencyList = computeFrequency (gtConceptList, frequencyList, globalConceptList)\ntestDatafilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData.txt\"\nw2vFPList = computeFP(testDatafilePath, globalConceptList, gtConceptList)\nw2vr = computeRecall(w2vFrequencyList, denom)\nw2vp = computePrecision(w2vFrequencyList, w2vFPList)\nresultWriteFile.write(\"withOutMetaMap\\t\"+str(w2vr)+\"\\t\"+str(w2vp)+\"\\n\")\nprint(str(w2vr)+\"\\t\"+str(w2vp)+\"\\n\")\nprint(str(len(globalConceptList)))\nprint(str(len(mmWordList)))\n# globalConceptList.append(list(mmWordList))",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "testDatafilePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "testDatafilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData.txt\"\nw2vFPList = computeFP(testDatafilePath, globalConceptList, gtConceptList)\nw2vr = computeRecall(w2vFrequencyList, denom)\nw2vp = computePrecision(w2vFrequencyList, w2vFPList)\nresultWriteFile.write(\"withOutMetaMap\\t\"+str(w2vr)+\"\\t\"+str(w2vp)+\"\\n\")\nprint(str(w2vr)+\"\\t\"+str(w2vp)+\"\\n\")\nprint(str(len(globalConceptList)))\nprint(str(len(mmWordList)))\n# globalConceptList.append(list(mmWordList))\nglobalConceptList = globalConceptList + mmWordList",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "w2vFPList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "w2vFPList = computeFP(testDatafilePath, globalConceptList, gtConceptList)\nw2vr = computeRecall(w2vFrequencyList, denom)\nw2vp = computePrecision(w2vFrequencyList, w2vFPList)\nresultWriteFile.write(\"withOutMetaMap\\t\"+str(w2vr)+\"\\t\"+str(w2vp)+\"\\n\")\nprint(str(w2vr)+\"\\t\"+str(w2vp)+\"\\n\")\nprint(str(len(globalConceptList)))\nprint(str(len(mmWordList)))\n# globalConceptList.append(list(mmWordList))\nglobalConceptList = globalConceptList + mmWordList\nprint(str(len(globalConceptList)))",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "w2vr",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "w2vr = computeRecall(w2vFrequencyList, denom)\nw2vp = computePrecision(w2vFrequencyList, w2vFPList)\nresultWriteFile.write(\"withOutMetaMap\\t\"+str(w2vr)+\"\\t\"+str(w2vp)+\"\\n\")\nprint(str(w2vr)+\"\\t\"+str(w2vp)+\"\\n\")\nprint(str(len(globalConceptList)))\nprint(str(len(mmWordList)))\n# globalConceptList.append(list(mmWordList))\nglobalConceptList = globalConceptList + mmWordList\nprint(str(len(globalConceptList)))\nglobalConceptList2 = list(set(globalConceptList))",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "w2vp",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "w2vp = computePrecision(w2vFrequencyList, w2vFPList)\nresultWriteFile.write(\"withOutMetaMap\\t\"+str(w2vr)+\"\\t\"+str(w2vp)+\"\\n\")\nprint(str(w2vr)+\"\\t\"+str(w2vp)+\"\\n\")\nprint(str(len(globalConceptList)))\nprint(str(len(mmWordList)))\n# globalConceptList.append(list(mmWordList))\nglobalConceptList = globalConceptList + mmWordList\nprint(str(len(globalConceptList)))\nglobalConceptList2 = list(set(globalConceptList))\nw2vFrequencyList = computeFrequency (gtConceptList, frequencyList, globalConceptList2)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "globalConceptList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "globalConceptList = globalConceptList + mmWordList\nprint(str(len(globalConceptList)))\nglobalConceptList2 = list(set(globalConceptList))\nw2vFrequencyList = computeFrequency (gtConceptList, frequencyList, globalConceptList2)\ntestDatafilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData.txt\"\nw2vFPList = computeFP(testDatafilePath, globalConceptList, gtConceptList)\nw2vr = computeRecall(w2vFrequencyList, denom)\nw2vp = computePrecision(w2vFrequencyList, w2vFPList)\nresultWriteFile.write(\"withMetaMap\\t\"+str(w2vr)+\"\\t\"+str(w2vp)+\"\\n\")\nprint(str(w2vr)+\"\\t\"+str(w2vp)+\"\\n\")",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "globalConceptList2",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "globalConceptList2 = list(set(globalConceptList))\nw2vFrequencyList = computeFrequency (gtConceptList, frequencyList, globalConceptList2)\ntestDatafilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData.txt\"\nw2vFPList = computeFP(testDatafilePath, globalConceptList, gtConceptList)\nw2vr = computeRecall(w2vFrequencyList, denom)\nw2vp = computePrecision(w2vFrequencyList, w2vFPList)\nresultWriteFile.write(\"withMetaMap\\t\"+str(w2vr)+\"\\t\"+str(w2vp)+\"\\n\")\nprint(str(w2vr)+\"\\t\"+str(w2vp)+\"\\n\")\nresultWriteFile.close()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "w2vFrequencyList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "w2vFrequencyList = computeFrequency (gtConceptList, frequencyList, globalConceptList2)\ntestDatafilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData.txt\"\nw2vFPList = computeFP(testDatafilePath, globalConceptList, gtConceptList)\nw2vr = computeRecall(w2vFrequencyList, denom)\nw2vp = computePrecision(w2vFrequencyList, w2vFPList)\nresultWriteFile.write(\"withMetaMap\\t\"+str(w2vr)+\"\\t\"+str(w2vp)+\"\\n\")\nprint(str(w2vr)+\"\\t\"+str(w2vp)+\"\\n\")\nresultWriteFile.close()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "testDatafilePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "testDatafilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData.txt\"\nw2vFPList = computeFP(testDatafilePath, globalConceptList, gtConceptList)\nw2vr = computeRecall(w2vFrequencyList, denom)\nw2vp = computePrecision(w2vFrequencyList, w2vFPList)\nresultWriteFile.write(\"withMetaMap\\t\"+str(w2vr)+\"\\t\"+str(w2vp)+\"\\n\")\nprint(str(w2vr)+\"\\t\"+str(w2vp)+\"\\n\")\nresultWriteFile.close()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "w2vFPList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "w2vFPList = computeFP(testDatafilePath, globalConceptList, gtConceptList)\nw2vr = computeRecall(w2vFrequencyList, denom)\nw2vp = computePrecision(w2vFrequencyList, w2vFPList)\nresultWriteFile.write(\"withMetaMap\\t\"+str(w2vr)+\"\\t\"+str(w2vp)+\"\\n\")\nprint(str(w2vr)+\"\\t\"+str(w2vp)+\"\\n\")\nresultWriteFile.close()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "w2vr",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "w2vr = computeRecall(w2vFrequencyList, denom)\nw2vp = computePrecision(w2vFrequencyList, w2vFPList)\nresultWriteFile.write(\"withMetaMap\\t\"+str(w2vr)+\"\\t\"+str(w2vp)+\"\\n\")\nprint(str(w2vr)+\"\\t\"+str(w2vp)+\"\\n\")\nresultWriteFile.close()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "w2vp",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "peekOfCode": "w2vp = computePrecision(w2vFrequencyList, w2vFPList)\nresultWriteFile.write(\"withMetaMap\\t\"+str(w2vr)+\"\\t\"+str(w2vp)+\"\\n\")\nprint(str(w2vr)+\"\\t\"+str(w2vp)+\"\\n\")\nresultWriteFile.close()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performanceWordEmbedding_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "isNumber",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "peekOfCode": "def isNumber(inputString):\n    try:\n        float(inputString)\n        return True\n    except ValueError:\n        return False\ndef loadMMConceptList(filePath):\n    conceptList = list()\n    rfile = open(filePath, \"r\")    \n    for line in rfile:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "documentation": {}
    },
    {
        "label": "loadMMConceptList",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "peekOfCode": "def loadMMConceptList(filePath):\n    conceptList = list()\n    rfile = open(filePath, \"r\")    \n    for line in rfile:\n        line = re.sub(\"[^a-zA-Z0-9_]\",\" \", line)\n        items = line.lower().strip().split(\" \")\n        for item in items:\n            item = str(item)\n            if not item.isdigit():\n                if item not in conceptList:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "documentation": {}
    },
    {
        "label": "loadSimpleLis",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "peekOfCode": "def loadSimpleList (filePath, splitFlag = False):\n    rfile = open(filePath, \"r\")\n    itemList = list()\n    table = str.maketrans({key: None for key in string.punctuation})\n    for line in rfile:\n        item = line.lower().strip()\n        if splitFlag:\n            items = item.split(\" \")\n            stops = set(stopwords.words(\"english\"))\n            items = [w for w in items if not w in stops]",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "documentation": {}
    },
    {
        "label": "computeFrequenc",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "peekOfCode": "def computeFrequency (gtConceptList, frequencyList, wordList):\n    wordFrequencyList = list(repeat(0,len(gtConceptList)))\n    for item in wordList:\n        for i in range (0,len(gtConceptList)):\n            wordTokens = item.lower().strip().split(\" \")\n            for word in wordTokens:\n                if word.find(gtConceptList[i]) is not -1 and gtConceptList[i].find(word) is not -1:\n                    wordFrequencyList[i] = frequencyList[i]\n    return wordFrequencyList\ndef computeFP (filePath, conceptList, gtConceptList):",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "documentation": {}
    },
    {
        "label": "computeF",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "peekOfCode": "def computeFP (filePath, conceptList, gtConceptList):\n    filteredConceptList = [x for x in conceptList if x not in gtConceptList]\n    stops = set(stopwords.words(\"english\"))\n    filteredConceptList = [w for w in filteredConceptList if not w in stops]\n    rfile = open(filePath, \"r\")\n    frequencyList = list(repeat(0,len(filteredConceptList)))\n    for line in rfile:\n        line = re.sub(\"[^a-zA-Z0-9_]\",\" \", line)\n        for i in range (0,len(filteredConceptList)):\n            concept = str(filteredConceptList[i])",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "documentation": {}
    },
    {
        "label": "computePrecisio",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "peekOfCode": "def computePrecision (TPlist, FPList):\n    p = sum(TPlist)/float( sum(TPlist) + sum(FPList))\n    return round( p,4 )\ndef computeRecall (TPlist, denom):\n    p = sum(TPlist)/float(denom)\n    return round( p,4 )\n#################################################################################\n##load groundTruth\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/FormattedTags/allInOne.txt\"\ngtConceptList = loadSimpleList(filePath, True)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "documentation": {}
    },
    {
        "label": "computeRecal",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "peekOfCode": "def computeRecall (TPlist, denom):\n    p = sum(TPlist)/float(denom)\n    return round( p,4 )\n#################################################################################\n##load groundTruth\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/FormattedTags/allInOne.txt\"\ngtConceptList = loadSimpleList(filePath, True)\nfrequencyList = list(repeat(0,len(gtConceptList)))\n##count ground truth in testData\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData.txt\"",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "peekOfCode": "filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/FormattedTags/allInOne.txt\"\ngtConceptList = loadSimpleList(filePath, True)\nfrequencyList = list(repeat(0,len(gtConceptList)))\n##count ground truth in testData\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData.txt\"\nrfile = open(filePath, \"r\")\nfor line in rfile:\n    for i in range (0,len(gtConceptList)):\n##        if len(gtConceptList[i]) > 2:\n##            if line.find(gtConceptList[i]) is not -1:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "documentation": {}
    },
    {
        "label": "gtConceptList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "peekOfCode": "gtConceptList = loadSimpleList(filePath, True)\nfrequencyList = list(repeat(0,len(gtConceptList)))\n##count ground truth in testData\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData.txt\"\nrfile = open(filePath, \"r\")\nfor line in rfile:\n    for i in range (0,len(gtConceptList)):\n##        if len(gtConceptList[i]) > 2:\n##            if line.find(gtConceptList[i]) is not -1:\n##                frequencyList[i] += 1",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "documentation": {}
    },
    {
        "label": "frequencyList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "peekOfCode": "frequencyList = list(repeat(0,len(gtConceptList)))\n##count ground truth in testData\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData.txt\"\nrfile = open(filePath, \"r\")\nfor line in rfile:\n    for i in range (0,len(gtConceptList)):\n##        if len(gtConceptList[i]) > 2:\n##            if line.find(gtConceptList[i]) is not -1:\n##                frequencyList[i] += 1\n##        else:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "peekOfCode": "filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData.txt\"\nrfile = open(filePath, \"r\")\nfor line in rfile:\n    for i in range (0,len(gtConceptList)):\n##        if len(gtConceptList[i]) > 2:\n##            if line.find(gtConceptList[i]) is not -1:\n##                frequencyList[i] += 1\n##        else:\n        wordTokens = line.lower().strip().split(\" \")\n        for word in wordTokens:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "documentation": {}
    },
    {
        "label": "rfile",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "peekOfCode": "rfile = open(filePath, \"r\")\nfor line in rfile:\n    for i in range (0,len(gtConceptList)):\n##        if len(gtConceptList[i]) > 2:\n##            if line.find(gtConceptList[i]) is not -1:\n##                frequencyList[i] += 1\n##        else:\n        wordTokens = line.lower().strip().split(\" \")\n        for word in wordTokens:\n            if word.find(gtConceptList[i]) is not -1 and gtConceptList[i].find(word) is not -1:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "peekOfCode": "filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/version2/MetaMapOutputsConciseW.txt\"\nmmList = loadMMConceptList(filePath)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/version2/MetaMapOutputs_FilteredST_ConciseW.txt\"\nmm_FiltST_List = loadMMConceptList(filePath)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/MetaMapExtendedConceptList_JavaAPI.txt\"\nmmExtConceptList = loadMMConceptList(filePath)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/MetaMapExtendedConceptList_FilteredST_JavaAPI.txt\"\nmm_FiltST_ExtConceptList = loadMMConceptList(filePath)\n##construct TP list\nmmFrequencyList = computeFrequency (gtConceptList, frequencyList, mmList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "documentation": {}
    },
    {
        "label": "mmList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "peekOfCode": "mmList = loadMMConceptList(filePath)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/version2/MetaMapOutputs_FilteredST_ConciseW.txt\"\nmm_FiltST_List = loadMMConceptList(filePath)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/MetaMapExtendedConceptList_JavaAPI.txt\"\nmmExtConceptList = loadMMConceptList(filePath)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/MetaMapExtendedConceptList_FilteredST_JavaAPI.txt\"\nmm_FiltST_ExtConceptList = loadMMConceptList(filePath)\n##construct TP list\nmmFrequencyList = computeFrequency (gtConceptList, frequencyList, mmList)\nmm_FiltST_FrequencyList = computeFrequency (gtConceptList, frequencyList, mm_FiltST_List)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "peekOfCode": "filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/version2/MetaMapOutputs_FilteredST_ConciseW.txt\"\nmm_FiltST_List = loadMMConceptList(filePath)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/MetaMapExtendedConceptList_JavaAPI.txt\"\nmmExtConceptList = loadMMConceptList(filePath)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/MetaMapExtendedConceptList_FilteredST_JavaAPI.txt\"\nmm_FiltST_ExtConceptList = loadMMConceptList(filePath)\n##construct TP list\nmmFrequencyList = computeFrequency (gtConceptList, frequencyList, mmList)\nmm_FiltST_FrequencyList = computeFrequency (gtConceptList, frequencyList, mm_FiltST_List)\nmmExtConceptFrequencyList = computeFrequency (gtConceptList, frequencyList, mmExtConceptList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "documentation": {}
    },
    {
        "label": "mm_FiltST_List",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "peekOfCode": "mm_FiltST_List = loadMMConceptList(filePath)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/MetaMapExtendedConceptList_JavaAPI.txt\"\nmmExtConceptList = loadMMConceptList(filePath)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/MetaMapExtendedConceptList_FilteredST_JavaAPI.txt\"\nmm_FiltST_ExtConceptList = loadMMConceptList(filePath)\n##construct TP list\nmmFrequencyList = computeFrequency (gtConceptList, frequencyList, mmList)\nmm_FiltST_FrequencyList = computeFrequency (gtConceptList, frequencyList, mm_FiltST_List)\nmmExtConceptFrequencyList = computeFrequency (gtConceptList, frequencyList, mmExtConceptList)\nmm_FiltST_ExtConceptFrequencyList = computeFrequency (gtConceptList, frequencyList, mm_FiltST_ExtConceptList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "peekOfCode": "filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/MetaMapExtendedConceptList_JavaAPI.txt\"\nmmExtConceptList = loadMMConceptList(filePath)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/MetaMapExtendedConceptList_FilteredST_JavaAPI.txt\"\nmm_FiltST_ExtConceptList = loadMMConceptList(filePath)\n##construct TP list\nmmFrequencyList = computeFrequency (gtConceptList, frequencyList, mmList)\nmm_FiltST_FrequencyList = computeFrequency (gtConceptList, frequencyList, mm_FiltST_List)\nmmExtConceptFrequencyList = computeFrequency (gtConceptList, frequencyList, mmExtConceptList)\nmm_FiltST_ExtConceptFrequencyList = computeFrequency (gtConceptList, frequencyList, mm_FiltST_ExtConceptList)\ndenom = sum(frequencyList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "documentation": {}
    },
    {
        "label": "mmExtConceptList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "peekOfCode": "mmExtConceptList = loadMMConceptList(filePath)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/MetaMapExtendedConceptList_FilteredST_JavaAPI.txt\"\nmm_FiltST_ExtConceptList = loadMMConceptList(filePath)\n##construct TP list\nmmFrequencyList = computeFrequency (gtConceptList, frequencyList, mmList)\nmm_FiltST_FrequencyList = computeFrequency (gtConceptList, frequencyList, mm_FiltST_List)\nmmExtConceptFrequencyList = computeFrequency (gtConceptList, frequencyList, mmExtConceptList)\nmm_FiltST_ExtConceptFrequencyList = computeFrequency (gtConceptList, frequencyList, mm_FiltST_ExtConceptList)\ndenom = sum(frequencyList)\nfinalList = list(zip(gtConceptList, frequencyList, mmFrequencyList, mm_FiltST_FrequencyList, mmExtConceptFrequencyList, mm_FiltST_ExtConceptFrequencyList))",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "peekOfCode": "filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/MetaMapExtendedConceptList_FilteredST_JavaAPI.txt\"\nmm_FiltST_ExtConceptList = loadMMConceptList(filePath)\n##construct TP list\nmmFrequencyList = computeFrequency (gtConceptList, frequencyList, mmList)\nmm_FiltST_FrequencyList = computeFrequency (gtConceptList, frequencyList, mm_FiltST_List)\nmmExtConceptFrequencyList = computeFrequency (gtConceptList, frequencyList, mmExtConceptList)\nmm_FiltST_ExtConceptFrequencyList = computeFrequency (gtConceptList, frequencyList, mm_FiltST_ExtConceptList)\ndenom = sum(frequencyList)\nfinalList = list(zip(gtConceptList, frequencyList, mmFrequencyList, mm_FiltST_FrequencyList, mmExtConceptFrequencyList, mm_FiltST_ExtConceptFrequencyList))\nfor item in finalList:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "documentation": {}
    },
    {
        "label": "mm_FiltST_ExtConceptList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "peekOfCode": "mm_FiltST_ExtConceptList = loadMMConceptList(filePath)\n##construct TP list\nmmFrequencyList = computeFrequency (gtConceptList, frequencyList, mmList)\nmm_FiltST_FrequencyList = computeFrequency (gtConceptList, frequencyList, mm_FiltST_List)\nmmExtConceptFrequencyList = computeFrequency (gtConceptList, frequencyList, mmExtConceptList)\nmm_FiltST_ExtConceptFrequencyList = computeFrequency (gtConceptList, frequencyList, mm_FiltST_ExtConceptList)\ndenom = sum(frequencyList)\nfinalList = list(zip(gtConceptList, frequencyList, mmFrequencyList, mm_FiltST_FrequencyList, mmExtConceptFrequencyList, mm_FiltST_ExtConceptFrequencyList))\nfor item in finalList:\n    if item[1] > 0:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "documentation": {}
    },
    {
        "label": "mmFrequencyList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "peekOfCode": "mmFrequencyList = computeFrequency (gtConceptList, frequencyList, mmList)\nmm_FiltST_FrequencyList = computeFrequency (gtConceptList, frequencyList, mm_FiltST_List)\nmmExtConceptFrequencyList = computeFrequency (gtConceptList, frequencyList, mmExtConceptList)\nmm_FiltST_ExtConceptFrequencyList = computeFrequency (gtConceptList, frequencyList, mm_FiltST_ExtConceptList)\ndenom = sum(frequencyList)\nfinalList = list(zip(gtConceptList, frequencyList, mmFrequencyList, mm_FiltST_FrequencyList, mmExtConceptFrequencyList, mm_FiltST_ExtConceptFrequencyList))\nfor item in finalList:\n    if item[1] > 0:\n        print(str(item[0]) + \" \" + str(item[1]) + \" \" + str(item[2])+\" \" +str(item[3]) + \" \" + str(item[4]) + \" \" + str(item[5]))\n##finalList = list(zip(gtConceptList, frequencyList, w2vFrequencyList, d2vFrequencyList, i2b2FrequencyList, googleFrequencyList, mmFrequencyList))",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "documentation": {}
    },
    {
        "label": "mm_FiltST_FrequencyList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "peekOfCode": "mm_FiltST_FrequencyList = computeFrequency (gtConceptList, frequencyList, mm_FiltST_List)\nmmExtConceptFrequencyList = computeFrequency (gtConceptList, frequencyList, mmExtConceptList)\nmm_FiltST_ExtConceptFrequencyList = computeFrequency (gtConceptList, frequencyList, mm_FiltST_ExtConceptList)\ndenom = sum(frequencyList)\nfinalList = list(zip(gtConceptList, frequencyList, mmFrequencyList, mm_FiltST_FrequencyList, mmExtConceptFrequencyList, mm_FiltST_ExtConceptFrequencyList))\nfor item in finalList:\n    if item[1] > 0:\n        print(str(item[0]) + \" \" + str(item[1]) + \" \" + str(item[2])+\" \" +str(item[3]) + \" \" + str(item[4]) + \" \" + str(item[5]))\n##finalList = list(zip(gtConceptList, frequencyList, w2vFrequencyList, d2vFrequencyList, i2b2FrequencyList, googleFrequencyList, mmFrequencyList))\n##for item in finalList:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "documentation": {}
    },
    {
        "label": "mmExtConceptFrequencyList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "peekOfCode": "mmExtConceptFrequencyList = computeFrequency (gtConceptList, frequencyList, mmExtConceptList)\nmm_FiltST_ExtConceptFrequencyList = computeFrequency (gtConceptList, frequencyList, mm_FiltST_ExtConceptList)\ndenom = sum(frequencyList)\nfinalList = list(zip(gtConceptList, frequencyList, mmFrequencyList, mm_FiltST_FrequencyList, mmExtConceptFrequencyList, mm_FiltST_ExtConceptFrequencyList))\nfor item in finalList:\n    if item[1] > 0:\n        print(str(item[0]) + \" \" + str(item[1]) + \" \" + str(item[2])+\" \" +str(item[3]) + \" \" + str(item[4]) + \" \" + str(item[5]))\n##finalList = list(zip(gtConceptList, frequencyList, w2vFrequencyList, d2vFrequencyList, i2b2FrequencyList, googleFrequencyList, mmFrequencyList))\n##for item in finalList:\n##    if item[1] > 0:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "documentation": {}
    },
    {
        "label": "mm_FiltST_ExtConceptFrequencyList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "peekOfCode": "mm_FiltST_ExtConceptFrequencyList = computeFrequency (gtConceptList, frequencyList, mm_FiltST_ExtConceptList)\ndenom = sum(frequencyList)\nfinalList = list(zip(gtConceptList, frequencyList, mmFrequencyList, mm_FiltST_FrequencyList, mmExtConceptFrequencyList, mm_FiltST_ExtConceptFrequencyList))\nfor item in finalList:\n    if item[1] > 0:\n        print(str(item[0]) + \" \" + str(item[1]) + \" \" + str(item[2])+\" \" +str(item[3]) + \" \" + str(item[4]) + \" \" + str(item[5]))\n##finalList = list(zip(gtConceptList, frequencyList, w2vFrequencyList, d2vFrequencyList, i2b2FrequencyList, googleFrequencyList, mmFrequencyList))\n##for item in finalList:\n##    if item[1] > 0:\n##        print(str(item[0]) + \" \" + str(item[2]) + \" \" + str(item[3]) + \" \" + str(item[4]) + \" \" + str(item[5]) + \" \" + str(item[6]))",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "documentation": {}
    },
    {
        "label": "denom",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "peekOfCode": "denom = sum(frequencyList)\nfinalList = list(zip(gtConceptList, frequencyList, mmFrequencyList, mm_FiltST_FrequencyList, mmExtConceptFrequencyList, mm_FiltST_ExtConceptFrequencyList))\nfor item in finalList:\n    if item[1] > 0:\n        print(str(item[0]) + \" \" + str(item[1]) + \" \" + str(item[2])+\" \" +str(item[3]) + \" \" + str(item[4]) + \" \" + str(item[5]))\n##finalList = list(zip(gtConceptList, frequencyList, w2vFrequencyList, d2vFrequencyList, i2b2FrequencyList, googleFrequencyList, mmFrequencyList))\n##for item in finalList:\n##    if item[1] > 0:\n##        print(str(item[0]) + \" \" + str(item[2]) + \" \" + str(item[3]) + \" \" + str(item[4]) + \" \" + str(item[5]) + \" \" + str(item[6]))\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData.txt\"",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "documentation": {}
    },
    {
        "label": "finalList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "peekOfCode": "finalList = list(zip(gtConceptList, frequencyList, mmFrequencyList, mm_FiltST_FrequencyList, mmExtConceptFrequencyList, mm_FiltST_ExtConceptFrequencyList))\nfor item in finalList:\n    if item[1] > 0:\n        print(str(item[0]) + \" \" + str(item[1]) + \" \" + str(item[2])+\" \" +str(item[3]) + \" \" + str(item[4]) + \" \" + str(item[5]))\n##finalList = list(zip(gtConceptList, frequencyList, w2vFrequencyList, d2vFrequencyList, i2b2FrequencyList, googleFrequencyList, mmFrequencyList))\n##for item in finalList:\n##    if item[1] > 0:\n##        print(str(item[0]) + \" \" + str(item[2]) + \" \" + str(item[3]) + \" \" + str(item[4]) + \" \" + str(item[5]) + \" \" + str(item[6]))\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData.txt\"\nmmFPList = computeFP(filePath, mmList, gtConceptList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "documentation": {}
    },
    {
        "label": "##finalList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "peekOfCode": "##finalList = list(zip(gtConceptList, frequencyList, w2vFrequencyList, d2vFrequencyList, i2b2FrequencyList, googleFrequencyList, mmFrequencyList))\n##for item in finalList:\n##    if item[1] > 0:\n##        print(str(item[0]) + \" \" + str(item[2]) + \" \" + str(item[3]) + \" \" + str(item[4]) + \" \" + str(item[5]) + \" \" + str(item[6]))\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData.txt\"\nmmFPList = computeFP(filePath, mmList, gtConceptList)\nmm_FiltST_FPList = computeFP(filePath, mm_FiltST_List, gtConceptList)\nmmExtConceptFPList = computeFP(filePath, mmExtConceptList, gtConceptList)\nmm_FiltST_ExtConceptFPList = computeFP(filePath, mm_FiltST_ExtConceptList, gtConceptList)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/version2/MM_Results.txt\"",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "peekOfCode": "filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData.txt\"\nmmFPList = computeFP(filePath, mmList, gtConceptList)\nmm_FiltST_FPList = computeFP(filePath, mm_FiltST_List, gtConceptList)\nmmExtConceptFPList = computeFP(filePath, mmExtConceptList, gtConceptList)\nmm_FiltST_ExtConceptFPList = computeFP(filePath, mm_FiltST_ExtConceptList, gtConceptList)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/version2/MM_Results.txt\"\nwfile = open (filePath, \"w\")\n##wfile.write(str(sum(w2vFrequencyList))+ \"\\t\" + str(sum(d2vFrequencyList))+ \"\\t\" + str(sum(i2b2FrequencyList))+ \"\\t\" + str(sum(googleFrequencyList))+ \"\\t\" +  str(sum(mmFrequencyList)) +\"\\n\")\n##wfile.write(str(sum(w2vFPList))+ \"\\t\" + str(sum(d2vFPList))+ \"\\t\" + str(sum(i2b2FPList))+ \"\\t\" + str(sum(googleFPList))+ \"\\t\" +  str(sum(mmFPList)) +\"\\n\")\nwfile.write(\"ModelName\\t MMoutput\\tMMoutputFiltST\\tMMList\\tMeMListFiltST\\n\")",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "documentation": {}
    },
    {
        "label": "mmFPList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "peekOfCode": "mmFPList = computeFP(filePath, mmList, gtConceptList)\nmm_FiltST_FPList = computeFP(filePath, mm_FiltST_List, gtConceptList)\nmmExtConceptFPList = computeFP(filePath, mmExtConceptList, gtConceptList)\nmm_FiltST_ExtConceptFPList = computeFP(filePath, mm_FiltST_ExtConceptList, gtConceptList)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/version2/MM_Results.txt\"\nwfile = open (filePath, \"w\")\n##wfile.write(str(sum(w2vFrequencyList))+ \"\\t\" + str(sum(d2vFrequencyList))+ \"\\t\" + str(sum(i2b2FrequencyList))+ \"\\t\" + str(sum(googleFrequencyList))+ \"\\t\" +  str(sum(mmFrequencyList)) +\"\\n\")\n##wfile.write(str(sum(w2vFPList))+ \"\\t\" + str(sum(d2vFPList))+ \"\\t\" + str(sum(i2b2FPList))+ \"\\t\" + str(sum(googleFPList))+ \"\\t\" +  str(sum(mmFPList)) +\"\\n\")\nwfile.write(\"ModelName\\t MMoutput\\tMMoutputFiltST\\tMMList\\tMeMListFiltST\\n\")\nwfile.write(\"Recall\\t\"+str(round(sum(mmFrequencyList)/float(denom),4))+ \"\\t\" + str(round(sum(mm_FiltST_FrequencyList)/float(denom),4))+ \"\\t\" +str(round(sum(mmExtConceptFrequencyList)/float(denom),4))+ \"\\t\" + str(round(sum(mm_FiltST_ExtConceptFrequencyList)/float(denom),4))+\"\\n\")",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "documentation": {}
    },
    {
        "label": "mm_FiltST_FPList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "peekOfCode": "mm_FiltST_FPList = computeFP(filePath, mm_FiltST_List, gtConceptList)\nmmExtConceptFPList = computeFP(filePath, mmExtConceptList, gtConceptList)\nmm_FiltST_ExtConceptFPList = computeFP(filePath, mm_FiltST_ExtConceptList, gtConceptList)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/version2/MM_Results.txt\"\nwfile = open (filePath, \"w\")\n##wfile.write(str(sum(w2vFrequencyList))+ \"\\t\" + str(sum(d2vFrequencyList))+ \"\\t\" + str(sum(i2b2FrequencyList))+ \"\\t\" + str(sum(googleFrequencyList))+ \"\\t\" +  str(sum(mmFrequencyList)) +\"\\n\")\n##wfile.write(str(sum(w2vFPList))+ \"\\t\" + str(sum(d2vFPList))+ \"\\t\" + str(sum(i2b2FPList))+ \"\\t\" + str(sum(googleFPList))+ \"\\t\" +  str(sum(mmFPList)) +\"\\n\")\nwfile.write(\"ModelName\\t MMoutput\\tMMoutputFiltST\\tMMList\\tMeMListFiltST\\n\")\nwfile.write(\"Recall\\t\"+str(round(sum(mmFrequencyList)/float(denom),4))+ \"\\t\" + str(round(sum(mm_FiltST_FrequencyList)/float(denom),4))+ \"\\t\" +str(round(sum(mmExtConceptFrequencyList)/float(denom),4))+ \"\\t\" + str(round(sum(mm_FiltST_ExtConceptFrequencyList)/float(denom),4))+\"\\n\")\nmmp = computePrecision(mmFrequencyList, mmFPList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "documentation": {}
    },
    {
        "label": "mmExtConceptFPList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "peekOfCode": "mmExtConceptFPList = computeFP(filePath, mmExtConceptList, gtConceptList)\nmm_FiltST_ExtConceptFPList = computeFP(filePath, mm_FiltST_ExtConceptList, gtConceptList)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/version2/MM_Results.txt\"\nwfile = open (filePath, \"w\")\n##wfile.write(str(sum(w2vFrequencyList))+ \"\\t\" + str(sum(d2vFrequencyList))+ \"\\t\" + str(sum(i2b2FrequencyList))+ \"\\t\" + str(sum(googleFrequencyList))+ \"\\t\" +  str(sum(mmFrequencyList)) +\"\\n\")\n##wfile.write(str(sum(w2vFPList))+ \"\\t\" + str(sum(d2vFPList))+ \"\\t\" + str(sum(i2b2FPList))+ \"\\t\" + str(sum(googleFPList))+ \"\\t\" +  str(sum(mmFPList)) +\"\\n\")\nwfile.write(\"ModelName\\t MMoutput\\tMMoutputFiltST\\tMMList\\tMeMListFiltST\\n\")\nwfile.write(\"Recall\\t\"+str(round(sum(mmFrequencyList)/float(denom),4))+ \"\\t\" + str(round(sum(mm_FiltST_FrequencyList)/float(denom),4))+ \"\\t\" +str(round(sum(mmExtConceptFrequencyList)/float(denom),4))+ \"\\t\" + str(round(sum(mm_FiltST_ExtConceptFrequencyList)/float(denom),4))+\"\\n\")\nmmp = computePrecision(mmFrequencyList, mmFPList)\nmm_FiltST_p = computePrecision(mm_FiltST_FrequencyList, mm_FiltST_FPList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "documentation": {}
    },
    {
        "label": "mm_FiltST_ExtConceptFPList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "peekOfCode": "mm_FiltST_ExtConceptFPList = computeFP(filePath, mm_FiltST_ExtConceptList, gtConceptList)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/version2/MM_Results.txt\"\nwfile = open (filePath, \"w\")\n##wfile.write(str(sum(w2vFrequencyList))+ \"\\t\" + str(sum(d2vFrequencyList))+ \"\\t\" + str(sum(i2b2FrequencyList))+ \"\\t\" + str(sum(googleFrequencyList))+ \"\\t\" +  str(sum(mmFrequencyList)) +\"\\n\")\n##wfile.write(str(sum(w2vFPList))+ \"\\t\" + str(sum(d2vFPList))+ \"\\t\" + str(sum(i2b2FPList))+ \"\\t\" + str(sum(googleFPList))+ \"\\t\" +  str(sum(mmFPList)) +\"\\n\")\nwfile.write(\"ModelName\\t MMoutput\\tMMoutputFiltST\\tMMList\\tMeMListFiltST\\n\")\nwfile.write(\"Recall\\t\"+str(round(sum(mmFrequencyList)/float(denom),4))+ \"\\t\" + str(round(sum(mm_FiltST_FrequencyList)/float(denom),4))+ \"\\t\" +str(round(sum(mmExtConceptFrequencyList)/float(denom),4))+ \"\\t\" + str(round(sum(mm_FiltST_ExtConceptFrequencyList)/float(denom),4))+\"\\n\")\nmmp = computePrecision(mmFrequencyList, mmFPList)\nmm_FiltST_p = computePrecision(mm_FiltST_FrequencyList, mm_FiltST_FPList)\nmmListp = computePrecision(mmExtConceptFrequencyList, mmExtConceptFPList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "peekOfCode": "filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/version2/MM_Results.txt\"\nwfile = open (filePath, \"w\")\n##wfile.write(str(sum(w2vFrequencyList))+ \"\\t\" + str(sum(d2vFrequencyList))+ \"\\t\" + str(sum(i2b2FrequencyList))+ \"\\t\" + str(sum(googleFrequencyList))+ \"\\t\" +  str(sum(mmFrequencyList)) +\"\\n\")\n##wfile.write(str(sum(w2vFPList))+ \"\\t\" + str(sum(d2vFPList))+ \"\\t\" + str(sum(i2b2FPList))+ \"\\t\" + str(sum(googleFPList))+ \"\\t\" +  str(sum(mmFPList)) +\"\\n\")\nwfile.write(\"ModelName\\t MMoutput\\tMMoutputFiltST\\tMMList\\tMeMListFiltST\\n\")\nwfile.write(\"Recall\\t\"+str(round(sum(mmFrequencyList)/float(denom),4))+ \"\\t\" + str(round(sum(mm_FiltST_FrequencyList)/float(denom),4))+ \"\\t\" +str(round(sum(mmExtConceptFrequencyList)/float(denom),4))+ \"\\t\" + str(round(sum(mm_FiltST_ExtConceptFrequencyList)/float(denom),4))+\"\\n\")\nmmp = computePrecision(mmFrequencyList, mmFPList)\nmm_FiltST_p = computePrecision(mm_FiltST_FrequencyList, mm_FiltST_FPList)\nmmListp = computePrecision(mmExtConceptFrequencyList, mmExtConceptFPList)\nmm_FiltST_Listp = computePrecision(mm_FiltST_ExtConceptFrequencyList, mm_FiltST_ExtConceptFPList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "documentation": {}
    },
    {
        "label": "wfile",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "peekOfCode": "wfile = open (filePath, \"w\")\n##wfile.write(str(sum(w2vFrequencyList))+ \"\\t\" + str(sum(d2vFrequencyList))+ \"\\t\" + str(sum(i2b2FrequencyList))+ \"\\t\" + str(sum(googleFrequencyList))+ \"\\t\" +  str(sum(mmFrequencyList)) +\"\\n\")\n##wfile.write(str(sum(w2vFPList))+ \"\\t\" + str(sum(d2vFPList))+ \"\\t\" + str(sum(i2b2FPList))+ \"\\t\" + str(sum(googleFPList))+ \"\\t\" +  str(sum(mmFPList)) +\"\\n\")\nwfile.write(\"ModelName\\t MMoutput\\tMMoutputFiltST\\tMMList\\tMeMListFiltST\\n\")\nwfile.write(\"Recall\\t\"+str(round(sum(mmFrequencyList)/float(denom),4))+ \"\\t\" + str(round(sum(mm_FiltST_FrequencyList)/float(denom),4))+ \"\\t\" +str(round(sum(mmExtConceptFrequencyList)/float(denom),4))+ \"\\t\" + str(round(sum(mm_FiltST_ExtConceptFrequencyList)/float(denom),4))+\"\\n\")\nmmp = computePrecision(mmFrequencyList, mmFPList)\nmm_FiltST_p = computePrecision(mm_FiltST_FrequencyList, mm_FiltST_FPList)\nmmListp = computePrecision(mmExtConceptFrequencyList, mmExtConceptFPList)\nmm_FiltST_Listp = computePrecision(mm_FiltST_ExtConceptFrequencyList, mm_FiltST_ExtConceptFPList)\nwfile.write(\"ModelName\\t MMoutput\\tMMoutputFiltST\\tMMList\\tMeMListFiltST\\n\")",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "documentation": {}
    },
    {
        "label": "mmp",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "peekOfCode": "mmp = computePrecision(mmFrequencyList, mmFPList)\nmm_FiltST_p = computePrecision(mm_FiltST_FrequencyList, mm_FiltST_FPList)\nmmListp = computePrecision(mmExtConceptFrequencyList, mmExtConceptFPList)\nmm_FiltST_Listp = computePrecision(mm_FiltST_ExtConceptFrequencyList, mm_FiltST_ExtConceptFPList)\nwfile.write(\"ModelName\\t MMoutput\\tMMoutputFiltST\\tMMList\\tMeMListFiltST\\n\")\nwfile.write(\"Precision\\t\"+str(mmp)+ \"\\t\" + str(mm_FiltST_p)+ \"\\t\" +str(mmListp)+ \"\\t\" + str(mm_FiltST_Listp)+ \"\\n\")\nwfile.close()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "documentation": {}
    },
    {
        "label": "mm_FiltST_p",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "peekOfCode": "mm_FiltST_p = computePrecision(mm_FiltST_FrequencyList, mm_FiltST_FPList)\nmmListp = computePrecision(mmExtConceptFrequencyList, mmExtConceptFPList)\nmm_FiltST_Listp = computePrecision(mm_FiltST_ExtConceptFrequencyList, mm_FiltST_ExtConceptFPList)\nwfile.write(\"ModelName\\t MMoutput\\tMMoutputFiltST\\tMMList\\tMeMListFiltST\\n\")\nwfile.write(\"Precision\\t\"+str(mmp)+ \"\\t\" + str(mm_FiltST_p)+ \"\\t\" +str(mmListp)+ \"\\t\" + str(mm_FiltST_Listp)+ \"\\n\")\nwfile.close()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "documentation": {}
    },
    {
        "label": "mmListp",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "peekOfCode": "mmListp = computePrecision(mmExtConceptFrequencyList, mmExtConceptFPList)\nmm_FiltST_Listp = computePrecision(mm_FiltST_ExtConceptFrequencyList, mm_FiltST_ExtConceptFPList)\nwfile.write(\"ModelName\\t MMoutput\\tMMoutputFiltST\\tMMList\\tMeMListFiltST\\n\")\nwfile.write(\"Precision\\t\"+str(mmp)+ \"\\t\" + str(mm_FiltST_p)+ \"\\t\" +str(mmListp)+ \"\\t\" + str(mm_FiltST_Listp)+ \"\\n\")\nwfile.close()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "documentation": {}
    },
    {
        "label": "mm_FiltST_Listp",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "peekOfCode": "mm_FiltST_Listp = computePrecision(mm_FiltST_ExtConceptFrequencyList, mm_FiltST_ExtConceptFPList)\nwfile.write(\"ModelName\\t MMoutput\\tMMoutputFiltST\\tMMList\\tMeMListFiltST\\n\")\nwfile.write(\"Precision\\t\"+str(mmp)+ \"\\t\" + str(mm_FiltST_p)+ \"\\t\" +str(mmListp)+ \"\\t\" + str(mm_FiltST_Listp)+ \"\\n\")\nwfile.close()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_MetaMap",
        "documentation": {}
    },
    {
        "label": "fileLength",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "peekOfCode": "def fileLength(fname):\n    with open(fname) as f:\n        for i, l in enumerate(f):\n            pass\n    return i + 1\ndef hasNumbers(inputString):\n    return any(char.isdigit() for char in inputString)\ndef loadTrainingData(filepath, n, remove_stopwords=True):\n    dataList = [[] for x in range(n)]\n    rfile = open(filepath, \"r\")",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "documentation": {}
    },
    {
        "label": "hasNumbers",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "peekOfCode": "def hasNumbers(inputString):\n    return any(char.isdigit() for char in inputString)\ndef loadTrainingData(filepath, n, remove_stopwords=True):\n    dataList = [[] for x in range(n)]\n    rfile = open(filepath, \"r\")\n    for line in rfile:\n        line = line.lower().strip()\n        line = re.sub(\"[^a-zA-Z0-9'-]\",\" \", line)\n        stops = loadStopWords()\n        innerList = line.split()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "documentation": {}
    },
    {
        "label": "loadTrainingData",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "peekOfCode": "def loadTrainingData(filepath, n, remove_stopwords=True):\n    dataList = [[] for x in range(n)]\n    rfile = open(filepath, \"r\")\n    for line in rfile:\n        line = line.lower().strip()\n        line = re.sub(\"[^a-zA-Z0-9'-]\",\" \", line)\n        stops = loadStopWords()\n        innerList = line.split()\n        innerList = [w for w in innerList if not w in stops and not w.isdigit()]\n        dataList.append(innerList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "documentation": {}
    },
    {
        "label": "loadConceptList2",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "peekOfCode": "def loadConceptList2(filepath):\n    conceptWList = list()\n    conceptPList = list()\n    rfile = open(filepath, \"r\")\n    for line in rfile:\n        line = line.lower().strip()\n        if line.find(\" \") is not -1:\n            if line.find(\"\\t\") is -1:\n                conceptPList.append(line)\n            else:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "documentation": {}
    },
    {
        "label": "loadStopWords",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "peekOfCode": "def loadStopWords():\n    filePath = \"/Users/sarahmasudpreum/Desktop/PythonTCode/general/stopwords.txt\"\n    rfile = open(filePath, \"r\")\n    stopWords = list()\n    for line in rfile:\n        stopWords.append(line.strip())\n    rfile.close()\n    return stopWords\ndef loadGTList (filePath):\n    gtWList = list()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "documentation": {}
    },
    {
        "label": "loadGTLis",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "peekOfCode": "def loadGTList (filePath):\n    gtWList = list()\n    gtPList = list()\n    rfile = open(filePath, \"r\")\n    itemList = list()\n##    table = str.maketrans({key: None for key in string.punctuation})\n    for line in rfile:\n        line = line.lower().strip()\n        line = re.sub(\"[^a-zA-Z0-9'-]\",\" \", line)\n        stops = loadStopWords()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "documentation": {}
    },
    {
        "label": "loadMMConceptList",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "peekOfCode": "def loadMMConceptList(filePath):\n    conceptList = list()\n    rfile = open(filePath, \"r\")    \n    for line in rfile:\n        line = line.lower().strip()\n##        line = re.sub(\"[^a-zA-Z0-9'-]\",\" \", line)\n        if line not in conceptList:\n            conceptList.append(line)\n##        items = line.lower().strip().split(\" \")\n##        for item in items:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "documentation": {}
    },
    {
        "label": "loadConceptList",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "peekOfCode": "def loadConceptList(filePath, fielName):\n    conceptList = list()\n##    if \"w2v\" in fielName:\n##        filePath = filePath + \"w2v\" + \".txt\"\n##    if \"d2v\" in fielName:\n##        filePath = filePath + \"d2v\" + \".txt\"\n    if \"i2b2\" in fielName:\n        filePath = filePath + \"i2b2\" + \".txt\"\n    if \"google\" in fielName:\n        filePath = filePath + \"google\" + \".txt\"",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "documentation": {}
    },
    {
        "label": "computeFrequenc",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "peekOfCode": "def computeFrequency (gtConceptList, frequencyList, wordList):\n    wordFrequencyList = list(repeat(0,len(gtConceptList)))\n    for item in wordList:\n        for i in range (0,len(gtConceptList)):\n            wordTokens = item.lower().strip().split(\" \")\n            for word in wordTokens:\n                if word.find(gtConceptList[i]) is not -1 and gtConceptList[i].find(word) is not -1:\n                    wordFrequencyList[i] = frequencyList[i]\n##    filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/May20/RecallPrecision/Results_W2V_CBOW_MM_words_TPWordList.txt\"\n##    wfile = open(filePath, \"a\")",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "documentation": {}
    },
    {
        "label": "computeF",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "peekOfCode": "def computeFP (filePath, conceptList, gtConceptList):\n    filteredConceptList = [x.strip() for x in conceptList if x not in gtConceptList]\n    filteredConceptList = list(set(filteredConceptList))\n    rfile = open(filePath, \"r\")\n    frequencyList = list(repeat(0,len(filteredConceptList)))\n    for line in rfile:\n        for i in range (0,len(filteredConceptList)):\n            wordTokens = line.lower().strip().split(\" \")\n            for word in wordTokens:\n                word = word.strip()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "documentation": {}
    },
    {
        "label": "computePrecisio",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "peekOfCode": "def computePrecision (TPlist, FPList):\n    t = sum(TPlist) + sum(FPList)\n    p = float(0)\n    if t>0:\n        p = sum(TPlist)/float( t )\n    return round( p,4 )\ndef computeRecall (TPlist, denom):\n    p = sum(TPlist)/float(denom)\n    return round( p,4 )\n##load groundTruth",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "documentation": {}
    },
    {
        "label": "computeRecal",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "peekOfCode": "def computeRecall (TPlist, denom):\n    p = sum(TPlist)/float(denom)\n    return round( p,4 )\n##load groundTruth\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/FormattedTags/allInOne_new.txt\"\n##gtConceptList = loadSimpleList(filePath, True)\ngtWList, gtPList = loadGTList(filePath)\ngtConceptList = gtWList\nfrequencyList = list(repeat(0,len(gtConceptList)))\n##count ground truth in testData",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "peekOfCode": "filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/FormattedTags/allInOne_new.txt\"\n##gtConceptList = loadSimpleList(filePath, True)\ngtWList, gtPList = loadGTList(filePath)\ngtConceptList = gtWList\nfrequencyList = list(repeat(0,len(gtConceptList)))\n##count ground truth in testData\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData_new.txt\"\nrfile = open(filePath, \"r\")\nfor line in rfile:\n    for i in range (0,len(gtConceptList)):",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "documentation": {}
    },
    {
        "label": "##gtConceptList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "peekOfCode": "##gtConceptList = loadSimpleList(filePath, True)\ngtWList, gtPList = loadGTList(filePath)\ngtConceptList = gtWList\nfrequencyList = list(repeat(0,len(gtConceptList)))\n##count ground truth in testData\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData_new.txt\"\nrfile = open(filePath, \"r\")\nfor line in rfile:\n    for i in range (0,len(gtConceptList)):\n        wordTokens = line.lower().strip().split(\" \")",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "documentation": {}
    },
    {
        "label": "gtConceptList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "peekOfCode": "gtConceptList = gtWList\nfrequencyList = list(repeat(0,len(gtConceptList)))\n##count ground truth in testData\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData_new.txt\"\nrfile = open(filePath, \"r\")\nfor line in rfile:\n    for i in range (0,len(gtConceptList)):\n        wordTokens = line.lower().strip().split(\" \")\n        for word in wordTokens:\n            if word.find(gtConceptList[i]) is not -1 and gtConceptList[i].find(word) is not -1:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "documentation": {}
    },
    {
        "label": "frequencyList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "peekOfCode": "frequencyList = list(repeat(0,len(gtConceptList)))\n##count ground truth in testData\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData_new.txt\"\nrfile = open(filePath, \"r\")\nfor line in rfile:\n    for i in range (0,len(gtConceptList)):\n        wordTokens = line.lower().strip().split(\" \")\n        for word in wordTokens:\n            if word.find(gtConceptList[i]) is not -1 and gtConceptList[i].find(word) is not -1:\n                frequencyList[i] += 1            ",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "peekOfCode": "filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData_new.txt\"\nrfile = open(filePath, \"r\")\nfor line in rfile:\n    for i in range (0,len(gtConceptList)):\n        wordTokens = line.lower().strip().split(\" \")\n        for word in wordTokens:\n            if word.find(gtConceptList[i]) is not -1 and gtConceptList[i].find(word) is not -1:\n                frequencyList[i] += 1            \nrfile.close()\n##finalList = list(zip(gtConceptList, frequencyList))",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "documentation": {}
    },
    {
        "label": "rfile",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "peekOfCode": "rfile = open(filePath, \"r\")\nfor line in rfile:\n    for i in range (0,len(gtConceptList)):\n        wordTokens = line.lower().strip().split(\" \")\n        for word in wordTokens:\n            if word.find(gtConceptList[i]) is not -1 and gtConceptList[i].find(word) is not -1:\n                frequencyList[i] += 1            \nrfile.close()\n##finalList = list(zip(gtConceptList, frequencyList))\n##for item in finalList:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "documentation": {}
    },
    {
        "label": "##finalList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "peekOfCode": "##finalList = list(zip(gtConceptList, frequencyList))\n##for item in finalList:\n##    if item[1] > 0:\n##        print(str(item[0]) + \" \" + str(item[1]))\n##count word frequnecy in models\n##load originalConceptList\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/conceptList.txt\"\nconceptWList, conceptPList = loadConceptList2(filePath)\nstops = loadStopWords()\n##w2vFrequencyList = computeFrequency (gtConceptList, frequencyList, w2vList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "peekOfCode": "filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/conceptList.txt\"\nconceptWList, conceptPList = loadConceptList2(filePath)\nstops = loadStopWords()\n##w2vFrequencyList = computeFrequency (gtConceptList, frequencyList, w2vList)\n##d2vFrequencyList = computeFrequency (gtConceptList, frequencyList, d2vList)\nSpcFrequencyList = computeFrequency (gtConceptList, frequencyList, conceptWList)\ndenom = sum(frequencyList)\n##finalList = list(zip(gtConceptList, frequencyList, w2vFrequencyList, d2vFrequencyList, i2b2FrequencyList, googleFrequencyList, mmFrequencyList))\n##for item in finalList:\n##    if item[1] > 0:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "documentation": {}
    },
    {
        "label": "stops",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "peekOfCode": "stops = loadStopWords()\n##w2vFrequencyList = computeFrequency (gtConceptList, frequencyList, w2vList)\n##d2vFrequencyList = computeFrequency (gtConceptList, frequencyList, d2vList)\nSpcFrequencyList = computeFrequency (gtConceptList, frequencyList, conceptWList)\ndenom = sum(frequencyList)\n##finalList = list(zip(gtConceptList, frequencyList, w2vFrequencyList, d2vFrequencyList, i2b2FrequencyList, googleFrequencyList, mmFrequencyList))\n##for item in finalList:\n##    if item[1] > 0:\n##        print(str(item[0]) + \" \" + str(item[2]) + \" \" + str(item[3]) + \" \" + str(item[4]) + \" \" + str(item[5]) + \" \" + str(item[6]))\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData_new.txt\"",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "documentation": {}
    },
    {
        "label": "##w2vFrequencyList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "peekOfCode": "##w2vFrequencyList = computeFrequency (gtConceptList, frequencyList, w2vList)\n##d2vFrequencyList = computeFrequency (gtConceptList, frequencyList, d2vList)\nSpcFrequencyList = computeFrequency (gtConceptList, frequencyList, conceptWList)\ndenom = sum(frequencyList)\n##finalList = list(zip(gtConceptList, frequencyList, w2vFrequencyList, d2vFrequencyList, i2b2FrequencyList, googleFrequencyList, mmFrequencyList))\n##for item in finalList:\n##    if item[1] > 0:\n##        print(str(item[0]) + \" \" + str(item[2]) + \" \" + str(item[3]) + \" \" + str(item[4]) + \" \" + str(item[5]) + \" \" + str(item[6]))\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData_new.txt\"\n##w2vFPList = computeFP(filePath, w2vList, gtConceptList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "documentation": {}
    },
    {
        "label": "##d2vFrequencyList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "peekOfCode": "##d2vFrequencyList = computeFrequency (gtConceptList, frequencyList, d2vList)\nSpcFrequencyList = computeFrequency (gtConceptList, frequencyList, conceptWList)\ndenom = sum(frequencyList)\n##finalList = list(zip(gtConceptList, frequencyList, w2vFrequencyList, d2vFrequencyList, i2b2FrequencyList, googleFrequencyList, mmFrequencyList))\n##for item in finalList:\n##    if item[1] > 0:\n##        print(str(item[0]) + \" \" + str(item[2]) + \" \" + str(item[3]) + \" \" + str(item[4]) + \" \" + str(item[5]) + \" \" + str(item[6]))\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData_new.txt\"\n##w2vFPList = computeFP(filePath, w2vList, gtConceptList)\n##d2vFPList = computeFP(filePath, d2vList, gtConceptList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "documentation": {}
    },
    {
        "label": "SpcFrequencyList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "peekOfCode": "SpcFrequencyList = computeFrequency (gtConceptList, frequencyList, conceptWList)\ndenom = sum(frequencyList)\n##finalList = list(zip(gtConceptList, frequencyList, w2vFrequencyList, d2vFrequencyList, i2b2FrequencyList, googleFrequencyList, mmFrequencyList))\n##for item in finalList:\n##    if item[1] > 0:\n##        print(str(item[0]) + \" \" + str(item[2]) + \" \" + str(item[3]) + \" \" + str(item[4]) + \" \" + str(item[5]) + \" \" + str(item[6]))\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData_new.txt\"\n##w2vFPList = computeFP(filePath, w2vList, gtConceptList)\n##d2vFPList = computeFP(filePath, d2vList, gtConceptList)\nSpcFPList = computeFP(filePath, conceptWList, gtConceptList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "documentation": {}
    },
    {
        "label": "denom",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "peekOfCode": "denom = sum(frequencyList)\n##finalList = list(zip(gtConceptList, frequencyList, w2vFrequencyList, d2vFrequencyList, i2b2FrequencyList, googleFrequencyList, mmFrequencyList))\n##for item in finalList:\n##    if item[1] > 0:\n##        print(str(item[0]) + \" \" + str(item[2]) + \" \" + str(item[3]) + \" \" + str(item[4]) + \" \" + str(item[5]) + \" \" + str(item[6]))\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData_new.txt\"\n##w2vFPList = computeFP(filePath, w2vList, gtConceptList)\n##d2vFPList = computeFP(filePath, d2vList, gtConceptList)\nSpcFPList = computeFP(filePath, conceptWList, gtConceptList)\nprint(\"Recall\\t\" + str(round(sum(SpcFrequencyList)/float(denom),4)))",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "documentation": {}
    },
    {
        "label": "##finalList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "peekOfCode": "##finalList = list(zip(gtConceptList, frequencyList, w2vFrequencyList, d2vFrequencyList, i2b2FrequencyList, googleFrequencyList, mmFrequencyList))\n##for item in finalList:\n##    if item[1] > 0:\n##        print(str(item[0]) + \" \" + str(item[2]) + \" \" + str(item[3]) + \" \" + str(item[4]) + \" \" + str(item[5]) + \" \" + str(item[6]))\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData_new.txt\"\n##w2vFPList = computeFP(filePath, w2vList, gtConceptList)\n##d2vFPList = computeFP(filePath, d2vList, gtConceptList)\nSpcFPList = computeFP(filePath, conceptWList, gtConceptList)\nprint(\"Recall\\t\" + str(round(sum(SpcFrequencyList)/float(denom),4)))\n##print(sum(frequencyList), sum(w2vFrequencyList), sum(d2vFrequencyList), sum(i2b2FrequencyList), sum(googleFrequencyList), sum(mmFrequencyList), sum(mmFilteredFrequencyList))",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "peekOfCode": "filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData_new.txt\"\n##w2vFPList = computeFP(filePath, w2vList, gtConceptList)\n##d2vFPList = computeFP(filePath, d2vList, gtConceptList)\nSpcFPList = computeFP(filePath, conceptWList, gtConceptList)\nprint(\"Recall\\t\" + str(round(sum(SpcFrequencyList)/float(denom),4)))\n##print(sum(frequencyList), sum(w2vFrequencyList), sum(d2vFrequencyList), sum(i2b2FrequencyList), sum(googleFrequencyList), sum(mmFrequencyList), sum(mmFilteredFrequencyList))\n##print(sum(w2vFPList), sum(d2vFPList), sum(i2b2FPList), sum(googleFPList), sum(mmFPList), sum(mmFilteredFPList))\n##w2vp = computePrecision(w2vFrequencyList, w2vFPList)\n##d2vp = computePrecision(d2vFrequencyList, d2vFPList)\nSpcp = computePrecision(SpcFrequencyList, SpcFPList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "documentation": {}
    },
    {
        "label": "##w2vFPList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "peekOfCode": "##w2vFPList = computeFP(filePath, w2vList, gtConceptList)\n##d2vFPList = computeFP(filePath, d2vList, gtConceptList)\nSpcFPList = computeFP(filePath, conceptWList, gtConceptList)\nprint(\"Recall\\t\" + str(round(sum(SpcFrequencyList)/float(denom),4)))\n##print(sum(frequencyList), sum(w2vFrequencyList), sum(d2vFrequencyList), sum(i2b2FrequencyList), sum(googleFrequencyList), sum(mmFrequencyList), sum(mmFilteredFrequencyList))\n##print(sum(w2vFPList), sum(d2vFPList), sum(i2b2FPList), sum(googleFPList), sum(mmFPList), sum(mmFilteredFPList))\n##w2vp = computePrecision(w2vFrequencyList, w2vFPList)\n##d2vp = computePrecision(d2vFrequencyList, d2vFPList)\nSpcp = computePrecision(SpcFrequencyList, SpcFPList)\n##wfile.write(\"ModelName\\tWord2Vec\\tDoc2Vec\\tI2B2\\tGoogle\\tMMoutput\\tMMoutputFiltST\\tMM_ManualList\\tMMList\\tMMListFiltST\\n\")",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "documentation": {}
    },
    {
        "label": "##d2vFPList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "peekOfCode": "##d2vFPList = computeFP(filePath, d2vList, gtConceptList)\nSpcFPList = computeFP(filePath, conceptWList, gtConceptList)\nprint(\"Recall\\t\" + str(round(sum(SpcFrequencyList)/float(denom),4)))\n##print(sum(frequencyList), sum(w2vFrequencyList), sum(d2vFrequencyList), sum(i2b2FrequencyList), sum(googleFrequencyList), sum(mmFrequencyList), sum(mmFilteredFrequencyList))\n##print(sum(w2vFPList), sum(d2vFPList), sum(i2b2FPList), sum(googleFPList), sum(mmFPList), sum(mmFilteredFPList))\n##w2vp = computePrecision(w2vFrequencyList, w2vFPList)\n##d2vp = computePrecision(d2vFrequencyList, d2vFPList)\nSpcp = computePrecision(SpcFrequencyList, SpcFPList)\n##wfile.write(\"ModelName\\tWord2Vec\\tDoc2Vec\\tI2B2\\tGoogle\\tMMoutput\\tMMoutputFiltST\\tMM_ManualList\\tMMList\\tMMListFiltST\\n\")\n##wfile.write(\"Precision\\t\"+str(w2vp)+ \"\\t\" + str(d2vp)+ \"\\t\" + str(i2b2p)+ \"\\t\" + str(gp)+ \"\\t\" + str(mmp)+ \"\\t\" + str(mm_FiltST_p)+ \"\\t\" + str(mmManp) + \"\\t\"+ str(mmListp)+ \"\\t\" + str(mm_FiltST_Listp)+ \"\\n\")",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "documentation": {}
    },
    {
        "label": "SpcFPList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "peekOfCode": "SpcFPList = computeFP(filePath, conceptWList, gtConceptList)\nprint(\"Recall\\t\" + str(round(sum(SpcFrequencyList)/float(denom),4)))\n##print(sum(frequencyList), sum(w2vFrequencyList), sum(d2vFrequencyList), sum(i2b2FrequencyList), sum(googleFrequencyList), sum(mmFrequencyList), sum(mmFilteredFrequencyList))\n##print(sum(w2vFPList), sum(d2vFPList), sum(i2b2FPList), sum(googleFPList), sum(mmFPList), sum(mmFilteredFPList))\n##w2vp = computePrecision(w2vFrequencyList, w2vFPList)\n##d2vp = computePrecision(d2vFrequencyList, d2vFPList)\nSpcp = computePrecision(SpcFrequencyList, SpcFPList)\n##wfile.write(\"ModelName\\tWord2Vec\\tDoc2Vec\\tI2B2\\tGoogle\\tMMoutput\\tMMoutputFiltST\\tMM_ManualList\\tMMList\\tMMListFiltST\\n\")\n##wfile.write(\"Precision\\t\"+str(w2vp)+ \"\\t\" + str(d2vp)+ \"\\t\" + str(i2b2p)+ \"\\t\" + str(gp)+ \"\\t\" + str(mmp)+ \"\\t\" + str(mm_FiltST_p)+ \"\\t\" + str(mmManp) + \"\\t\"+ str(mmListp)+ \"\\t\" + str(mm_FiltST_Listp)+ \"\\n\")\nprint(\"Precision\\t\"+ str(Spcp))",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "documentation": {}
    },
    {
        "label": "##w2vp",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "peekOfCode": "##w2vp = computePrecision(w2vFrequencyList, w2vFPList)\n##d2vp = computePrecision(d2vFrequencyList, d2vFPList)\nSpcp = computePrecision(SpcFrequencyList, SpcFPList)\n##wfile.write(\"ModelName\\tWord2Vec\\tDoc2Vec\\tI2B2\\tGoogle\\tMMoutput\\tMMoutputFiltST\\tMM_ManualList\\tMMList\\tMMListFiltST\\n\")\n##wfile.write(\"Precision\\t\"+str(w2vp)+ \"\\t\" + str(d2vp)+ \"\\t\" + str(i2b2p)+ \"\\t\" + str(gp)+ \"\\t\" + str(mmp)+ \"\\t\" + str(mm_FiltST_p)+ \"\\t\" + str(mmManp) + \"\\t\"+ str(mmListp)+ \"\\t\" + str(mm_FiltST_Listp)+ \"\\n\")\nprint(\"Precision\\t\"+ str(Spcp))",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "documentation": {}
    },
    {
        "label": "##d2vp",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "peekOfCode": "##d2vp = computePrecision(d2vFrequencyList, d2vFPList)\nSpcp = computePrecision(SpcFrequencyList, SpcFPList)\n##wfile.write(\"ModelName\\tWord2Vec\\tDoc2Vec\\tI2B2\\tGoogle\\tMMoutput\\tMMoutputFiltST\\tMM_ManualList\\tMMList\\tMMListFiltST\\n\")\n##wfile.write(\"Precision\\t\"+str(w2vp)+ \"\\t\" + str(d2vp)+ \"\\t\" + str(i2b2p)+ \"\\t\" + str(gp)+ \"\\t\" + str(mmp)+ \"\\t\" + str(mm_FiltST_p)+ \"\\t\" + str(mmManp) + \"\\t\"+ str(mmListp)+ \"\\t\" + str(mm_FiltST_Listp)+ \"\\n\")\nprint(\"Precision\\t\"+ str(Spcp))",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "documentation": {}
    },
    {
        "label": "Spcp",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "peekOfCode": "Spcp = computePrecision(SpcFrequencyList, SpcFPList)\n##wfile.write(\"ModelName\\tWord2Vec\\tDoc2Vec\\tI2B2\\tGoogle\\tMMoutput\\tMMoutputFiltST\\tMM_ManualList\\tMMList\\tMMListFiltST\\n\")\n##wfile.write(\"Precision\\t\"+str(w2vp)+ \"\\t\" + str(d2vp)+ \"\\t\" + str(i2b2p)+ \"\\t\" + str(gp)+ \"\\t\" + str(mmp)+ \"\\t\" + str(mm_FiltST_p)+ \"\\t\" + str(mmManp) + \"\\t\"+ str(mmListp)+ \"\\t\" + str(mm_FiltST_Listp)+ \"\\n\")\nprint(\"Precision\\t\"+ str(Spcp))",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline",
        "documentation": {}
    },
    {
        "label": "fileLength",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "peekOfCode": "def fileLength(fname):\n    with open(fname) as f:\n        for i, l in enumerate(f):\n            pass\n    return i + 1\ndef hasNumbers(inputString):\n    return any(char.isdigit() for char in inputString)\ndef loadTrainingData(filepath, n, remove_stopwords=True):\n    dataList = [[] for x in range(n)]\n    rfile = open(filepath, \"r\")",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "documentation": {}
    },
    {
        "label": "hasNumbers",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "peekOfCode": "def hasNumbers(inputString):\n    return any(char.isdigit() for char in inputString)\ndef loadTrainingData(filepath, n, remove_stopwords=True):\n    dataList = [[] for x in range(n)]\n    rfile = open(filepath, \"r\")\n    for line in rfile:\n        line = line.lower().strip()\n        line = re.sub(\"[^a-zA-Z0-9'-]\",\" \", line)\n        stops = loadStopWords()\n        innerList = line.split()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "documentation": {}
    },
    {
        "label": "loadTrainingData",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "peekOfCode": "def loadTrainingData(filepath, n, remove_stopwords=True):\n    dataList = [[] for x in range(n)]\n    rfile = open(filepath, \"r\")\n    for line in rfile:\n        line = line.lower().strip()\n        line = re.sub(\"[^a-zA-Z0-9'-]\",\" \", line)\n        stops = loadStopWords()\n        innerList = line.split()\n        innerList = [w for w in innerList if not w in stops and not w.isdigit()]\n        dataList.append(innerList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "documentation": {}
    },
    {
        "label": "loadConceptList2",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "peekOfCode": "def loadConceptList2(filepath):\n    conceptWList = list()\n    conceptPList = list()\n    rfile = open(filepath, \"r\")\n    for line in rfile:\n        line = line.lower().strip()\n        if line.find(\" \") is not -1:\n            if line.find(\"\\t\") is -1:\n                conceptPList.append(line)\n            else:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "documentation": {}
    },
    {
        "label": "loadStopWords",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "peekOfCode": "def loadStopWords():\n    filePath = \"/Users/sarahmasudpreum/Desktop/PythonTCode/general/stopwords.txt\"\n    rfile = open(filePath, \"r\")\n    stopWords = list()\n    for line in rfile:\n        stopWords.append(line.strip())\n    rfile.close()\n    return stopWords\ndef loadGTList (filePath):\n    gtWList = list()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "documentation": {}
    },
    {
        "label": "loadGTLis",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "peekOfCode": "def loadGTList (filePath):\n    gtWList = list()\n    gtPList = list()\n    rfile = open(filePath, \"r\")\n    itemList = list()\n##    table = str.maketrans({key: None for key in string.punctuation})\n    for line in rfile:\n        line = line.lower().strip()\n        line = re.sub(\"[^a-zA-Z0-9'-]\",\" \", line)\n        stops = loadStopWords()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "documentation": {}
    },
    {
        "label": "loadMMConceptList",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "peekOfCode": "def loadMMConceptList(filePath):\n    conceptList = list()\n    rfile = open(filePath, \"r\")    \n    for line in rfile:\n        line = line.lower().strip()\n##        line = re.sub(\"[^a-zA-Z0-9'-]\",\" \", line)\n        if line not in conceptList:\n            conceptList.append(line)\n##        items = line.lower().strip().split(\" \")\n##        for item in items:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "documentation": {}
    },
    {
        "label": "loadConceptList",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "peekOfCode": "def loadConceptList(filePath, fielName):\n    conceptList = list()\n##    if \"w2v\" in fielName:\n##        filePath = filePath + \"w2v\" + \".txt\"\n##    if \"d2v\" in fielName:\n##        filePath = filePath + \"d2v\" + \".txt\"\n    if \"i2b2\" in fielName:\n        filePath = filePath + \"i2b2\" + \".txt\"\n    if \"google\" in fielName:\n        filePath = filePath + \"google\" + \".txt\"",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "documentation": {}
    },
    {
        "label": "computeFrequenc",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "peekOfCode": "def computeFrequency (gtConceptList, frequencyList, wordList):\n    wordFrequencyList = list(repeat(0,len(gtConceptList)))\n    for item in wordList:\n        for i in range (0,len(gtConceptList)):\n            gTerm = str(gtConceptList[i]).lower().strip()\n            item = item.lower().strip()\n##            if len(gTerm.split(\" \")) > 1:\n            if item.find(gTerm) is not -1: #or gTerm.find(item) is not -1:\n                wordFrequencyList[i] = frequencyList[i]\n    # filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/May20/RecallPrecision/Results_W2V_CBOW_phrases_AvgDist_TP_Phrases.txt\"",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "documentation": {}
    },
    {
        "label": "computeF",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "peekOfCode": "def computeFP (filePath, conceptList, gtConceptList):\n##    filteredConceptList = [x.strip() for x in conceptList if x not in gtConceptList]\n    tpConceptList = list()\n    for item in conceptList:\n        for i in range (0,len(gtConceptList)):\n            gTerm = str(gtConceptList[i]).lower().strip()\n            item = item.lower().strip()\n##            if len(gTerm.split(\" \")) > 1:\n            if item.find(gTerm) is not -1: #or gTerm.find(item) is not -1:\n                tpConceptList.append(item)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "documentation": {}
    },
    {
        "label": "computePrecisio",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "peekOfCode": "def computePrecision (TPlist, FPList):\n    t = sum(TPlist) + sum(FPList)\n    p = float(0)\n    if t>0:\n        p = sum(TPlist)/float( t )\n    return round( p,4 )\ndef computeRecall (TPlist, denom):\n    p = sum(TPlist)/float(denom)\n    return round( p,4 )\n##load groundTruth",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "documentation": {}
    },
    {
        "label": "computeRecal",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "peekOfCode": "def computeRecall (TPlist, denom):\n    p = sum(TPlist)/float(denom)\n    return round( p,4 )\n##load groundTruth\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/FormattedTags/allInOne_new.txt\"\n##gtConceptList = loadSimpleList(filePath, True)\ngtWList, gtPList = loadGTList(filePath)\ngtConceptList = gtPList\nfrequencyList = list(repeat(0,len(gtConceptList)))\n##count ground truth in testData",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "peekOfCode": "filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/FormattedTags/allInOne_new.txt\"\n##gtConceptList = loadSimpleList(filePath, True)\ngtWList, gtPList = loadGTList(filePath)\ngtConceptList = gtPList\nfrequencyList = list(repeat(0,len(gtConceptList)))\n##count ground truth in testData\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData_new.txt\"\nrfile = open(filePath, \"r\")\nfor line in rfile:\n    for i in range (0,len(gtConceptList)):",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "documentation": {}
    },
    {
        "label": "##gtConceptList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "peekOfCode": "##gtConceptList = loadSimpleList(filePath, True)\ngtWList, gtPList = loadGTList(filePath)\ngtConceptList = gtPList\nfrequencyList = list(repeat(0,len(gtConceptList)))\n##count ground truth in testData\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData_new.txt\"\nrfile = open(filePath, \"r\")\nfor line in rfile:\n    for i in range (0,len(gtConceptList)):\n        line = line.lower().strip()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "documentation": {}
    },
    {
        "label": "gtConceptList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "peekOfCode": "gtConceptList = gtPList\nfrequencyList = list(repeat(0,len(gtConceptList)))\n##count ground truth in testData\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData_new.txt\"\nrfile = open(filePath, \"r\")\nfor line in rfile:\n    for i in range (0,len(gtConceptList)):\n        line = line.lower().strip()\n        line = re.sub(\"[^a-zA-Z0-9'-]\",\" \", line)\n        if line.find(gtConceptList[i]) is not -1:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "documentation": {}
    },
    {
        "label": "frequencyList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "peekOfCode": "frequencyList = list(repeat(0,len(gtConceptList)))\n##count ground truth in testData\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData_new.txt\"\nrfile = open(filePath, \"r\")\nfor line in rfile:\n    for i in range (0,len(gtConceptList)):\n        line = line.lower().strip()\n        line = re.sub(\"[^a-zA-Z0-9'-]\",\" \", line)\n        if line.find(gtConceptList[i]) is not -1:\n            frequencyList[i] += 1 ",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "peekOfCode": "filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData_new.txt\"\nrfile = open(filePath, \"r\")\nfor line in rfile:\n    for i in range (0,len(gtConceptList)):\n        line = line.lower().strip()\n        line = re.sub(\"[^a-zA-Z0-9'-]\",\" \", line)\n        if line.find(gtConceptList[i]) is not -1:\n            frequencyList[i] += 1 \n        # wordTokens = line.lower().strip().split(\" \")\n        # for word in wordTokens:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "documentation": {}
    },
    {
        "label": "rfile",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "peekOfCode": "rfile = open(filePath, \"r\")\nfor line in rfile:\n    for i in range (0,len(gtConceptList)):\n        line = line.lower().strip()\n        line = re.sub(\"[^a-zA-Z0-9'-]\",\" \", line)\n        if line.find(gtConceptList[i]) is not -1:\n            frequencyList[i] += 1 \n        # wordTokens = line.lower().strip().split(\" \")\n        # for word in wordTokens:\n        #     if word.find(gtConceptList[i]) is not -1 and gtConceptList[i].find(word) is not -1:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "peekOfCode": "filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/conceptList.txt\"\nconceptWList, conceptListP = loadConceptList2(filePath)\nstops = loadStopWords()\n##w2vFrequencyList = computeFrequency (gtConceptList, frequencyList, w2vList)\n##d2vFrequencyList = computeFrequency (gtConceptList, frequencyList, d2vList)\nSpcFrequencyList = computeFrequency (gtConceptList, frequencyList, conceptListP)\ndenom = sum(frequencyList)\n##finalList = list(zip(gtConceptList, frequencyList, w2vFrequencyList, d2vFrequencyList, i2b2FrequencyList, googleFrequencyList, mmFrequencyList))\n##for item in finalList:\n##    if item[1] > 0:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "documentation": {}
    },
    {
        "label": "stops",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "peekOfCode": "stops = loadStopWords()\n##w2vFrequencyList = computeFrequency (gtConceptList, frequencyList, w2vList)\n##d2vFrequencyList = computeFrequency (gtConceptList, frequencyList, d2vList)\nSpcFrequencyList = computeFrequency (gtConceptList, frequencyList, conceptListP)\ndenom = sum(frequencyList)\n##finalList = list(zip(gtConceptList, frequencyList, w2vFrequencyList, d2vFrequencyList, i2b2FrequencyList, googleFrequencyList, mmFrequencyList))\n##for item in finalList:\n##    if item[1] > 0:\n##        print(str(item[0]) + \" \" + str(item[2]) + \" \" + str(item[3]) + \" \" + str(item[4]) + \" \" + str(item[5]) + \" \" + str(item[6]))\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData_new.txt\"",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "documentation": {}
    },
    {
        "label": "##w2vFrequencyList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "peekOfCode": "##w2vFrequencyList = computeFrequency (gtConceptList, frequencyList, w2vList)\n##d2vFrequencyList = computeFrequency (gtConceptList, frequencyList, d2vList)\nSpcFrequencyList = computeFrequency (gtConceptList, frequencyList, conceptListP)\ndenom = sum(frequencyList)\n##finalList = list(zip(gtConceptList, frequencyList, w2vFrequencyList, d2vFrequencyList, i2b2FrequencyList, googleFrequencyList, mmFrequencyList))\n##for item in finalList:\n##    if item[1] > 0:\n##        print(str(item[0]) + \" \" + str(item[2]) + \" \" + str(item[3]) + \" \" + str(item[4]) + \" \" + str(item[5]) + \" \" + str(item[6]))\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData_new.txt\"\n##w2vFPList = computeFP(filePath, w2vList, gtConceptList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "documentation": {}
    },
    {
        "label": "##d2vFrequencyList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "peekOfCode": "##d2vFrequencyList = computeFrequency (gtConceptList, frequencyList, d2vList)\nSpcFrequencyList = computeFrequency (gtConceptList, frequencyList, conceptListP)\ndenom = sum(frequencyList)\n##finalList = list(zip(gtConceptList, frequencyList, w2vFrequencyList, d2vFrequencyList, i2b2FrequencyList, googleFrequencyList, mmFrequencyList))\n##for item in finalList:\n##    if item[1] > 0:\n##        print(str(item[0]) + \" \" + str(item[2]) + \" \" + str(item[3]) + \" \" + str(item[4]) + \" \" + str(item[5]) + \" \" + str(item[6]))\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData_new.txt\"\n##w2vFPList = computeFP(filePath, w2vList, gtConceptList)\n##d2vFPList = computeFP(filePath, d2vList, gtConceptList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "documentation": {}
    },
    {
        "label": "SpcFrequencyList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "peekOfCode": "SpcFrequencyList = computeFrequency (gtConceptList, frequencyList, conceptListP)\ndenom = sum(frequencyList)\n##finalList = list(zip(gtConceptList, frequencyList, w2vFrequencyList, d2vFrequencyList, i2b2FrequencyList, googleFrequencyList, mmFrequencyList))\n##for item in finalList:\n##    if item[1] > 0:\n##        print(str(item[0]) + \" \" + str(item[2]) + \" \" + str(item[3]) + \" \" + str(item[4]) + \" \" + str(item[5]) + \" \" + str(item[6]))\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData_new.txt\"\n##w2vFPList = computeFP(filePath, w2vList, gtConceptList)\n##d2vFPList = computeFP(filePath, d2vList, gtConceptList)\nSpcFPList = computeFP(filePath, conceptListP, gtConceptList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "documentation": {}
    },
    {
        "label": "denom",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "peekOfCode": "denom = sum(frequencyList)\n##finalList = list(zip(gtConceptList, frequencyList, w2vFrequencyList, d2vFrequencyList, i2b2FrequencyList, googleFrequencyList, mmFrequencyList))\n##for item in finalList:\n##    if item[1] > 0:\n##        print(str(item[0]) + \" \" + str(item[2]) + \" \" + str(item[3]) + \" \" + str(item[4]) + \" \" + str(item[5]) + \" \" + str(item[6]))\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData_new.txt\"\n##w2vFPList = computeFP(filePath, w2vList, gtConceptList)\n##d2vFPList = computeFP(filePath, d2vList, gtConceptList)\nSpcFPList = computeFP(filePath, conceptListP, gtConceptList)\n##print(sum(frequencyList), sum(w2vFrequencyList), sum(d2vFrequencyList), sum(i2b2FrequencyList), sum(googleFrequencyList), sum(mmFrequencyList), sum(mmFilteredFrequencyList))",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "documentation": {}
    },
    {
        "label": "##finalList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "peekOfCode": "##finalList = list(zip(gtConceptList, frequencyList, w2vFrequencyList, d2vFrequencyList, i2b2FrequencyList, googleFrequencyList, mmFrequencyList))\n##for item in finalList:\n##    if item[1] > 0:\n##        print(str(item[0]) + \" \" + str(item[2]) + \" \" + str(item[3]) + \" \" + str(item[4]) + \" \" + str(item[5]) + \" \" + str(item[6]))\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData_new.txt\"\n##w2vFPList = computeFP(filePath, w2vList, gtConceptList)\n##d2vFPList = computeFP(filePath, d2vList, gtConceptList)\nSpcFPList = computeFP(filePath, conceptListP, gtConceptList)\n##print(sum(frequencyList), sum(w2vFrequencyList), sum(d2vFrequencyList), sum(i2b2FrequencyList), sum(googleFrequencyList), sum(mmFrequencyList), sum(mmFilteredFrequencyList))\n##print(sum(w2vFPList), sum(d2vFPList), sum(i2b2FPList), sum(googleFPList), sum(mmFPList), sum(mmFilteredFPList))",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "peekOfCode": "filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData_new.txt\"\n##w2vFPList = computeFP(filePath, w2vList, gtConceptList)\n##d2vFPList = computeFP(filePath, d2vList, gtConceptList)\nSpcFPList = computeFP(filePath, conceptListP, gtConceptList)\n##print(sum(frequencyList), sum(w2vFrequencyList), sum(d2vFrequencyList), sum(i2b2FrequencyList), sum(googleFrequencyList), sum(mmFrequencyList), sum(mmFilteredFrequencyList))\n##print(sum(w2vFPList), sum(d2vFPList), sum(i2b2FPList), sum(googleFPList), sum(mmFPList), sum(mmFilteredFPList))\n##wfile.write(\"Recall\\t\"+str(round(sum(w2vFrequencyList)/float(denom),4))+ \"\\t\" + str(round(sum(d2vFrequencyList)/float(denom),4))+ \"\\t\" + str(round(sum(i2b2FrequencyList)/float(denom),4))+ \"\\t\" + str(round(sum(googleFrequencyList)/float(denom),4))+\"\\t\"+ str(round(sum(mmFrequencyList)/float(denom),4))+ \"\\t\" + str(round(sum(mm_FiltST_FrequencyList)/float(denom),4))+ \"\\t\" +  str(round(sum(mmManFrequencyList)/float(denom),4)) + \"\\t\" +str(round(sum(mmExtConceptFrequencyList)/float(denom),4))+ \"\\t\" + str(round(sum(mm_FiltST_ExtConceptFrequencyList)/float(denom),4))+\"\\n\")\nprint(\"Recall\\t\" + str(round(sum(SpcFrequencyList)/float(denom),4)))\n##w2vp = computePrecision(w2vFrequencyList, w2vFPList)\n##d2vp = computePrecision(d2vFrequencyList, d2vFPList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "documentation": {}
    },
    {
        "label": "##w2vFPList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "peekOfCode": "##w2vFPList = computeFP(filePath, w2vList, gtConceptList)\n##d2vFPList = computeFP(filePath, d2vList, gtConceptList)\nSpcFPList = computeFP(filePath, conceptListP, gtConceptList)\n##print(sum(frequencyList), sum(w2vFrequencyList), sum(d2vFrequencyList), sum(i2b2FrequencyList), sum(googleFrequencyList), sum(mmFrequencyList), sum(mmFilteredFrequencyList))\n##print(sum(w2vFPList), sum(d2vFPList), sum(i2b2FPList), sum(googleFPList), sum(mmFPList), sum(mmFilteredFPList))\n##wfile.write(\"Recall\\t\"+str(round(sum(w2vFrequencyList)/float(denom),4))+ \"\\t\" + str(round(sum(d2vFrequencyList)/float(denom),4))+ \"\\t\" + str(round(sum(i2b2FrequencyList)/float(denom),4))+ \"\\t\" + str(round(sum(googleFrequencyList)/float(denom),4))+\"\\t\"+ str(round(sum(mmFrequencyList)/float(denom),4))+ \"\\t\" + str(round(sum(mm_FiltST_FrequencyList)/float(denom),4))+ \"\\t\" +  str(round(sum(mmManFrequencyList)/float(denom),4)) + \"\\t\" +str(round(sum(mmExtConceptFrequencyList)/float(denom),4))+ \"\\t\" + str(round(sum(mm_FiltST_ExtConceptFrequencyList)/float(denom),4))+\"\\n\")\nprint(\"Recall\\t\" + str(round(sum(SpcFrequencyList)/float(denom),4)))\n##w2vp = computePrecision(w2vFrequencyList, w2vFPList)\n##d2vp = computePrecision(d2vFrequencyList, d2vFPList)\nSpcp = computePrecision(SpcFrequencyList, SpcFPList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "documentation": {}
    },
    {
        "label": "##d2vFPList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "peekOfCode": "##d2vFPList = computeFP(filePath, d2vList, gtConceptList)\nSpcFPList = computeFP(filePath, conceptListP, gtConceptList)\n##print(sum(frequencyList), sum(w2vFrequencyList), sum(d2vFrequencyList), sum(i2b2FrequencyList), sum(googleFrequencyList), sum(mmFrequencyList), sum(mmFilteredFrequencyList))\n##print(sum(w2vFPList), sum(d2vFPList), sum(i2b2FPList), sum(googleFPList), sum(mmFPList), sum(mmFilteredFPList))\n##wfile.write(\"Recall\\t\"+str(round(sum(w2vFrequencyList)/float(denom),4))+ \"\\t\" + str(round(sum(d2vFrequencyList)/float(denom),4))+ \"\\t\" + str(round(sum(i2b2FrequencyList)/float(denom),4))+ \"\\t\" + str(round(sum(googleFrequencyList)/float(denom),4))+\"\\t\"+ str(round(sum(mmFrequencyList)/float(denom),4))+ \"\\t\" + str(round(sum(mm_FiltST_FrequencyList)/float(denom),4))+ \"\\t\" +  str(round(sum(mmManFrequencyList)/float(denom),4)) + \"\\t\" +str(round(sum(mmExtConceptFrequencyList)/float(denom),4))+ \"\\t\" + str(round(sum(mm_FiltST_ExtConceptFrequencyList)/float(denom),4))+\"\\n\")\nprint(\"Recall\\t\" + str(round(sum(SpcFrequencyList)/float(denom),4)))\n##w2vp = computePrecision(w2vFrequencyList, w2vFPList)\n##d2vp = computePrecision(d2vFrequencyList, d2vFPList)\nSpcp = computePrecision(SpcFrequencyList, SpcFPList)\n##wfile.write(\"ModelName\\tWord2Vec\\tDoc2Vec\\tI2B2\\tGoogle\\tMMoutput\\tMMoutputFiltST\\tMM_ManualList\\tMMList\\tMMListFiltST\\n\")",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "documentation": {}
    },
    {
        "label": "SpcFPList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "peekOfCode": "SpcFPList = computeFP(filePath, conceptListP, gtConceptList)\n##print(sum(frequencyList), sum(w2vFrequencyList), sum(d2vFrequencyList), sum(i2b2FrequencyList), sum(googleFrequencyList), sum(mmFrequencyList), sum(mmFilteredFrequencyList))\n##print(sum(w2vFPList), sum(d2vFPList), sum(i2b2FPList), sum(googleFPList), sum(mmFPList), sum(mmFilteredFPList))\n##wfile.write(\"Recall\\t\"+str(round(sum(w2vFrequencyList)/float(denom),4))+ \"\\t\" + str(round(sum(d2vFrequencyList)/float(denom),4))+ \"\\t\" + str(round(sum(i2b2FrequencyList)/float(denom),4))+ \"\\t\" + str(round(sum(googleFrequencyList)/float(denom),4))+\"\\t\"+ str(round(sum(mmFrequencyList)/float(denom),4))+ \"\\t\" + str(round(sum(mm_FiltST_FrequencyList)/float(denom),4))+ \"\\t\" +  str(round(sum(mmManFrequencyList)/float(denom),4)) + \"\\t\" +str(round(sum(mmExtConceptFrequencyList)/float(denom),4))+ \"\\t\" + str(round(sum(mm_FiltST_ExtConceptFrequencyList)/float(denom),4))+\"\\n\")\nprint(\"Recall\\t\" + str(round(sum(SpcFrequencyList)/float(denom),4)))\n##w2vp = computePrecision(w2vFrequencyList, w2vFPList)\n##d2vp = computePrecision(d2vFrequencyList, d2vFPList)\nSpcp = computePrecision(SpcFrequencyList, SpcFPList)\n##wfile.write(\"ModelName\\tWord2Vec\\tDoc2Vec\\tI2B2\\tGoogle\\tMMoutput\\tMMoutputFiltST\\tMM_ManualList\\tMMList\\tMMListFiltST\\n\")\n##wfile.write(\"Precision\\t\"+str(w2vp)+ \"\\t\" + str(d2vp)+ \"\\t\" + str(i2b2p)+ \"\\t\" + str(gp)+ \"\\t\" + str(mmp)+ \"\\t\" + str(mm_FiltST_p)+ \"\\t\" + str(mmManp) + \"\\t\"+ str(mmListp)+ \"\\t\" + str(mm_FiltST_Listp)+ \"\\n\")",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "documentation": {}
    },
    {
        "label": "##w2vp",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "peekOfCode": "##w2vp = computePrecision(w2vFrequencyList, w2vFPList)\n##d2vp = computePrecision(d2vFrequencyList, d2vFPList)\nSpcp = computePrecision(SpcFrequencyList, SpcFPList)\n##wfile.write(\"ModelName\\tWord2Vec\\tDoc2Vec\\tI2B2\\tGoogle\\tMMoutput\\tMMoutputFiltST\\tMM_ManualList\\tMMList\\tMMListFiltST\\n\")\n##wfile.write(\"Precision\\t\"+str(w2vp)+ \"\\t\" + str(d2vp)+ \"\\t\" + str(i2b2p)+ \"\\t\" + str(gp)+ \"\\t\" + str(mmp)+ \"\\t\" + str(mm_FiltST_p)+ \"\\t\" + str(mmManp) + \"\\t\"+ str(mmListp)+ \"\\t\" + str(mm_FiltST_Listp)+ \"\\n\")\nprint(\"Precision\\t\"+ str(Spcp))",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "documentation": {}
    },
    {
        "label": "##d2vp",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "peekOfCode": "##d2vp = computePrecision(d2vFrequencyList, d2vFPList)\nSpcp = computePrecision(SpcFrequencyList, SpcFPList)\n##wfile.write(\"ModelName\\tWord2Vec\\tDoc2Vec\\tI2B2\\tGoogle\\tMMoutput\\tMMoutputFiltST\\tMM_ManualList\\tMMList\\tMMListFiltST\\n\")\n##wfile.write(\"Precision\\t\"+str(w2vp)+ \"\\t\" + str(d2vp)+ \"\\t\" + str(i2b2p)+ \"\\t\" + str(gp)+ \"\\t\" + str(mmp)+ \"\\t\" + str(mm_FiltST_p)+ \"\\t\" + str(mmManp) + \"\\t\"+ str(mmListp)+ \"\\t\" + str(mm_FiltST_Listp)+ \"\\n\")\nprint(\"Precision\\t\"+ str(Spcp))",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "documentation": {}
    },
    {
        "label": "Spcp",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "peekOfCode": "Spcp = computePrecision(SpcFrequencyList, SpcFPList)\n##wfile.write(\"ModelName\\tWord2Vec\\tDoc2Vec\\tI2B2\\tGoogle\\tMMoutput\\tMMoutputFiltST\\tMM_ManualList\\tMMList\\tMMListFiltST\\n\")\n##wfile.write(\"Precision\\t\"+str(w2vp)+ \"\\t\" + str(d2vp)+ \"\\t\" + str(i2b2p)+ \"\\t\" + str(gp)+ \"\\t\" + str(mmp)+ \"\\t\" + str(mm_FiltST_p)+ \"\\t\" + str(mmManp) + \"\\t\"+ str(mmListp)+ \"\\t\" + str(mm_FiltST_Listp)+ \"\\n\")\nprint(\"Precision\\t\"+ str(Spcp))",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_Spc_baseline_phrase",
        "documentation": {}
    },
    {
        "label": "fileLength",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "def fileLength(fname):\n    with open(fname) as f:\n        for i, l in enumerate(f):\n            pass\n    return i + 1\ndef hasNumbers(inputString):\n    return any(char.isdigit() for char in inputString)\n# def loadTrainingData(filepath, n, remove_stopwords=True):\n#     dataList = [[] for x in range(n)]\n#     rfile = open(filepath, \"r\")",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "hasNumbers",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "def hasNumbers(inputString):\n    return any(char.isdigit() for char in inputString)\n# def loadTrainingData(filepath, n, remove_stopwords=True):\n#     dataList = [[] for x in range(n)]\n#     rfile = open(filepath, \"r\")\n#     for line in rfile:\n#         line = line.lower().strip()\n#         line = re.sub(\"[^a-zA-Z0-9'-]\",\" \", line)\n#         stops = loadStopWords()\n#         innerList = line.split()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "loadConceptList2",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "def loadConceptList2(filepath):\n    conceptWList = list()\n    conceptPList = list()\n    rfile = open(filepath, \"r\")\n    for line in rfile:\n        line = line.lower().strip()\n        if line.find(\" \") is not -1:\n            if line.find(\"\\t\") is -1:\n                conceptPList.append(line)\n            else:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "loadStopWords",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "def loadStopWords():\n    filePath = \"/Users/sarahmasudpreum/Desktop/PythonTCode/general/stopwords.txt\"\n    rfile = open(filePath, \"r\")\n    stopWords = list()\n    for line in rfile:\n        stopWords.append(line.strip())\n    rfile.close()\n    return stopWords\ndef loadGTList (filePath):\n    gtWList = list()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "loadGTLis",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "def loadGTList (filePath):\n    gtWList = list()\n    gtPList = list()\n    rfile = open(filePath, \"r\")\n    itemList = list()\n##    table = str.maketrans({key: None for key in string.punctuation})\n    for line in rfile:\n        line = line.lower().strip()\n        line = re.sub(\"[^a-zA-Z0-9 ]\",\" \", line)\n        stops = loadStopWords()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "loadMMConceptList",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "def loadMMConceptList(filePath):\n    conceptList = list()\n    rfile = open(filePath, \"r\")    \n    for line in rfile:\n        line = line.lower().strip()\n##        line = re.sub(\"[^a-zA-Z0-9'-]\",\" \", line)\n        if line not in conceptList:\n            conceptList.append(line)\n##        items = line.lower().strip().split(\" \")\n##        for item in items:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "loadConceptList",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "def loadConceptList(filePath):\n    conceptWList = list()\n    rfile = open(filePath, \"r\")\n    for line in rfile:\n        # line = re.sub(\"[^a-zA-Z0-9'-]\",\" \", line)\n        line = line.lower().strip()\n        tokens = line.split(\"\\t\")\n        for token in tokens:\n            if token not in conceptWList:\n                conceptWList.append(token)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "computeFrequenc",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "def computeFrequency (gtConceptList, frequencyList, wordList):\n    wordFrequencyList = list(repeat(0,len(gtConceptList)))\n    for item in wordList:\n        flag = 1\n        for i in range (0,len(gtConceptList)):\n            wordTokens = item.lower().strip()#.split(\" \")\n            if wordTokens.find(gtConceptList[i]) is not -1 and gtConceptList[i].find(wordTokens) is not -1:\n               wordFrequencyList[i] = frequencyList[i]\n               flag = 0\n               print(item)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "computeF",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "def computeFP (filePath, conceptList, gtConceptList):\n    filteredConceptList = [x.strip() for x in conceptList if x not in gtConceptList]\n    filteredConceptList = list(set(filteredConceptList))\n    rfile = open(filePath, \"r\")\n    frequencyList = list(repeat(0,len(filteredConceptList)))\n    for line in rfile:\n        for i in range (0,len(filteredConceptList)):\n            wordTokens = line.lower().strip()#.split(\" \")\n            # if wordTokens.find(filteredConceptList[i]) is not -1 and filteredConceptList[i].find(wordTokens) is not -1:\n            #         frequencyList[i] += 1",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "computePrecisio",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "def computePrecision (TPlist, FPList):\n    t = sum(TPlist) + sum(FPList)\n    # print(\"*** Precision \"+str(t))\n    p = float(0)\n    if t>0:\n        p = sum(TPlist)/float( t )\n    return round( p,4 )\ndef computeRecall (TPlist, denom):\n    p = float(0)\n    if denom>0:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "computeRecal",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "def computeRecall (TPlist, denom):\n    p = float(0)\n    if denom>0:\n        p = sum(TPlist)/float(denom)\n    return round( p,4 )\n##load groundTruth\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/FormattedTags/allInOne_new.txt\"\n##gtConceptList = loadSimpleList(filePath, True)\ngtWList, gtPList = loadGTList(filePath)\ngtConceptList = gtWList",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/FormattedTags/allInOne_new.txt\"\n##gtConceptList = loadSimpleList(filePath, True)\ngtWList, gtPList = loadGTList(filePath)\ngtConceptList = gtWList\nfrequencyList = list(repeat(0,len(gtConceptList)))\n##count ground truth in testData\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData_new.txt\"\nrfile = open(filePath, \"r\")\nfor line in rfile:\n    for i in range (0,len(gtConceptList)):",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "##gtConceptList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "##gtConceptList = loadSimpleList(filePath, True)\ngtWList, gtPList = loadGTList(filePath)\ngtConceptList = gtWList\nfrequencyList = list(repeat(0,len(gtConceptList)))\n##count ground truth in testData\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData_new.txt\"\nrfile = open(filePath, \"r\")\nfor line in rfile:\n    for i in range (0,len(gtConceptList)):\n        wordTokens = line.lower().strip().split(\" \")",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "gtConceptList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "gtConceptList = gtWList\nfrequencyList = list(repeat(0,len(gtConceptList)))\n##count ground truth in testData\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData_new.txt\"\nrfile = open(filePath, \"r\")\nfor line in rfile:\n    for i in range (0,len(gtConceptList)):\n        wordTokens = line.lower().strip().split(\" \")\n        for word in wordTokens:\n            if word.find(gtConceptList[i]) is not -1 and gtConceptList[i].find(word) is not -1:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "frequencyList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "frequencyList = list(repeat(0,len(gtConceptList)))\n##count ground truth in testData\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData_new.txt\"\nrfile = open(filePath, \"r\")\nfor line in rfile:\n    for i in range (0,len(gtConceptList)):\n        wordTokens = line.lower().strip().split(\" \")\n        for word in wordTokens:\n            if word.find(gtConceptList[i]) is not -1 and gtConceptList[i].find(word) is not -1:\n                frequencyList[i] += 1            ",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData_new.txt\"\nrfile = open(filePath, \"r\")\nfor line in rfile:\n    for i in range (0,len(gtConceptList)):\n        wordTokens = line.lower().strip().split(\" \")\n        for word in wordTokens:\n            if word.find(gtConceptList[i]) is not -1 and gtConceptList[i].find(word) is not -1:\n                frequencyList[i] += 1            \nrfile.close()\n##finalList = list(zip(gtConceptList, frequencyList))",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "rfile",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "rfile = open(filePath, \"r\")\nfor line in rfile:\n    for i in range (0,len(gtConceptList)):\n        wordTokens = line.lower().strip().split(\" \")\n        for word in wordTokens:\n            if word.find(gtConceptList[i]) is not -1 and gtConceptList[i].find(word) is not -1:\n                frequencyList[i] += 1            \nrfile.close()\n##finalList = list(zip(gtConceptList, frequencyList))\n##for item in finalList:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "##finalList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "##finalList = list(zip(gtConceptList, frequencyList))\n##for item in finalList:\n##    if item[1] > 0:\n##        print(str(item[0]) + \" \" + str(item[1]))\n##count word frequnecy in models\n##load originalConceptList\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/conceptList.txt\"\nconceptWList, conceptPList = loadConceptList2(filePath)\nstops = loadStopWords()\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/25candidateConcepts_\" + \"RAA\"+\".txt\"",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/conceptList.txt\"\nconceptWList, conceptPList = loadConceptList2(filePath)\nstops = loadStopWords()\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/25candidateConcepts_\" + \"RAA\"+\".txt\"\nRAAList = loadConceptList(filePath)\n# print(\"RAAListLen: \"+str(len(RAAList)))\n# for item in RAAList:\n#     print(item)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/25candidateConcepts_\" + \"Forum\"+\".txt\"\nForumList = loadConceptList(filePath)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "stops",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "stops = loadStopWords()\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/25candidateConcepts_\" + \"RAA\"+\".txt\"\nRAAList = loadConceptList(filePath)\n# print(\"RAAListLen: \"+str(len(RAAList)))\n# for item in RAAList:\n#     print(item)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/25candidateConcepts_\" + \"Forum\"+\".txt\"\nForumList = loadConceptList(filePath)\n# print(\"ForumListLen: \"+str(len(ForumList)))\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/25candidateConcepts_\" + \"EMS\"+\".txt\"",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/25candidateConcepts_\" + \"RAA\"+\".txt\"\nRAAList = loadConceptList(filePath)\n# print(\"RAAListLen: \"+str(len(RAAList)))\n# for item in RAAList:\n#     print(item)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/25candidateConcepts_\" + \"Forum\"+\".txt\"\nForumList = loadConceptList(filePath)\n# print(\"ForumListLen: \"+str(len(ForumList)))\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/25candidateConcepts_\" + \"EMS\"+\".txt\"\nEMSList = loadConceptList(filePath)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "RAAList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "RAAList = loadConceptList(filePath)\n# print(\"RAAListLen: \"+str(len(RAAList)))\n# for item in RAAList:\n#     print(item)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/25candidateConcepts_\" + \"Forum\"+\".txt\"\nForumList = loadConceptList(filePath)\n# print(\"ForumListLen: \"+str(len(ForumList)))\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/25candidateConcepts_\" + \"EMS\"+\".txt\"\nEMSList = loadConceptList(filePath)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/25candidateConcepts_\" + \"I2B2\"+\".txt\"",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/25candidateConcepts_\" + \"Forum\"+\".txt\"\nForumList = loadConceptList(filePath)\n# print(\"ForumListLen: \"+str(len(ForumList)))\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/25candidateConcepts_\" + \"EMS\"+\".txt\"\nEMSList = loadConceptList(filePath)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/25candidateConcepts_\" + \"I2B2\"+\".txt\"\nI2B2List = loadConceptList(filePath)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/25candidateConcepts_\" + \"MIMIC3\"+\".txt\"\nMIMIC3List = loadConceptList(filePath)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/25candidateConcepts_\" + \"Med\"+\".txt\"",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "ForumList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "ForumList = loadConceptList(filePath)\n# print(\"ForumListLen: \"+str(len(ForumList)))\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/25candidateConcepts_\" + \"EMS\"+\".txt\"\nEMSList = loadConceptList(filePath)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/25candidateConcepts_\" + \"I2B2\"+\".txt\"\nI2B2List = loadConceptList(filePath)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/25candidateConcepts_\" + \"MIMIC3\"+\".txt\"\nMIMIC3List = loadConceptList(filePath)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/25candidateConcepts_\" + \"Med\"+\".txt\"\nMedList = loadConceptList(filePath)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/25candidateConcepts_\" + \"EMS\"+\".txt\"\nEMSList = loadConceptList(filePath)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/25candidateConcepts_\" + \"I2B2\"+\".txt\"\nI2B2List = loadConceptList(filePath)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/25candidateConcepts_\" + \"MIMIC3\"+\".txt\"\nMIMIC3List = loadConceptList(filePath)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/25candidateConcepts_\" + \"Med\"+\".txt\"\nMedList = loadConceptList(filePath)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/25candidateConcepts_\" + \"MedEMS\"+\".txt\"\nMedEMSList = loadConceptList(filePath)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "EMSList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "EMSList = loadConceptList(filePath)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/25candidateConcepts_\" + \"I2B2\"+\".txt\"\nI2B2List = loadConceptList(filePath)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/25candidateConcepts_\" + \"MIMIC3\"+\".txt\"\nMIMIC3List = loadConceptList(filePath)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/25candidateConcepts_\" + \"Med\"+\".txt\"\nMedList = loadConceptList(filePath)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/25candidateConcepts_\" + \"MedEMS\"+\".txt\"\nMedEMSList = loadConceptList(filePath)\n# print(\"MedEMSListLen: \"+str(len(MedEMSList)))",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/25candidateConcepts_\" + \"I2B2\"+\".txt\"\nI2B2List = loadConceptList(filePath)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/25candidateConcepts_\" + \"MIMIC3\"+\".txt\"\nMIMIC3List = loadConceptList(filePath)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/25candidateConcepts_\" + \"Med\"+\".txt\"\nMedList = loadConceptList(filePath)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/25candidateConcepts_\" + \"MedEMS\"+\".txt\"\nMedEMSList = loadConceptList(filePath)\n# print(\"MedEMSListLen: \"+str(len(MedEMSList)))\n# googleList, googlePList  = loadConceptList(filePath,\"google\")",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "I2B2List",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "I2B2List = loadConceptList(filePath)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/25candidateConcepts_\" + \"MIMIC3\"+\".txt\"\nMIMIC3List = loadConceptList(filePath)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/25candidateConcepts_\" + \"Med\"+\".txt\"\nMedList = loadConceptList(filePath)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/25candidateConcepts_\" + \"MedEMS\"+\".txt\"\nMedEMSList = loadConceptList(filePath)\n# print(\"MedEMSListLen: \"+str(len(MedEMSList)))\n# googleList, googlePList  = loadConceptList(filePath,\"google\")\n# filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/metaMapExtendedConceptListFiltered.txt\"",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/25candidateConcepts_\" + \"MIMIC3\"+\".txt\"\nMIMIC3List = loadConceptList(filePath)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/25candidateConcepts_\" + \"Med\"+\".txt\"\nMedList = loadConceptList(filePath)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/25candidateConcepts_\" + \"MedEMS\"+\".txt\"\nMedEMSList = loadConceptList(filePath)\n# print(\"MedEMSListLen: \"+str(len(MedEMSList)))\n# googleList, googlePList  = loadConceptList(filePath,\"google\")\n# filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/metaMapExtendedConceptListFiltered.txt\"\n##w2vFrequencyList = computeFrequency (gtConceptList, frequencyList, w2vList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "MIMIC3List",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "MIMIC3List = loadConceptList(filePath)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/25candidateConcepts_\" + \"Med\"+\".txt\"\nMedList = loadConceptList(filePath)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/25candidateConcepts_\" + \"MedEMS\"+\".txt\"\nMedEMSList = loadConceptList(filePath)\n# print(\"MedEMSListLen: \"+str(len(MedEMSList)))\n# googleList, googlePList  = loadConceptList(filePath,\"google\")\n# filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/metaMapExtendedConceptListFiltered.txt\"\n##w2vFrequencyList = computeFrequency (gtConceptList, frequencyList, w2vList)\n##d2vFrequencyList = computeFrequency (gtConceptList, frequencyList, d2vList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/25candidateConcepts_\" + \"Med\"+\".txt\"\nMedList = loadConceptList(filePath)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/25candidateConcepts_\" + \"MedEMS\"+\".txt\"\nMedEMSList = loadConceptList(filePath)\n# print(\"MedEMSListLen: \"+str(len(MedEMSList)))\n# googleList, googlePList  = loadConceptList(filePath,\"google\")\n# filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/metaMapExtendedConceptListFiltered.txt\"\n##w2vFrequencyList = computeFrequency (gtConceptList, frequencyList, w2vList)\n##d2vFrequencyList = computeFrequency (gtConceptList, frequencyList, d2vList)\nprint(\"RAAFrequencyList\")",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "MedList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "MedList = loadConceptList(filePath)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/25candidateConcepts_\" + \"MedEMS\"+\".txt\"\nMedEMSList = loadConceptList(filePath)\n# print(\"MedEMSListLen: \"+str(len(MedEMSList)))\n# googleList, googlePList  = loadConceptList(filePath,\"google\")\n# filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/metaMapExtendedConceptListFiltered.txt\"\n##w2vFrequencyList = computeFrequency (gtConceptList, frequencyList, w2vList)\n##d2vFrequencyList = computeFrequency (gtConceptList, frequencyList, d2vList)\nprint(\"RAAFrequencyList\")\n# print(\"===================================================================\")",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/25candidateConcepts_\" + \"MedEMS\"+\".txt\"\nMedEMSList = loadConceptList(filePath)\n# print(\"MedEMSListLen: \"+str(len(MedEMSList)))\n# googleList, googlePList  = loadConceptList(filePath,\"google\")\n# filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/metaMapExtendedConceptListFiltered.txt\"\n##w2vFrequencyList = computeFrequency (gtConceptList, frequencyList, w2vList)\n##d2vFrequencyList = computeFrequency (gtConceptList, frequencyList, d2vList)\nprint(\"RAAFrequencyList\")\n# print(\"===================================================================\")\nRAAFrequencyList = computeFrequency (gtConceptList, frequencyList, RAAList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "MedEMSList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "MedEMSList = loadConceptList(filePath)\n# print(\"MedEMSListLen: \"+str(len(MedEMSList)))\n# googleList, googlePList  = loadConceptList(filePath,\"google\")\n# filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/metaMapExtendedConceptListFiltered.txt\"\n##w2vFrequencyList = computeFrequency (gtConceptList, frequencyList, w2vList)\n##d2vFrequencyList = computeFrequency (gtConceptList, frequencyList, d2vList)\nprint(\"RAAFrequencyList\")\n# print(\"===================================================================\")\nRAAFrequencyList = computeFrequency (gtConceptList, frequencyList, RAAList)\n# print(\"===================================================================\")",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "##w2vFrequencyList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "##w2vFrequencyList = computeFrequency (gtConceptList, frequencyList, w2vList)\n##d2vFrequencyList = computeFrequency (gtConceptList, frequencyList, d2vList)\nprint(\"RAAFrequencyList\")\n# print(\"===================================================================\")\nRAAFrequencyList = computeFrequency (gtConceptList, frequencyList, RAAList)\n# print(\"===================================================================\")\n# print(\"ForumFrequencyList\")\nForumFrequencyList = computeFrequency (gtConceptList, frequencyList, ForumList)\nprint(\"===================================================================\")\nprint(\"EMSFrequencyList\")",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "##d2vFrequencyList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "##d2vFrequencyList = computeFrequency (gtConceptList, frequencyList, d2vList)\nprint(\"RAAFrequencyList\")\n# print(\"===================================================================\")\nRAAFrequencyList = computeFrequency (gtConceptList, frequencyList, RAAList)\n# print(\"===================================================================\")\n# print(\"ForumFrequencyList\")\nForumFrequencyList = computeFrequency (gtConceptList, frequencyList, ForumList)\nprint(\"===================================================================\")\nprint(\"EMSFrequencyList\")\nEMSFrequencyList = computeFrequency (gtConceptList, frequencyList, EMSList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "RAAFrequencyList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "RAAFrequencyList = computeFrequency (gtConceptList, frequencyList, RAAList)\n# print(\"===================================================================\")\n# print(\"ForumFrequencyList\")\nForumFrequencyList = computeFrequency (gtConceptList, frequencyList, ForumList)\nprint(\"===================================================================\")\nprint(\"EMSFrequencyList\")\nEMSFrequencyList = computeFrequency (gtConceptList, frequencyList, EMSList)\n# print(\"===================================================================\")\n# print(\"I2B2FrequencyList\")\nI2B2FrequencyList = computeFrequency (gtConceptList, frequencyList, I2B2List)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "ForumFrequencyList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "ForumFrequencyList = computeFrequency (gtConceptList, frequencyList, ForumList)\nprint(\"===================================================================\")\nprint(\"EMSFrequencyList\")\nEMSFrequencyList = computeFrequency (gtConceptList, frequencyList, EMSList)\n# print(\"===================================================================\")\n# print(\"I2B2FrequencyList\")\nI2B2FrequencyList = computeFrequency (gtConceptList, frequencyList, I2B2List)\n# print(\"===================================================================\")\n# print(\"MIMIC3FrequencyList\")\nMIMIC3FrequencyList = computeFrequency (gtConceptList, frequencyList, MIMIC3List)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "EMSFrequencyList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "EMSFrequencyList = computeFrequency (gtConceptList, frequencyList, EMSList)\n# print(\"===================================================================\")\n# print(\"I2B2FrequencyList\")\nI2B2FrequencyList = computeFrequency (gtConceptList, frequencyList, I2B2List)\n# print(\"===================================================================\")\n# print(\"MIMIC3FrequencyList\")\nMIMIC3FrequencyList = computeFrequency (gtConceptList, frequencyList, MIMIC3List)\nprint(\"===================================================================\")\nprint(\"MedFrequencyList\")\nMedFrequencyList = computeFrequency (gtConceptList, frequencyList, MedList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "I2B2FrequencyList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "I2B2FrequencyList = computeFrequency (gtConceptList, frequencyList, I2B2List)\n# print(\"===================================================================\")\n# print(\"MIMIC3FrequencyList\")\nMIMIC3FrequencyList = computeFrequency (gtConceptList, frequencyList, MIMIC3List)\nprint(\"===================================================================\")\nprint(\"MedFrequencyList\")\nMedFrequencyList = computeFrequency (gtConceptList, frequencyList, MedList)\nprint(\"===================================================================\")\nprint(\"MedEMSFrequencyList\")\nMedEMSFrequencyList = computeFrequency (gtConceptList, frequencyList, MedEMSList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "MIMIC3FrequencyList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "MIMIC3FrequencyList = computeFrequency (gtConceptList, frequencyList, MIMIC3List)\nprint(\"===================================================================\")\nprint(\"MedFrequencyList\")\nMedFrequencyList = computeFrequency (gtConceptList, frequencyList, MedList)\nprint(\"===================================================================\")\nprint(\"MedEMSFrequencyList\")\nMedEMSFrequencyList = computeFrequency (gtConceptList, frequencyList, MedEMSList)\nprint(\"===================================================================\")\n# googleFrequencyList = computeFrequency (gtConceptList, frequencyList, googleList)\ndenom = sum(frequencyList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "MedFrequencyList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "MedFrequencyList = computeFrequency (gtConceptList, frequencyList, MedList)\nprint(\"===================================================================\")\nprint(\"MedEMSFrequencyList\")\nMedEMSFrequencyList = computeFrequency (gtConceptList, frequencyList, MedEMSList)\nprint(\"===================================================================\")\n# googleFrequencyList = computeFrequency (gtConceptList, frequencyList, googleList)\ndenom = sum(frequencyList)\n# print(\"denom\"+str(denom))\n##finalList = list(zip(gtConceptList, frequencyList, w2vFrequencyList, d2vFrequencyList, i2b2FrequencyList, googleFrequencyList, mmFrequencyList))\n##for item in finalList:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "MedEMSFrequencyList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "MedEMSFrequencyList = computeFrequency (gtConceptList, frequencyList, MedEMSList)\nprint(\"===================================================================\")\n# googleFrequencyList = computeFrequency (gtConceptList, frequencyList, googleList)\ndenom = sum(frequencyList)\n# print(\"denom\"+str(denom))\n##finalList = list(zip(gtConceptList, frequencyList, w2vFrequencyList, d2vFrequencyList, i2b2FrequencyList, googleFrequencyList, mmFrequencyList))\n##for item in finalList:\n##    if item[1] > 0:\n##        print(str(item[0]) + \" \" + str(item[2]) + \" \" + str(item[3]) + \" \" + str(item[4]) + \" \" + str(item[5]) + \" \" + str(item[6]))\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData_new.txt\"",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "denom",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "denom = sum(frequencyList)\n# print(\"denom\"+str(denom))\n##finalList = list(zip(gtConceptList, frequencyList, w2vFrequencyList, d2vFrequencyList, i2b2FrequencyList, googleFrequencyList, mmFrequencyList))\n##for item in finalList:\n##    if item[1] > 0:\n##        print(str(item[0]) + \" \" + str(item[2]) + \" \" + str(item[3]) + \" \" + str(item[4]) + \" \" + str(item[5]) + \" \" + str(item[6]))\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData_new.txt\"\n##w2vFPList = computeFP(filePath, w2vList, gtConceptList)\n##d2vFPList = computeFP(filePath, d2vList, gtConceptList)\nRAAFPList = computeFP(filePath, RAAList, gtConceptList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "##finalList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "##finalList = list(zip(gtConceptList, frequencyList, w2vFrequencyList, d2vFrequencyList, i2b2FrequencyList, googleFrequencyList, mmFrequencyList))\n##for item in finalList:\n##    if item[1] > 0:\n##        print(str(item[0]) + \" \" + str(item[2]) + \" \" + str(item[3]) + \" \" + str(item[4]) + \" \" + str(item[5]) + \" \" + str(item[6]))\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData_new.txt\"\n##w2vFPList = computeFP(filePath, w2vList, gtConceptList)\n##d2vFPList = computeFP(filePath, d2vList, gtConceptList)\nRAAFPList = computeFP(filePath, RAAList, gtConceptList)\nForumFPList = computeFP(filePath, ForumList, gtConceptList)\nEMSFPList = computeFP(filePath, EMSList, gtConceptList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData_new.txt\"\n##w2vFPList = computeFP(filePath, w2vList, gtConceptList)\n##d2vFPList = computeFP(filePath, d2vList, gtConceptList)\nRAAFPList = computeFP(filePath, RAAList, gtConceptList)\nForumFPList = computeFP(filePath, ForumList, gtConceptList)\nEMSFPList = computeFP(filePath, EMSList, gtConceptList)\nI2B2FPList = computeFP(filePath, I2B2List, gtConceptList)\nMIMIC3FPList = computeFP(filePath, MIMIC3List, gtConceptList)\nMedFPList = computeFP(filePath, MedList, gtConceptList)\nMedEMSFPList = computeFP(filePath, MedEMSList, gtConceptList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "##w2vFPList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "##w2vFPList = computeFP(filePath, w2vList, gtConceptList)\n##d2vFPList = computeFP(filePath, d2vList, gtConceptList)\nRAAFPList = computeFP(filePath, RAAList, gtConceptList)\nForumFPList = computeFP(filePath, ForumList, gtConceptList)\nEMSFPList = computeFP(filePath, EMSList, gtConceptList)\nI2B2FPList = computeFP(filePath, I2B2List, gtConceptList)\nMIMIC3FPList = computeFP(filePath, MIMIC3List, gtConceptList)\nMedFPList = computeFP(filePath, MedList, gtConceptList)\nMedEMSFPList = computeFP(filePath, MedEMSList, gtConceptList)\n# googleFPList = computeFP(filePath, googleList, gtConceptList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "##d2vFPList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "##d2vFPList = computeFP(filePath, d2vList, gtConceptList)\nRAAFPList = computeFP(filePath, RAAList, gtConceptList)\nForumFPList = computeFP(filePath, ForumList, gtConceptList)\nEMSFPList = computeFP(filePath, EMSList, gtConceptList)\nI2B2FPList = computeFP(filePath, I2B2List, gtConceptList)\nMIMIC3FPList = computeFP(filePath, MIMIC3List, gtConceptList)\nMedFPList = computeFP(filePath, MedList, gtConceptList)\nMedEMSFPList = computeFP(filePath, MedEMSList, gtConceptList)\n# googleFPList = computeFP(filePath, googleList, gtConceptList)\n##print(sum(frequencyList), sum(w2vFrequencyList), sum(d2vFrequencyList), sum(i2b2FrequencyList), sum(googleFrequencyList), sum(mmFrequencyList), sum(mmFilteredFrequencyList))",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "RAAFPList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "RAAFPList = computeFP(filePath, RAAList, gtConceptList)\nForumFPList = computeFP(filePath, ForumList, gtConceptList)\nEMSFPList = computeFP(filePath, EMSList, gtConceptList)\nI2B2FPList = computeFP(filePath, I2B2List, gtConceptList)\nMIMIC3FPList = computeFP(filePath, MIMIC3List, gtConceptList)\nMedFPList = computeFP(filePath, MedList, gtConceptList)\nMedEMSFPList = computeFP(filePath, MedEMSList, gtConceptList)\n# googleFPList = computeFP(filePath, googleList, gtConceptList)\n##print(sum(frequencyList), sum(w2vFrequencyList), sum(d2vFrequencyList), sum(i2b2FrequencyList), sum(googleFrequencyList), sum(mmFrequencyList), sum(mmFilteredFrequencyList))\n##print(sum(w2vFPList), sum(d2vFPList), sum(i2b2FPList), sum(googleFPList), sum(mmFPList), sum(mmFilteredFPList))",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "ForumFPList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "ForumFPList = computeFP(filePath, ForumList, gtConceptList)\nEMSFPList = computeFP(filePath, EMSList, gtConceptList)\nI2B2FPList = computeFP(filePath, I2B2List, gtConceptList)\nMIMIC3FPList = computeFP(filePath, MIMIC3List, gtConceptList)\nMedFPList = computeFP(filePath, MedList, gtConceptList)\nMedEMSFPList = computeFP(filePath, MedEMSList, gtConceptList)\n# googleFPList = computeFP(filePath, googleList, gtConceptList)\n##print(sum(frequencyList), sum(w2vFrequencyList), sum(d2vFrequencyList), sum(i2b2FrequencyList), sum(googleFrequencyList), sum(mmFrequencyList), sum(mmFilteredFrequencyList))\n##print(sum(w2vFPList), sum(d2vFPList), sum(i2b2FPList), sum(googleFPList), sum(mmFPList), sum(mmFilteredFPList))\nRAAr = computeRecall(RAAFrequencyList,denom)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "EMSFPList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "EMSFPList = computeFP(filePath, EMSList, gtConceptList)\nI2B2FPList = computeFP(filePath, I2B2List, gtConceptList)\nMIMIC3FPList = computeFP(filePath, MIMIC3List, gtConceptList)\nMedFPList = computeFP(filePath, MedList, gtConceptList)\nMedEMSFPList = computeFP(filePath, MedEMSList, gtConceptList)\n# googleFPList = computeFP(filePath, googleList, gtConceptList)\n##print(sum(frequencyList), sum(w2vFrequencyList), sum(d2vFrequencyList), sum(i2b2FrequencyList), sum(googleFrequencyList), sum(mmFrequencyList), sum(mmFilteredFrequencyList))\n##print(sum(w2vFPList), sum(d2vFPList), sum(i2b2FPList), sum(googleFPList), sum(mmFPList), sum(mmFilteredFPList))\nRAAr = computeRecall(RAAFrequencyList,denom)\nForumr = computeRecall(ForumFrequencyList,denom)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "I2B2FPList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "I2B2FPList = computeFP(filePath, I2B2List, gtConceptList)\nMIMIC3FPList = computeFP(filePath, MIMIC3List, gtConceptList)\nMedFPList = computeFP(filePath, MedList, gtConceptList)\nMedEMSFPList = computeFP(filePath, MedEMSList, gtConceptList)\n# googleFPList = computeFP(filePath, googleList, gtConceptList)\n##print(sum(frequencyList), sum(w2vFrequencyList), sum(d2vFrequencyList), sum(i2b2FrequencyList), sum(googleFrequencyList), sum(mmFrequencyList), sum(mmFilteredFrequencyList))\n##print(sum(w2vFPList), sum(d2vFPList), sum(i2b2FPList), sum(googleFPList), sum(mmFPList), sum(mmFilteredFPList))\nRAAr = computeRecall(RAAFrequencyList,denom)\nForumr = computeRecall(ForumFrequencyList,denom)\nEMSr = computeRecall(EMSFrequencyList,denom)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "MIMIC3FPList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "MIMIC3FPList = computeFP(filePath, MIMIC3List, gtConceptList)\nMedFPList = computeFP(filePath, MedList, gtConceptList)\nMedEMSFPList = computeFP(filePath, MedEMSList, gtConceptList)\n# googleFPList = computeFP(filePath, googleList, gtConceptList)\n##print(sum(frequencyList), sum(w2vFrequencyList), sum(d2vFrequencyList), sum(i2b2FrequencyList), sum(googleFrequencyList), sum(mmFrequencyList), sum(mmFilteredFrequencyList))\n##print(sum(w2vFPList), sum(d2vFPList), sum(i2b2FPList), sum(googleFPList), sum(mmFPList), sum(mmFilteredFPList))\nRAAr = computeRecall(RAAFrequencyList,denom)\nForumr = computeRecall(ForumFrequencyList,denom)\nEMSr = computeRecall(EMSFrequencyList,denom)\nI2B2r = computeRecall(I2B2FrequencyList,denom)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "MedFPList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "MedFPList = computeFP(filePath, MedList, gtConceptList)\nMedEMSFPList = computeFP(filePath, MedEMSList, gtConceptList)\n# googleFPList = computeFP(filePath, googleList, gtConceptList)\n##print(sum(frequencyList), sum(w2vFrequencyList), sum(d2vFrequencyList), sum(i2b2FrequencyList), sum(googleFrequencyList), sum(mmFrequencyList), sum(mmFilteredFrequencyList))\n##print(sum(w2vFPList), sum(d2vFPList), sum(i2b2FPList), sum(googleFPList), sum(mmFPList), sum(mmFilteredFPList))\nRAAr = computeRecall(RAAFrequencyList,denom)\nForumr = computeRecall(ForumFrequencyList,denom)\nEMSr = computeRecall(EMSFrequencyList,denom)\nI2B2r = computeRecall(I2B2FrequencyList,denom)\nMIMIC3r  = computeRecall(MIMIC3FrequencyList,denom)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "MedEMSFPList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "MedEMSFPList = computeFP(filePath, MedEMSList, gtConceptList)\n# googleFPList = computeFP(filePath, googleList, gtConceptList)\n##print(sum(frequencyList), sum(w2vFrequencyList), sum(d2vFrequencyList), sum(i2b2FrequencyList), sum(googleFrequencyList), sum(mmFrequencyList), sum(mmFilteredFrequencyList))\n##print(sum(w2vFPList), sum(d2vFPList), sum(i2b2FPList), sum(googleFPList), sum(mmFPList), sum(mmFilteredFPList))\nRAAr = computeRecall(RAAFrequencyList,denom)\nForumr = computeRecall(ForumFrequencyList,denom)\nEMSr = computeRecall(EMSFrequencyList,denom)\nI2B2r = computeRecall(I2B2FrequencyList,denom)\nMIMIC3r  = computeRecall(MIMIC3FrequencyList,denom)\nMedr =  computeRecall(MedFrequencyList,denom)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "RAAr",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "RAAr = computeRecall(RAAFrequencyList,denom)\nForumr = computeRecall(ForumFrequencyList,denom)\nEMSr = computeRecall(EMSFrequencyList,denom)\nI2B2r = computeRecall(I2B2FrequencyList,denom)\nMIMIC3r  = computeRecall(MIMIC3FrequencyList,denom)\nMedr =  computeRecall(MedFrequencyList,denom)\nMedEMSr = computeRecall(MedEMSFrequencyList, denom)\nRAAp = computePrecision(RAAFrequencyList, RAAFPList)\nForump = computePrecision(ForumFrequencyList, ForumFPList)\nEMSp = computePrecision(EMSFrequencyList, EMSFPList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "Forumr",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "Forumr = computeRecall(ForumFrequencyList,denom)\nEMSr = computeRecall(EMSFrequencyList,denom)\nI2B2r = computeRecall(I2B2FrequencyList,denom)\nMIMIC3r  = computeRecall(MIMIC3FrequencyList,denom)\nMedr =  computeRecall(MedFrequencyList,denom)\nMedEMSr = computeRecall(MedEMSFrequencyList, denom)\nRAAp = computePrecision(RAAFrequencyList, RAAFPList)\nForump = computePrecision(ForumFrequencyList, ForumFPList)\nEMSp = computePrecision(EMSFrequencyList, EMSFPList)\nI2B2p = computePrecision(I2B2FrequencyList, I2B2FPList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "EMSr",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "EMSr = computeRecall(EMSFrequencyList,denom)\nI2B2r = computeRecall(I2B2FrequencyList,denom)\nMIMIC3r  = computeRecall(MIMIC3FrequencyList,denom)\nMedr =  computeRecall(MedFrequencyList,denom)\nMedEMSr = computeRecall(MedEMSFrequencyList, denom)\nRAAp = computePrecision(RAAFrequencyList, RAAFPList)\nForump = computePrecision(ForumFrequencyList, ForumFPList)\nEMSp = computePrecision(EMSFrequencyList, EMSFPList)\nI2B2p = computePrecision(I2B2FrequencyList, I2B2FPList)\nMIMIC3p = computePrecision(MIMIC3FrequencyList, MIMIC3FPList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "I2B2r",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "I2B2r = computeRecall(I2B2FrequencyList,denom)\nMIMIC3r  = computeRecall(MIMIC3FrequencyList,denom)\nMedr =  computeRecall(MedFrequencyList,denom)\nMedEMSr = computeRecall(MedEMSFrequencyList, denom)\nRAAp = computePrecision(RAAFrequencyList, RAAFPList)\nForump = computePrecision(ForumFrequencyList, ForumFPList)\nEMSp = computePrecision(EMSFrequencyList, EMSFPList)\nI2B2p = computePrecision(I2B2FrequencyList, I2B2FPList)\nMIMIC3p = computePrecision(MIMIC3FrequencyList, MIMIC3FPList)\nMedp = computePrecision(MedFrequencyList, MedFPList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "Medr",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "Medr =  computeRecall(MedFrequencyList,denom)\nMedEMSr = computeRecall(MedEMSFrequencyList, denom)\nRAAp = computePrecision(RAAFrequencyList, RAAFPList)\nForump = computePrecision(ForumFrequencyList, ForumFPList)\nEMSp = computePrecision(EMSFrequencyList, EMSFPList)\nI2B2p = computePrecision(I2B2FrequencyList, I2B2FPList)\nMIMIC3p = computePrecision(MIMIC3FrequencyList, MIMIC3FPList)\nMedp = computePrecision(MedFrequencyList, MedFPList)\nMedEMSp = computePrecision(MedEMSFrequencyList, MedEMSFPList)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/RecallPrecision/Results_wordEmbedding.txt\"",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "MedEMSr",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "MedEMSr = computeRecall(MedEMSFrequencyList, denom)\nRAAp = computePrecision(RAAFrequencyList, RAAFPList)\nForump = computePrecision(ForumFrequencyList, ForumFPList)\nEMSp = computePrecision(EMSFrequencyList, EMSFPList)\nI2B2p = computePrecision(I2B2FrequencyList, I2B2FPList)\nMIMIC3p = computePrecision(MIMIC3FrequencyList, MIMIC3FPList)\nMedp = computePrecision(MedFrequencyList, MedFPList)\nMedEMSp = computePrecision(MedEMSFrequencyList, MedEMSFPList)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/RecallPrecision/Results_wordEmbedding.txt\"\nwfile = open (filePath, \"w\")",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "RAAp",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "RAAp = computePrecision(RAAFrequencyList, RAAFPList)\nForump = computePrecision(ForumFrequencyList, ForumFPList)\nEMSp = computePrecision(EMSFrequencyList, EMSFPList)\nI2B2p = computePrecision(I2B2FrequencyList, I2B2FPList)\nMIMIC3p = computePrecision(MIMIC3FrequencyList, MIMIC3FPList)\nMedp = computePrecision(MedFrequencyList, MedFPList)\nMedEMSp = computePrecision(MedEMSFrequencyList, MedEMSFPList)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/RecallPrecision/Results_wordEmbedding.txt\"\nwfile = open (filePath, \"w\")\n##wfile.write(str(sum(w2vFrequencyList))+ \"\\t\" + str(sum(d2vFrequencyList))+ \"\\t\" + str(sum(i2b2FrequencyList))+ \"\\t\" + str(sum(googleFrequencyList))+ \"\\t\" +  str(sum(mmFrequencyList)) +\"\\n\")",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "Forump",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "Forump = computePrecision(ForumFrequencyList, ForumFPList)\nEMSp = computePrecision(EMSFrequencyList, EMSFPList)\nI2B2p = computePrecision(I2B2FrequencyList, I2B2FPList)\nMIMIC3p = computePrecision(MIMIC3FrequencyList, MIMIC3FPList)\nMedp = computePrecision(MedFrequencyList, MedFPList)\nMedEMSp = computePrecision(MedEMSFrequencyList, MedEMSFPList)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/RecallPrecision/Results_wordEmbedding.txt\"\nwfile = open (filePath, \"w\")\n##wfile.write(str(sum(w2vFrequencyList))+ \"\\t\" + str(sum(d2vFrequencyList))+ \"\\t\" + str(sum(i2b2FrequencyList))+ \"\\t\" + str(sum(googleFrequencyList))+ \"\\t\" +  str(sum(mmFrequencyList)) +\"\\n\")\n##wfile.write(str(sum(w2vFPList))+ \"\\t\" + str(sum(d2vFPList))+ \"\\t\" + str(sum(i2b2FPList))+ \"\\t\" + str(sum(googleFPList))+ \"\\t\" +  str(sum(mmFPList)) +\"\\n\")",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "EMSp",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "EMSp = computePrecision(EMSFrequencyList, EMSFPList)\nI2B2p = computePrecision(I2B2FrequencyList, I2B2FPList)\nMIMIC3p = computePrecision(MIMIC3FrequencyList, MIMIC3FPList)\nMedp = computePrecision(MedFrequencyList, MedFPList)\nMedEMSp = computePrecision(MedEMSFrequencyList, MedEMSFPList)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/RecallPrecision/Results_wordEmbedding.txt\"\nwfile = open (filePath, \"w\")\n##wfile.write(str(sum(w2vFrequencyList))+ \"\\t\" + str(sum(d2vFrequencyList))+ \"\\t\" + str(sum(i2b2FrequencyList))+ \"\\t\" + str(sum(googleFrequencyList))+ \"\\t\" +  str(sum(mmFrequencyList)) +\"\\n\")\n##wfile.write(str(sum(w2vFPList))+ \"\\t\" + str(sum(d2vFPList))+ \"\\t\" + str(sum(i2b2FPList))+ \"\\t\" + str(sum(googleFPList))+ \"\\t\" +  str(sum(mmFPList)) +\"\\n\")\n##wfile.write(\"ModelName\\tWord2Vec\\tDoc2Vec\\tI2B2\\tGoogle\\tMMoutput\\tMMoutputFiltST\\tMM_ManualList\\tMMList\\tMMListFiltST\\n\")",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "I2B2p",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "I2B2p = computePrecision(I2B2FrequencyList, I2B2FPList)\nMIMIC3p = computePrecision(MIMIC3FrequencyList, MIMIC3FPList)\nMedp = computePrecision(MedFrequencyList, MedFPList)\nMedEMSp = computePrecision(MedEMSFrequencyList, MedEMSFPList)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/RecallPrecision/Results_wordEmbedding.txt\"\nwfile = open (filePath, \"w\")\n##wfile.write(str(sum(w2vFrequencyList))+ \"\\t\" + str(sum(d2vFrequencyList))+ \"\\t\" + str(sum(i2b2FrequencyList))+ \"\\t\" + str(sum(googleFrequencyList))+ \"\\t\" +  str(sum(mmFrequencyList)) +\"\\n\")\n##wfile.write(str(sum(w2vFPList))+ \"\\t\" + str(sum(d2vFPList))+ \"\\t\" + str(sum(i2b2FPList))+ \"\\t\" + str(sum(googleFPList))+ \"\\t\" +  str(sum(mmFPList)) +\"\\n\")\n##wfile.write(\"ModelName\\tWord2Vec\\tDoc2Vec\\tI2B2\\tGoogle\\tMMoutput\\tMMoutputFiltST\\tMM_ManualList\\tMMList\\tMMListFiltST\\n\")\nwfile.write(\"ModelName\\tRAA\\tForum\\tEMS\\tI2B2\\tMIMIC3\\tMed\\tMedEMS\\n\")",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "MIMIC3p",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "MIMIC3p = computePrecision(MIMIC3FrequencyList, MIMIC3FPList)\nMedp = computePrecision(MedFrequencyList, MedFPList)\nMedEMSp = computePrecision(MedEMSFrequencyList, MedEMSFPList)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/RecallPrecision/Results_wordEmbedding.txt\"\nwfile = open (filePath, \"w\")\n##wfile.write(str(sum(w2vFrequencyList))+ \"\\t\" + str(sum(d2vFrequencyList))+ \"\\t\" + str(sum(i2b2FrequencyList))+ \"\\t\" + str(sum(googleFrequencyList))+ \"\\t\" +  str(sum(mmFrequencyList)) +\"\\n\")\n##wfile.write(str(sum(w2vFPList))+ \"\\t\" + str(sum(d2vFPList))+ \"\\t\" + str(sum(i2b2FPList))+ \"\\t\" + str(sum(googleFPList))+ \"\\t\" +  str(sum(mmFPList)) +\"\\n\")\n##wfile.write(\"ModelName\\tWord2Vec\\tDoc2Vec\\tI2B2\\tGoogle\\tMMoutput\\tMMoutputFiltST\\tMM_ManualList\\tMMList\\tMMListFiltST\\n\")\nwfile.write(\"ModelName\\tRAA\\tForum\\tEMS\\tI2B2\\tMIMIC3\\tMed\\tMedEMS\\n\")\nwfile.write(\"Recall\\t\"+str(RAAr)+\"\\t\"+str(Forumr)+\"\\t\"+str(EMSr)+\"\\t\"+str(I2B2r)+\"\\t\"+str(MIMIC3r)+\"\\t\"+str(Medr)+\"\\t\"+str(MedEMSr)+\"\\n\")",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "Medp",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "Medp = computePrecision(MedFrequencyList, MedFPList)\nMedEMSp = computePrecision(MedEMSFrequencyList, MedEMSFPList)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/RecallPrecision/Results_wordEmbedding.txt\"\nwfile = open (filePath, \"w\")\n##wfile.write(str(sum(w2vFrequencyList))+ \"\\t\" + str(sum(d2vFrequencyList))+ \"\\t\" + str(sum(i2b2FrequencyList))+ \"\\t\" + str(sum(googleFrequencyList))+ \"\\t\" +  str(sum(mmFrequencyList)) +\"\\n\")\n##wfile.write(str(sum(w2vFPList))+ \"\\t\" + str(sum(d2vFPList))+ \"\\t\" + str(sum(i2b2FPList))+ \"\\t\" + str(sum(googleFPList))+ \"\\t\" +  str(sum(mmFPList)) +\"\\n\")\n##wfile.write(\"ModelName\\tWord2Vec\\tDoc2Vec\\tI2B2\\tGoogle\\tMMoutput\\tMMoutputFiltST\\tMM_ManualList\\tMMList\\tMMListFiltST\\n\")\nwfile.write(\"ModelName\\tRAA\\tForum\\tEMS\\tI2B2\\tMIMIC3\\tMed\\tMedEMS\\n\")\nwfile.write(\"Recall\\t\"+str(RAAr)+\"\\t\"+str(Forumr)+\"\\t\"+str(EMSr)+\"\\t\"+str(I2B2r)+\"\\t\"+str(MIMIC3r)+\"\\t\"+str(Medr)+\"\\t\"+str(MedEMSr)+\"\\n\")\nwfile.write(\"Precision\\t\"+str(RAAp)+\"\\t\"+str(Forump)+\"\\t\"+str(EMSp)+\"\\t\"+str(I2B2p)+\"\\t\"+str(MIMIC3p)+\"\\t\"+str(Medp)+\"\\t\"+str(MedEMSp)+\"\\n\")",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "MedEMSp",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "MedEMSp = computePrecision(MedEMSFrequencyList, MedEMSFPList)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/RecallPrecision/Results_wordEmbedding.txt\"\nwfile = open (filePath, \"w\")\n##wfile.write(str(sum(w2vFrequencyList))+ \"\\t\" + str(sum(d2vFrequencyList))+ \"\\t\" + str(sum(i2b2FrequencyList))+ \"\\t\" + str(sum(googleFrequencyList))+ \"\\t\" +  str(sum(mmFrequencyList)) +\"\\n\")\n##wfile.write(str(sum(w2vFPList))+ \"\\t\" + str(sum(d2vFPList))+ \"\\t\" + str(sum(i2b2FPList))+ \"\\t\" + str(sum(googleFPList))+ \"\\t\" +  str(sum(mmFPList)) +\"\\n\")\n##wfile.write(\"ModelName\\tWord2Vec\\tDoc2Vec\\tI2B2\\tGoogle\\tMMoutput\\tMMoutputFiltST\\tMM_ManualList\\tMMList\\tMMListFiltST\\n\")\nwfile.write(\"ModelName\\tRAA\\tForum\\tEMS\\tI2B2\\tMIMIC3\\tMed\\tMedEMS\\n\")\nwfile.write(\"Recall\\t\"+str(RAAr)+\"\\t\"+str(Forumr)+\"\\t\"+str(EMSr)+\"\\t\"+str(I2B2r)+\"\\t\"+str(MIMIC3r)+\"\\t\"+str(Medr)+\"\\t\"+str(MedEMSr)+\"\\n\")\nwfile.write(\"Precision\\t\"+str(RAAp)+\"\\t\"+str(Forump)+\"\\t\"+str(EMSp)+\"\\t\"+str(I2B2p)+\"\\t\"+str(MIMIC3p)+\"\\t\"+str(Medp)+\"\\t\"+str(MedEMSp)+\"\\n\")\nwfile.close()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/RecallPrecision/Results_wordEmbedding.txt\"\nwfile = open (filePath, \"w\")\n##wfile.write(str(sum(w2vFrequencyList))+ \"\\t\" + str(sum(d2vFrequencyList))+ \"\\t\" + str(sum(i2b2FrequencyList))+ \"\\t\" + str(sum(googleFrequencyList))+ \"\\t\" +  str(sum(mmFrequencyList)) +\"\\n\")\n##wfile.write(str(sum(w2vFPList))+ \"\\t\" + str(sum(d2vFPList))+ \"\\t\" + str(sum(i2b2FPList))+ \"\\t\" + str(sum(googleFPList))+ \"\\t\" +  str(sum(mmFPList)) +\"\\n\")\n##wfile.write(\"ModelName\\tWord2Vec\\tDoc2Vec\\tI2B2\\tGoogle\\tMMoutput\\tMMoutputFiltST\\tMM_ManualList\\tMMList\\tMMListFiltST\\n\")\nwfile.write(\"ModelName\\tRAA\\tForum\\tEMS\\tI2B2\\tMIMIC3\\tMed\\tMedEMS\\n\")\nwfile.write(\"Recall\\t\"+str(RAAr)+\"\\t\"+str(Forumr)+\"\\t\"+str(EMSr)+\"\\t\"+str(I2B2r)+\"\\t\"+str(MIMIC3r)+\"\\t\"+str(Medr)+\"\\t\"+str(MedEMSr)+\"\\n\")\nwfile.write(\"Precision\\t\"+str(RAAp)+\"\\t\"+str(Forump)+\"\\t\"+str(EMSp)+\"\\t\"+str(I2B2p)+\"\\t\"+str(MIMIC3p)+\"\\t\"+str(Medp)+\"\\t\"+str(MedEMSp)+\"\\n\")\nwfile.close()\n##wfile.write(\"Recall\\t\"+str(round(sum(w2vFrequencyList)/float(denom),4))+ \"\\t\" + str(round(sum(d2vFrequencyList)/float(denom),4))+ \"\\t\" + str(round(sum(i2b2FrequencyList)/float(denom),4))+ \"\\t\" + str(round(sum(googleFrequencyList)/float(denom),4))+\"\\t\"+ str(round(sum(mmFrequencyList)/float(denom),4))+ \"\\t\" + str(round(sum(mm_FiltST_FrequencyList)/float(denom),4))+ \"\\t\" +  str(round(sum(mmManFrequencyList)/float(denom),4)) + \"\\t\" +str(round(sum(mmExtConceptFrequencyList)/float(denom),4))+ \"\\t\" + str(round(sum(mm_FiltST_ExtConceptFrequencyList)/float(denom),4))+\"\\n\")",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "wfile",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "peekOfCode": "wfile = open (filePath, \"w\")\n##wfile.write(str(sum(w2vFrequencyList))+ \"\\t\" + str(sum(d2vFrequencyList))+ \"\\t\" + str(sum(i2b2FrequencyList))+ \"\\t\" + str(sum(googleFrequencyList))+ \"\\t\" +  str(sum(mmFrequencyList)) +\"\\n\")\n##wfile.write(str(sum(w2vFPList))+ \"\\t\" + str(sum(d2vFPList))+ \"\\t\" + str(sum(i2b2FPList))+ \"\\t\" + str(sum(googleFPList))+ \"\\t\" +  str(sum(mmFPList)) +\"\\n\")\n##wfile.write(\"ModelName\\tWord2Vec\\tDoc2Vec\\tI2B2\\tGoogle\\tMMoutput\\tMMoutputFiltST\\tMM_ManualList\\tMMList\\tMMListFiltST\\n\")\nwfile.write(\"ModelName\\tRAA\\tForum\\tEMS\\tI2B2\\tMIMIC3\\tMed\\tMedEMS\\n\")\nwfile.write(\"Recall\\t\"+str(RAAr)+\"\\t\"+str(Forumr)+\"\\t\"+str(EMSr)+\"\\t\"+str(I2B2r)+\"\\t\"+str(MIMIC3r)+\"\\t\"+str(Medr)+\"\\t\"+str(MedEMSr)+\"\\n\")\nwfile.write(\"Precision\\t\"+str(RAAp)+\"\\t\"+str(Forump)+\"\\t\"+str(EMSp)+\"\\t\"+str(I2B2p)+\"\\t\"+str(MIMIC3p)+\"\\t\"+str(Medp)+\"\\t\"+str(MedEMSp)+\"\\n\")\nwfile.close()\n##wfile.write(\"Recall\\t\"+str(round(sum(w2vFrequencyList)/float(denom),4))+ \"\\t\" + str(round(sum(d2vFrequencyList)/float(denom),4))+ \"\\t\" + str(round(sum(i2b2FrequencyList)/float(denom),4))+ \"\\t\" + str(round(sum(googleFrequencyList)/float(denom),4))+\"\\t\"+ str(round(sum(mmFrequencyList)/float(denom),4))+ \"\\t\" + str(round(sum(mm_FiltST_FrequencyList)/float(denom),4))+ \"\\t\" +  str(round(sum(mmManFrequencyList)/float(denom),4)) + \"\\t\" +str(round(sum(mmExtConceptFrequencyList)/float(denom),4))+ \"\\t\" + str(round(sum(mm_FiltST_ExtConceptFrequencyList)/float(denom),4))+\"\\n\")\n# wfile.write(\"Recall\\t\" + str(round(sum(i2b2FrequencyList)/float(denom),4))+ \"\\t\" + str(round(sum(googleFrequencyList)/float(denom),4))+\"\\t\"+ str(round(sum(mmFrequencyList)/float(denom),4))+ \"\\t\" + str(round(sum(mm_FiltST_FrequencyList)/float(denom),4))+ \"\\t\" +  str(round(sum(mmManFrequencyList)/float(denom),4)) + \"\\t\" +str(round(sum(mmExtConceptFrequencyList)/float(denom),4))+ \"\\t\" + str(round(sum(mm_FiltST_ExtConceptFrequencyList)/float(denom),4))+\"\\n\")",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.performance_baseLines_words_Oct2018",
        "documentation": {}
    },
    {
        "label": "hasNumbers",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "def hasNumbers(inputString):\n    return any(char.isdigit() for char in inputString)\ndef hasStopwords(inputString):\n    stops = loadStopWords()\n    tokens = inputString.split(\" \")\n    for item in stops:\n        if item in tokens:\n            return True\n    return False\ndef fileLength(fname):",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "hasStopwords",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "def hasStopwords(inputString):\n    stops = loadStopWords()\n    tokens = inputString.split(\" \")\n    for item in stops:\n        if item in tokens:\n            return True\n    return False\ndef fileLength(fname):\n    with open(fname) as f:\n        for i, l in enumerate(f):",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "fileLength",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "def fileLength(fname):\n    with open(fname) as f:\n        for i, l in enumerate(f):\n            pass\n    return i + 1\ndef loadTrainingData(filepath, n, remove_stopwords=True):\n    dataList = [[] for x in range(n)]\n    rfile = open(filepath, \"r\")\n    for line in rfile:\n        line = line.lower().strip()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "loadTrainingData",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "def loadTrainingData(filepath, n, remove_stopwords=True):\n    dataList = [[] for x in range(n)]\n    rfile = open(filepath, \"r\")\n    for line in rfile:\n        line = line.lower().strip()\n        line = re.sub(\"[^a-zA-Z0-9'-]\",\" \", line)\n        stops = loadStopWords()\n        innerList = line.split()\n        innerList = [w for w in innerList if not w in stops and not w.isdigit()]\n        dataList.append(innerList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "loadStringData",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "def loadStringData(filepath):\n##    dataList = list()\n##    dataList = [[] for x in range(11891)]\n    dataString = \"\"\n    rfile = open(filepath, \"r\")\n    for line in rfile:\n        line2 = line.lower().strip()\n        line2 = re.sub(\"[^a-zA-Z0-9_]\",\" \", line2)\n        dataString += \" \"+ line2\n    rfile.close()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "loadStringData2",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "def loadStringData2(filepath):\n##    dataList = list()\n##    dataList = [[] for x in range(11891)]\n    dataString = \"\"\n    rfile = open(filepath, \"r\")\n    for line in rfile:\n        line = line.lower()\n        dataString += line\n    rfile.close()\n    return dataString",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "loadStopWords",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "def loadStopWords():\n    filePath = \"/Users/sarahmasudpreum/Desktop/PythonTCode/general/stopwords.txt\"\n    rfile = open(filePath, \"r\")\n    stopWords = list()\n    for line in rfile:\n        stopWords.append(line.lower().strip())\n    rfile.close()\n    return stopWords\ndef loadOriginalConceptListPhrase(filepath):\n    conceptList = list()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "loadOriginalConceptListPhrase",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "def loadOriginalConceptListPhrase(filepath):\n    conceptList = list()\n    rfile = open(filepath, \"r\")\n    for line in rfile:\n        item = line.lower().strip()\n        if item not in conceptList:\n            conceptList.append(item)\n    rfile.close()\n    return conceptList\ndef loadOriginalConceptList(filepath):",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "loadOriginalConceptList",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "def loadOriginalConceptList(filepath):\n    conceptList = list()\n    rfile = open(filepath, \"r\")\n    for line in rfile:\n        concept = line.split(\"\\t\")[0].lower().strip()\n        items = concept.split(\" \")\n        for item in items: \n            if item not in conceptList:\n                conceptList.append(item)\n    stops = loadStopWords()##set(stopwords.words(\"english\"))",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "loadSimpleListPhras",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "def loadSimpleListPhrase (filePath):\n    rfile = open(filePath, \"r\")\n    itemList = list()\n##    table = str.maketrans({key: None for key in string.punctuation})\n    for line in rfile:\n##        line = re.sub(\"[^a-zA-Z0-9_]\",\" \", line)\n        item = line.lower().strip()\n        if item not in itemList:\n            itemList.append(item)\n    rfile.close()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "computeFrequenc",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "def computeFrequency (gtConceptList, frequencyList, wordList):\n    wordFrequencyList = list(repeat(0,len(gtConceptList)))\n    for item in wordList:\n        for i in range (0,len(gtConceptList)):\n            gTerm = str(gtConceptList[i]).lower().strip()\n            item = item.lower().strip()\n##            if len(gTerm.split(\" \")) > 1:\n            if item.find(gTerm) is not -1: #or gTerm.find(item) is not -1:\n                wordFrequencyList[i] = frequencyList[i]\n    # filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/May20/RecallPrecision/Results_W2V_CBOW_phrases_AvgDist_TP_Phrases.txt\"",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "computeF",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "def computeFP (filePath, conceptList, gtConceptList):\n##    filteredConceptList = [x.strip() for x in conceptList if x not in gtConceptList]\n    tpConceptList = list()\n    for item in conceptList:\n        for i in range (0,len(gtConceptList)):\n            gTerm = str(gtConceptList[i]).lower().strip()\n            item = item.lower().strip()\n##            if len(gTerm.split(\" \")) > 1:\n            if item.find(gTerm) is not -1: #or gTerm.find(item) is not -1:\n                tpConceptList.append(item)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "computePrecisio",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "def computePrecision (TPlist, FPList):\n    t = sum(TPlist) + sum(FPList)\n    p = float(0)\n    if t>0:\n        p = sum(TPlist)/float( t )\n    return round( p,4 )\ndef computeRecall (TPlist, denom):\n    p = sum(TPlist)/float(denom)\n    return round( p,4 )\ndef score(seq1, seq2, model):",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "computeRecal",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "def computeRecall (TPlist, denom):\n    p = sum(TPlist)/float(denom)\n    return round( p,4 )\ndef score(seq1, seq2, model):\n\t'''prepares inputs for scoring'''\n\tseq1_word_list = word_tokenize(seq1.strip().lower())\n\tseq2_word_list = word_tokenize(seq2.strip().lower())\n\treturn sim_avg_score(seq1_word_list, seq2_word_list,model)\ndef sim_max_score(wordlist1, wordlist2,model):\n\t'''calculates average maximum similarity between two phrase inputs'''",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "score",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "def score(seq1, seq2, model):\n\t'''prepares inputs for scoring'''\n\tseq1_word_list = word_tokenize(seq1.strip().lower())\n\tseq2_word_list = word_tokenize(seq2.strip().lower())\n\treturn sim_avg_score(seq1_word_list, seq2_word_list,model)\ndef sim_max_score(wordlist1, wordlist2,model):\n\t'''calculates average maximum similarity between two phrase inputs'''\n\tmaxes = []\n\tfor word in wordlist1:\n\t\tcur_max = 0",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "sim_max_score",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "def sim_max_score(wordlist1, wordlist2,model):\n\t'''calculates average maximum similarity between two phrase inputs'''\n\tmaxes = []\n\tfor word in wordlist1:\n\t\tcur_max = 0\n\t\tfor word2 in wordlist2:\n\t\t\tif word == word2: #case where words are identical\n\t\t\t\tsim = 1\n\t\t\t\tcur_max = sim\n\t\t\telif word in model and word2 in model:\t",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "sim_sum_score",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "def sim_sum_score(wordlist1, wordlist2,model):\n    '''calculates similarity between two phrase inputs'''\n    x1 = np.zeros(100)\n    x2 = np.zeros(100)\n    for word in wordlist1:\n        if word in model:\n            x1 = x1 + model[word]\n    if len(wordlist1) > 0:\n        x1 = x1/len(wordlist1)\n    for word in wordlist2:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "sim_avg_score",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "def sim_avg_score(wordlist1, wordlist2,model):\n\tavgdist = []\n\tfor word in wordlist1:\n            cur_dist = 0\n            for word2 in wordlist2:\n                if word == word2:\n                    sim = 1\n                    cur_dist += sim\n                elif word in model and word2 in model:\t\n                    sim = model.similarity(word, word2) #calculate cosine similarity score for words",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "model_most_similar_phrases",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "def model_most_similar_phrases(model, inputSpace, seedConcepts, topn, vs, ws):\n    candidateSet = set()\n    filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/May20/RecallPrecision/Results_W2V_CBOW_AvgDist_PhraseList_\"+str(vs)+\"_\"+str(ws)+\"_\"+str(topn)+\".txt\" #DomainKnowledge_MM_\n    wfile2 = open (filePath, \"w\")\n    for sc in seedConcepts:\n        wfile2.write(str(sc)+\"\\t\")\n        scoreList = list(repeat(0,len(inputSpace)))\n        for i in range(0,len(inputSpace)):\n            ip = inputSpace[i]\n            scoreList[i] = score(ip, sc, model)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "\tseq1_word_list",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "\tseq1_word_list = word_tokenize(seq1.strip().lower())\n\tseq2_word_list = word_tokenize(seq2.strip().lower())\n\treturn sim_avg_score(seq1_word_list, seq2_word_list,model)\ndef sim_max_score(wordlist1, wordlist2,model):\n\t'''calculates average maximum similarity between two phrase inputs'''\n\tmaxes = []\n\tfor word in wordlist1:\n\t\tcur_max = 0\n\t\tfor word2 in wordlist2:\n\t\t\tif word == word2: #case where words are identical",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "\tseq2_word_list",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "\tseq2_word_list = word_tokenize(seq2.strip().lower())\n\treturn sim_avg_score(seq1_word_list, seq2_word_list,model)\ndef sim_max_score(wordlist1, wordlist2,model):\n\t'''calculates average maximum similarity between two phrase inputs'''\n\tmaxes = []\n\tfor word in wordlist1:\n\t\tcur_max = 0\n\t\tfor word2 in wordlist2:\n\t\t\tif word == word2: #case where words are identical\n\t\t\t\tsim = 1",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "\tmaxes",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "\tmaxes = []\n\tfor word in wordlist1:\n\t\tcur_max = 0\n\t\tfor word2 in wordlist2:\n\t\t\tif word == word2: #case where words are identical\n\t\t\t\tsim = 1\n\t\t\t\tcur_max = sim\n\t\t\telif word in model and word2 in model:\t\n\t\t\t\tsim = model.similarity(word, word2) #calculate cosine similarity score for words\n\t\t\t\tif sim > cur_max:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "\t\tcur_max",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "\t\tcur_max = 0\n\t\tfor word2 in wordlist2:\n\t\t\tif word == word2: #case where words are identical\n\t\t\t\tsim = 1\n\t\t\t\tcur_max = sim\n\t\t\telif word in model and word2 in model:\t\n\t\t\t\tsim = model.similarity(word, word2) #calculate cosine similarity score for words\n\t\t\t\tif sim > cur_max:\n\t\t\t\t\tcur_max = sim\n\t\tif cur_max != 0:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "\t\t\t\tsim",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "\t\t\t\tsim = 1\n\t\t\t\tcur_max = sim\n\t\t\telif word in model and word2 in model:\t\n\t\t\t\tsim = model.similarity(word, word2) #calculate cosine similarity score for words\n\t\t\t\tif sim > cur_max:\n\t\t\t\t\tcur_max = sim\n\t\tif cur_max != 0:\n\t\t\tmaxes.append(cur_max)\n\tif sum(maxes) == 0:\n\t    return 0",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "\t\t\t\tcur_max",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "\t\t\t\tcur_max = sim\n\t\t\telif word in model and word2 in model:\t\n\t\t\t\tsim = model.similarity(word, word2) #calculate cosine similarity score for words\n\t\t\t\tif sim > cur_max:\n\t\t\t\t\tcur_max = sim\n\t\tif cur_max != 0:\n\t\t\tmaxes.append(cur_max)\n\tif sum(maxes) == 0:\n\t    return 0\n\treturn float(sum(maxes)) / len(maxes)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "\t\t\t\tsim",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "\t\t\t\tsim = model.similarity(word, word2) #calculate cosine similarity score for words\n\t\t\t\tif sim > cur_max:\n\t\t\t\t\tcur_max = sim\n\t\tif cur_max != 0:\n\t\t\tmaxes.append(cur_max)\n\tif sum(maxes) == 0:\n\t    return 0\n\treturn float(sum(maxes)) / len(maxes)\ndef sim_sum_score(wordlist1, wordlist2,model):\n    '''calculates similarity between two phrase inputs'''",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "\t\t\t\t\tcur_max",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "\t\t\t\t\tcur_max = sim\n\t\tif cur_max != 0:\n\t\t\tmaxes.append(cur_max)\n\tif sum(maxes) == 0:\n\t    return 0\n\treturn float(sum(maxes)) / len(maxes)\ndef sim_sum_score(wordlist1, wordlist2,model):\n    '''calculates similarity between two phrase inputs'''\n    x1 = np.zeros(100)\n    x2 = np.zeros(100)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "\tavgdist",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "\tavgdist = []\n\tfor word in wordlist1:\n            cur_dist = 0\n            for word2 in wordlist2:\n                if word == word2:\n                    sim = 1\n                    cur_dist += sim\n                elif word in model and word2 in model:\t\n                    sim = model.similarity(word, word2) #calculate cosine similarity score for words\n                    cur_dist += sim",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/trainingData/DataLines.txt\" \ndataString = loadStringData2(filePath)\ndataString = re.sub(\"[^a-zA-Z0-9'-]\",\" \", dataString)\n##        tokens = line.split(\" \")\n##        tokens = [w for w in tokens if w.find(\" \") is -1 and not w.isdigit() and not w in stops]\n##        line = \" \".join(tokens)\n# sent_tokenize_list = sent_tokenize(dataString)\n# tokens = nltk.wordpunct_tokenize(dataString) #word_tokenize\ntokens = word_tokenize(dataString) #word_tokenize\ntokens = [w for w in tokens if not w.isdigit()]",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "dataString",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "dataString = loadStringData2(filePath)\ndataString = re.sub(\"[^a-zA-Z0-9'-]\",\" \", dataString)\n##        tokens = line.split(\" \")\n##        tokens = [w for w in tokens if w.find(\" \") is -1 and not w.isdigit() and not w in stops]\n##        line = \" \".join(tokens)\n# sent_tokenize_list = sent_tokenize(dataString)\n# tokens = nltk.wordpunct_tokenize(dataString) #word_tokenize\ntokens = word_tokenize(dataString) #word_tokenize\ntokens = [w for w in tokens if not w.isdigit()]\nstops = loadStopWords()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "dataString",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "dataString = re.sub(\"[^a-zA-Z0-9'-]\",\" \", dataString)\n##        tokens = line.split(\" \")\n##        tokens = [w for w in tokens if w.find(\" \") is -1 and not w.isdigit() and not w in stops]\n##        line = \" \".join(tokens)\n# sent_tokenize_list = sent_tokenize(dataString)\n# tokens = nltk.wordpunct_tokenize(dataString) #word_tokenize\ntokens = word_tokenize(dataString) #word_tokenize\ntokens = [w for w in tokens if not w.isdigit()]\nstops = loadStopWords()\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/conceptList.txt\"",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "tokens = word_tokenize(dataString) #word_tokenize\ntokens = [w for w in tokens if not w.isdigit()]\nstops = loadStopWords()\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/conceptList.txt\"\nconceptList = loadOriginalConceptList(filePath)\nconceptList = [w for w in conceptList if w.find(\" \") is -1]\nfinder = BigramCollocationFinder.from_words(tokens)\nbigram_measures = nltk.collocations.BigramAssocMeasures()\n##sorted(finder.nbest(bigram_measures.raw_freq, 100))\nscored = finder.score_ngrams(bigram_measures.raw_freq)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "tokens = [w for w in tokens if not w.isdigit()]\nstops = loadStopWords()\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/conceptList.txt\"\nconceptList = loadOriginalConceptList(filePath)\nconceptList = [w for w in conceptList if w.find(\" \") is -1]\nfinder = BigramCollocationFinder.from_words(tokens)\nbigram_measures = nltk.collocations.BigramAssocMeasures()\n##sorted(finder.nbest(bigram_measures.raw_freq, 100))\nscored = finder.score_ngrams(bigram_measures.raw_freq)\nbigramSet = set(sorted(bigram for bigram, score in scored))  ",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "stops",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "stops = loadStopWords()\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/conceptList.txt\"\nconceptList = loadOriginalConceptList(filePath)\nconceptList = [w for w in conceptList if w.find(\" \") is -1]\nfinder = BigramCollocationFinder.from_words(tokens)\nbigram_measures = nltk.collocations.BigramAssocMeasures()\n##sorted(finder.nbest(bigram_measures.raw_freq, 100))\nscored = finder.score_ngrams(bigram_measures.raw_freq)\nbigramSet = set(sorted(bigram for bigram, score in scored))  \nbigramList = list(bigramSet)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/conceptList.txt\"\nconceptList = loadOriginalConceptList(filePath)\nconceptList = [w for w in conceptList if w.find(\" \") is -1]\nfinder = BigramCollocationFinder.from_words(tokens)\nbigram_measures = nltk.collocations.BigramAssocMeasures()\n##sorted(finder.nbest(bigram_measures.raw_freq, 100))\nscored = finder.score_ngrams(bigram_measures.raw_freq)\nbigramSet = set(sorted(bigram for bigram, score in scored))  \nbigramList = list(bigramSet)\nlen(bigramList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "conceptList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "conceptList = loadOriginalConceptList(filePath)\nconceptList = [w for w in conceptList if w.find(\" \") is -1]\nfinder = BigramCollocationFinder.from_words(tokens)\nbigram_measures = nltk.collocations.BigramAssocMeasures()\n##sorted(finder.nbest(bigram_measures.raw_freq, 100))\nscored = finder.score_ngrams(bigram_measures.raw_freq)\nbigramSet = set(sorted(bigram for bigram, score in scored))  \nbigramList = list(bigramSet)\nlen(bigramList)\nfiltBigramList = list()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "conceptList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "conceptList = [w for w in conceptList if w.find(\" \") is -1]\nfinder = BigramCollocationFinder.from_words(tokens)\nbigram_measures = nltk.collocations.BigramAssocMeasures()\n##sorted(finder.nbest(bigram_measures.raw_freq, 100))\nscored = finder.score_ngrams(bigram_measures.raw_freq)\nbigramSet = set(sorted(bigram for bigram, score in scored))  \nbigramList = list(bigramSet)\nlen(bigramList)\nfiltBigramList = list()\nfiltBigramList= [u+\" \"+v for u,v in bigramList if not u in stops and not v in stops and (u in conceptList or v in conceptList)]",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "finder",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "finder = BigramCollocationFinder.from_words(tokens)\nbigram_measures = nltk.collocations.BigramAssocMeasures()\n##sorted(finder.nbest(bigram_measures.raw_freq, 100))\nscored = finder.score_ngrams(bigram_measures.raw_freq)\nbigramSet = set(sorted(bigram for bigram, score in scored))  \nbigramList = list(bigramSet)\nlen(bigramList)\nfiltBigramList = list()\nfiltBigramList= [u+\" \"+v for u,v in bigramList if not u in stops and not v in stops and (u in conceptList or v in conceptList)]\nlen(filtBigramList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "bigram_measures",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "bigram_measures = nltk.collocations.BigramAssocMeasures()\n##sorted(finder.nbest(bigram_measures.raw_freq, 100))\nscored = finder.score_ngrams(bigram_measures.raw_freq)\nbigramSet = set(sorted(bigram for bigram, score in scored))  \nbigramList = list(bigramSet)\nlen(bigramList)\nfiltBigramList = list()\nfiltBigramList= [u+\" \"+v for u,v in bigramList if not u in stops and not v in stops and (u in conceptList or v in conceptList)]\nlen(filtBigramList)\nfiltBigramList= [w for w in filtBigramList if len(set('[~!@#$%^&*()_-+{}<>\":,.;?/|\\']+$').intersection(w)) == 0 ]",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "scored",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "scored = finder.score_ngrams(bigram_measures.raw_freq)\nbigramSet = set(sorted(bigram for bigram, score in scored))  \nbigramList = list(bigramSet)\nlen(bigramList)\nfiltBigramList = list()\nfiltBigramList= [u+\" \"+v for u,v in bigramList if not u in stops and not v in stops and (u in conceptList or v in conceptList)]\nlen(filtBigramList)\nfiltBigramList= [w for w in filtBigramList if len(set('[~!@#$%^&*()_-+{}<>\":,.;?/|\\']+$').intersection(w)) == 0 ]\n# filtBigramList= [w for w in filtBigramList if w.find(\"patient\") is -1 and w.find(\" pt\") is -1]\nlen(filtBigramList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "bigramSet",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "bigramSet = set(sorted(bigram for bigram, score in scored))  \nbigramList = list(bigramSet)\nlen(bigramList)\nfiltBigramList = list()\nfiltBigramList= [u+\" \"+v for u,v in bigramList if not u in stops and not v in stops and (u in conceptList or v in conceptList)]\nlen(filtBigramList)\nfiltBigramList= [w for w in filtBigramList if len(set('[~!@#$%^&*()_-+{}<>\":,.;?/|\\']+$').intersection(w)) == 0 ]\n# filtBigramList= [w for w in filtBigramList if w.find(\"patient\") is -1 and w.find(\" pt\") is -1]\nlen(filtBigramList)\nfiltBigramList = list(set(filtBigramList))",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "bigramList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "bigramList = list(bigramSet)\nlen(bigramList)\nfiltBigramList = list()\nfiltBigramList= [u+\" \"+v for u,v in bigramList if not u in stops and not v in stops and (u in conceptList or v in conceptList)]\nlen(filtBigramList)\nfiltBigramList= [w for w in filtBigramList if len(set('[~!@#$%^&*()_-+{}<>\":,.;?/|\\']+$').intersection(w)) == 0 ]\n# filtBigramList= [w for w in filtBigramList if w.find(\"patient\") is -1 and w.find(\" pt\") is -1]\nlen(filtBigramList)\nfiltBigramList = list(set(filtBigramList))\nfinder = TrigramCollocationFinder.from_words(tokens)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "filtBigramList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "filtBigramList = list()\nfiltBigramList= [u+\" \"+v for u,v in bigramList if not u in stops and not v in stops and (u in conceptList or v in conceptList)]\nlen(filtBigramList)\nfiltBigramList= [w for w in filtBigramList if len(set('[~!@#$%^&*()_-+{}<>\":,.;?/|\\']+$').intersection(w)) == 0 ]\n# filtBigramList= [w for w in filtBigramList if w.find(\"patient\") is -1 and w.find(\" pt\") is -1]\nlen(filtBigramList)\nfiltBigramList = list(set(filtBigramList))\nfinder = TrigramCollocationFinder.from_words(tokens)\ntrigram_measures = nltk.collocations.TrigramAssocMeasures()\nscored = finder.score_ngrams(trigram_measures.raw_freq)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "filtBigramList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "filtBigramList = list(set(filtBigramList))\nfinder = TrigramCollocationFinder.from_words(tokens)\ntrigram_measures = nltk.collocations.TrigramAssocMeasures()\nscored = finder.score_ngrams(trigram_measures.raw_freq)\ntrigramSet = set(sorted(trigram for trigram, score in scored))  \ntrigramList = list(trigramSet)\nlen(trigramList)\nfiltTrigramList = list()\nfiltTrigramList= [u+\" \"+v+\" \"+w for u,v,w in trigramList if not u in stops and not v in stops and not w in stops and (u in conceptList or v in conceptList or w in conceptList)]\nlen(filtTrigramList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "finder",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "finder = TrigramCollocationFinder.from_words(tokens)\ntrigram_measures = nltk.collocations.TrigramAssocMeasures()\nscored = finder.score_ngrams(trigram_measures.raw_freq)\ntrigramSet = set(sorted(trigram for trigram, score in scored))  \ntrigramList = list(trigramSet)\nlen(trigramList)\nfiltTrigramList = list()\nfiltTrigramList= [u+\" \"+v+\" \"+w for u,v,w in trigramList if not u in stops and not v in stops and not w in stops and (u in conceptList or v in conceptList or w in conceptList)]\nlen(filtTrigramList)\nfiltTrigramList= [w for w in filtTrigramList if len(set('[~!@#$%^&*()_-+{}<>\":,.;?/|\\']+$').intersection(w)) == 0 ]",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "trigram_measures",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "trigram_measures = nltk.collocations.TrigramAssocMeasures()\nscored = finder.score_ngrams(trigram_measures.raw_freq)\ntrigramSet = set(sorted(trigram for trigram, score in scored))  \ntrigramList = list(trigramSet)\nlen(trigramList)\nfiltTrigramList = list()\nfiltTrigramList= [u+\" \"+v+\" \"+w for u,v,w in trigramList if not u in stops and not v in stops and not w in stops and (u in conceptList or v in conceptList or w in conceptList)]\nlen(filtTrigramList)\nfiltTrigramList= [w for w in filtTrigramList if len(set('[~!@#$%^&*()_-+{}<>\":,.;?/|\\']+$').intersection(w)) == 0 ]\n# filtTrigramList = [w for w in filtTrigramList if w.find(\"patient\") is -1 and w.find(\" pt\") is -1] ",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "scored",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "scored = finder.score_ngrams(trigram_measures.raw_freq)\ntrigramSet = set(sorted(trigram for trigram, score in scored))  \ntrigramList = list(trigramSet)\nlen(trigramList)\nfiltTrigramList = list()\nfiltTrigramList= [u+\" \"+v+\" \"+w for u,v,w in trigramList if not u in stops and not v in stops and not w in stops and (u in conceptList or v in conceptList or w in conceptList)]\nlen(filtTrigramList)\nfiltTrigramList= [w for w in filtTrigramList if len(set('[~!@#$%^&*()_-+{}<>\":,.;?/|\\']+$').intersection(w)) == 0 ]\n# filtTrigramList = [w for w in filtTrigramList if w.find(\"patient\") is -1 and w.find(\" pt\") is -1] \n##filtTrigramList= [u+\" \"+v+\" \"+w for u,v,w in trigramList if not u in stops and not w in stops and (u in conceptList or v in conceptList or w in conceptList)]",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "trigramSet",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "trigramSet = set(sorted(trigram for trigram, score in scored))  \ntrigramList = list(trigramSet)\nlen(trigramList)\nfiltTrigramList = list()\nfiltTrigramList= [u+\" \"+v+\" \"+w for u,v,w in trigramList if not u in stops and not v in stops and not w in stops and (u in conceptList or v in conceptList or w in conceptList)]\nlen(filtTrigramList)\nfiltTrigramList= [w for w in filtTrigramList if len(set('[~!@#$%^&*()_-+{}<>\":,.;?/|\\']+$').intersection(w)) == 0 ]\n# filtTrigramList = [w for w in filtTrigramList if w.find(\"patient\") is -1 and w.find(\" pt\") is -1] \n##filtTrigramList= [u+\" \"+v+\" \"+w for u,v,w in trigramList if not u in stops and not w in stops and (u in conceptList or v in conceptList or w in conceptList)]\nfiltTrigramList = list(set(filtTrigramList))",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "trigramList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "trigramList = list(trigramSet)\nlen(trigramList)\nfiltTrigramList = list()\nfiltTrigramList= [u+\" \"+v+\" \"+w for u,v,w in trigramList if not u in stops and not v in stops and not w in stops and (u in conceptList or v in conceptList or w in conceptList)]\nlen(filtTrigramList)\nfiltTrigramList= [w for w in filtTrigramList if len(set('[~!@#$%^&*()_-+{}<>\":,.;?/|\\']+$').intersection(w)) == 0 ]\n# filtTrigramList = [w for w in filtTrigramList if w.find(\"patient\") is -1 and w.find(\" pt\") is -1] \n##filtTrigramList= [u+\" \"+v+\" \"+w for u,v,w in trigramList if not u in stops and not w in stops and (u in conceptList or v in conceptList or w in conceptList)]\nfiltTrigramList = list(set(filtTrigramList))\nlen(filtTrigramList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "filtTrigramList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "filtTrigramList = list()\nfiltTrigramList= [u+\" \"+v+\" \"+w for u,v,w in trigramList if not u in stops and not v in stops and not w in stops and (u in conceptList or v in conceptList or w in conceptList)]\nlen(filtTrigramList)\nfiltTrigramList= [w for w in filtTrigramList if len(set('[~!@#$%^&*()_-+{}<>\":,.;?/|\\']+$').intersection(w)) == 0 ]\n# filtTrigramList = [w for w in filtTrigramList if w.find(\"patient\") is -1 and w.find(\" pt\") is -1] \n##filtTrigramList= [u+\" \"+v+\" \"+w for u,v,w in trigramList if not u in stops and not w in stops and (u in conceptList or v in conceptList or w in conceptList)]\nfiltTrigramList = list(set(filtTrigramList))\nlen(filtTrigramList)\n##filtTrigramList = set()\n##for trigram in trigramList:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "filtTrigramList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "filtTrigramList = list(set(filtTrigramList))\nlen(filtTrigramList)\n##filtTrigramList = set()\n##for trigram in trigramList:\n##    if not trigram[0] in stops and not trigram[2] in stops:\n##        print(trigram[0]+\" \"+trigram[1]+\" \"+trigram[2] )\n##        if trigram[0] in conceptList or trigram[1] in conceptList or trigram[2] in conceptList:\n##            filtTrigramList.update(trigram[0]+\" \"+trigram[1]+\" \"+trigram[2])\n##            \n##len(filtTrigramList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "##filtTrigramList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "##filtTrigramList = set()\n##for trigram in trigramList:\n##    if not trigram[0] in stops and not trigram[2] in stops:\n##        print(trigram[0]+\" \"+trigram[1]+\" \"+trigram[2] )\n##        if trigram[0] in conceptList or trigram[1] in conceptList or trigram[2] in conceptList:\n##            filtTrigramList.update(trigram[0]+\" \"+trigram[1]+\" \"+trigram[2])\n##            \n##len(filtTrigramList)\n##load GT annotation\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/FormattedTags/allInOne_new.txt\"",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/FormattedTags/allInOne_new.txt\"\ngtConceptList = loadSimpleListPhrase(filePath)\nfrequencyList = list(repeat(0,len(gtConceptList)))\n##load protocol concept list\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/conceptList.txt\"\nconceptList = loadOriginalConceptListPhrase(filePath)\n##load metaMap extended concept list\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/version2/MetaMapOutputsConciseP.txt\" ## MetaMapOutputsConciseW MetaMapOutputs_FilteredST_ConciseW\nrfile1 = open(filePath,\"r\")\nmmWordList = list()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "gtConceptList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "gtConceptList = loadSimpleListPhrase(filePath)\nfrequencyList = list(repeat(0,len(gtConceptList)))\n##load protocol concept list\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/conceptList.txt\"\nconceptList = loadOriginalConceptListPhrase(filePath)\n##load metaMap extended concept list\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/version2/MetaMapOutputsConciseP.txt\" ## MetaMapOutputsConciseW MetaMapOutputs_FilteredST_ConciseW\nrfile1 = open(filePath,\"r\")\nmmWordList = list()\nfor line in rfile1:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "frequencyList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "frequencyList = list(repeat(0,len(gtConceptList)))\n##load protocol concept list\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/conceptList.txt\"\nconceptList = loadOriginalConceptListPhrase(filePath)\n##load metaMap extended concept list\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/version2/MetaMapOutputsConciseP.txt\" ## MetaMapOutputsConciseW MetaMapOutputs_FilteredST_ConciseW\nrfile1 = open(filePath,\"r\")\nmmWordList = list()\nfor line in rfile1:\n    word = line.strip()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/conceptList.txt\"\nconceptList = loadOriginalConceptListPhrase(filePath)\n##load metaMap extended concept list\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/version2/MetaMapOutputsConciseP.txt\" ## MetaMapOutputsConciseW MetaMapOutputs_FilteredST_ConciseW\nrfile1 = open(filePath,\"r\")\nmmWordList = list()\nfor line in rfile1:\n    word = line.strip()\n    if not word.isdigit():\n        if word not in mmWordList:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "conceptList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "conceptList = loadOriginalConceptListPhrase(filePath)\n##load metaMap extended concept list\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/version2/MetaMapOutputsConciseP.txt\" ## MetaMapOutputsConciseW MetaMapOutputs_FilteredST_ConciseW\nrfile1 = open(filePath,\"r\")\nmmWordList = list()\nfor line in rfile1:\n    word = line.strip()\n    if not word.isdigit():\n        if word not in mmWordList:\n            mmWordList.append(word)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/version2/MetaMapOutputsConciseP.txt\" ## MetaMapOutputsConciseW MetaMapOutputs_FilteredST_ConciseW\nrfile1 = open(filePath,\"r\")\nmmWordList = list()\nfor line in rfile1:\n    word = line.strip()\n    if not word.isdigit():\n        if word not in mmWordList:\n            mmWordList.append(word)\nrfile1.close()\n##count ground truth in testData",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "rfile1",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "rfile1 = open(filePath,\"r\")\nmmWordList = list()\nfor line in rfile1:\n    word = line.strip()\n    if not word.isdigit():\n        if word not in mmWordList:\n            mmWordList.append(word)\nrfile1.close()\n##count ground truth in testData\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData_new.txt\"",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "mmWordList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "mmWordList = list()\nfor line in rfile1:\n    word = line.strip()\n    if not word.isdigit():\n        if word not in mmWordList:\n            mmWordList.append(word)\nrfile1.close()\n##count ground truth in testData\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData_new.txt\"\nrfile = open(filePath, \"r\")",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData_new.txt\"\nrfile = open(filePath, \"r\")\nfor line in rfile:\n    for i in range (0,len(gtConceptList)):\n        line = line.lower().strip()\n        line = re.sub(\"[^a-zA-Z0-9'-]\",\" \", line)\n        if line.find(gtConceptList[i]) is not -1:\n            frequencyList[i] += 1            \nrfile.close()\n##create Output file",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "rfile",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "rfile = open(filePath, \"r\")\nfor line in rfile:\n    for i in range (0,len(gtConceptList)):\n        line = line.lower().strip()\n        line = re.sub(\"[^a-zA-Z0-9'-]\",\" \", line)\n        if line.find(gtConceptList[i]) is not -1:\n            frequencyList[i] += 1            \nrfile.close()\n##create Output file\ndenom = sum(frequencyList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "denom",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "denom = sum(frequencyList)\n###calculate phrase similarity\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/trainingData/DataLines.txt\" \nn= fileLength(filePath)\nsentenceList = loadTrainingData(filePath, n, True)\nmin_count = 1\nvectorSize = 100\nwindowSize = 10\ncbow_mean = 0\ndownsampling = 1e-3",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/trainingData/DataLines.txt\" \nn= fileLength(filePath)\nsentenceList = loadTrainingData(filePath, n, True)\nmin_count = 1\nvectorSize = 100\nwindowSize = 10\ncbow_mean = 0\ndownsampling = 1e-3\n# model = gensim.models.Word2Vec(sentenceList, sg=0, min_count=min_count, size=vectorSize, window=windowSize, sample = downsampling)\ncandidateList = list()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "sentenceList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "sentenceList = loadTrainingData(filePath, n, True)\nmin_count = 1\nvectorSize = 100\nwindowSize = 10\ncbow_mean = 0\ndownsampling = 1e-3\n# model = gensim.models.Word2Vec(sentenceList, sg=0, min_count=min_count, size=vectorSize, window=windowSize, sample = downsampling)\ncandidateList = list()\ninputSpace = set()\ninputSpace.update(filtTrigramList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "min_count",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "min_count = 1\nvectorSize = 100\nwindowSize = 10\ncbow_mean = 0\ndownsampling = 1e-3\n# model = gensim.models.Word2Vec(sentenceList, sg=0, min_count=min_count, size=vectorSize, window=windowSize, sample = downsampling)\ncandidateList = list()\ninputSpace = set()\ninputSpace.update(filtTrigramList)\ninputSpace.update(filtBigramList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "vectorSize",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "vectorSize = 100\nwindowSize = 10\ncbow_mean = 0\ndownsampling = 1e-3\n# model = gensim.models.Word2Vec(sentenceList, sg=0, min_count=min_count, size=vectorSize, window=windowSize, sample = downsampling)\ncandidateList = list()\ninputSpace = set()\ninputSpace.update(filtTrigramList)\ninputSpace.update(filtBigramList)\ninputSpace = list(inputSpace)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "windowSize",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "windowSize = 10\ncbow_mean = 0\ndownsampling = 1e-3\n# model = gensim.models.Word2Vec(sentenceList, sg=0, min_count=min_count, size=vectorSize, window=windowSize, sample = downsampling)\ncandidateList = list()\ninputSpace = set()\ninputSpace.update(filtTrigramList)\ninputSpace.update(filtBigramList)\ninputSpace = list(inputSpace)\n# inputSpace = inputSpace[5500:5505]",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "cbow_mean",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "cbow_mean = 0\ndownsampling = 1e-3\n# model = gensim.models.Word2Vec(sentenceList, sg=0, min_count=min_count, size=vectorSize, window=windowSize, sample = downsampling)\ncandidateList = list()\ninputSpace = set()\ninputSpace.update(filtTrigramList)\ninputSpace.update(filtBigramList)\ninputSpace = list(inputSpace)\n# inputSpace = inputSpace[5500:5505]\n# conceptList = conceptList[1:10]",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "downsampling",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "downsampling = 1e-3\n# model = gensim.models.Word2Vec(sentenceList, sg=0, min_count=min_count, size=vectorSize, window=windowSize, sample = downsampling)\ncandidateList = list()\ninputSpace = set()\ninputSpace.update(filtTrigramList)\ninputSpace.update(filtBigramList)\ninputSpace = list(inputSpace)\n# inputSpace = inputSpace[5500:5505]\n# conceptList = conceptList[1:10]\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/May20/RecallPrecision/Results_W2V_CBOW_phrases_AvgDist.txt\" #DomainKnowledge_MM_",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "candidateList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "candidateList = list()\ninputSpace = set()\ninputSpace.update(filtTrigramList)\ninputSpace.update(filtBigramList)\ninputSpace = list(inputSpace)\n# inputSpace = inputSpace[5500:5505]\n# conceptList = conceptList[1:10]\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/May20/RecallPrecision/Results_W2V_CBOW_phrases_AvgDist.txt\" #DomainKnowledge_MM_\nwfile = open (filePath, \"a\")\nwfile.write(\"cbow_mean\\tvectorSize\\twindowLength\\ttopN\\tRecall\\tPrecision\\tRecallMM\\tPrecisionMM\\n\")",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "inputSpace",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "inputSpace = set()\ninputSpace.update(filtTrigramList)\ninputSpace.update(filtBigramList)\ninputSpace = list(inputSpace)\n# inputSpace = inputSpace[5500:5505]\n# conceptList = conceptList[1:10]\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/May20/RecallPrecision/Results_W2V_CBOW_phrases_AvgDist.txt\" #DomainKnowledge_MM_\nwfile = open (filePath, \"a\")\nwfile.write(\"cbow_mean\\tvectorSize\\twindowLength\\ttopN\\tRecall\\tPrecision\\tRecallMM\\tPrecisionMM\\n\")\n##wfile.write(\"cbow_mean\\tvectorSize\\twindowLength\\ttopN\\tRecall\\tPrecision\\n\")",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "inputSpace",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "inputSpace = list(inputSpace)\n# inputSpace = inputSpace[5500:5505]\n# conceptList = conceptList[1:10]\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/May20/RecallPrecision/Results_W2V_CBOW_phrases_AvgDist.txt\" #DomainKnowledge_MM_\nwfile = open (filePath, \"a\")\nwfile.write(\"cbow_mean\\tvectorSize\\twindowLength\\ttopN\\tRecall\\tPrecision\\tRecallMM\\tPrecisionMM\\n\")\n##wfile.write(\"cbow_mean\\tvectorSize\\twindowLength\\ttopN\\tRecall\\tPrecision\\n\")\n################################################################################################################################################\n##candidateList = model_most_similar_phrases(model, inputSpace, conceptList, topn=200)\n##w2vList.update(candidateList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/May20/RecallPrecision/Results_W2V_CBOW_phrases_AvgDist.txt\" #DomainKnowledge_MM_\nwfile = open (filePath, \"a\")\nwfile.write(\"cbow_mean\\tvectorSize\\twindowLength\\ttopN\\tRecall\\tPrecision\\tRecallMM\\tPrecisionMM\\n\")\n##wfile.write(\"cbow_mean\\tvectorSize\\twindowLength\\ttopN\\tRecall\\tPrecision\\n\")\n################################################################################################################################################\n##candidateList = model_most_similar_phrases(model, inputSpace, conceptList, topn=200)\n##w2vList.update(candidateList)\n##w2vList = list(w2vList)\n##for item in w2vList:\n####    print(item)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "wfile",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "wfile = open (filePath, \"a\")\nwfile.write(\"cbow_mean\\tvectorSize\\twindowLength\\ttopN\\tRecall\\tPrecision\\tRecallMM\\tPrecisionMM\\n\")\n##wfile.write(\"cbow_mean\\tvectorSize\\twindowLength\\ttopN\\tRecall\\tPrecision\\n\")\n################################################################################################################################################\n##candidateList = model_most_similar_phrases(model, inputSpace, conceptList, topn=200)\n##w2vList.update(candidateList)\n##w2vList = list(w2vList)\n##for item in w2vList:\n####    print(item)\n##    wfile.write(item+\"\\n\")",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "##candidateList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "##candidateList = model_most_similar_phrases(model, inputSpace, conceptList, topn=200)\n##w2vList.update(candidateList)\n##w2vList = list(w2vList)\n##for item in w2vList:\n####    print(item)\n##    wfile.write(item+\"\\n\")\n##wfile.close()\n################################################################################################################################################\nfor cbow_mean in [0]:#[1,0]:\n    for vectorSize in [100]:#[50, 100, 150, 200, 250, 300]:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "##w2vList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "peekOfCode": "##w2vList = list(w2vList)\n##for item in w2vList:\n####    print(item)\n##    wfile.write(item+\"\\n\")\n##wfile.close()\n################################################################################################################################################\nfor cbow_mean in [0]:#[1,0]:\n    for vectorSize in [100]:#[50, 100, 150, 200, 250, 300]:\n        for windowSize in [10]: #, 15, 20\n            for TOPN in [100]:#[100, 150, 200]:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Phrases",
        "documentation": {}
    },
    {
        "label": "fileLength",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "peekOfCode": "def fileLength(fname):\n    with open(fname) as f:\n        for i, l in enumerate(f):\n            pass\n    return i + 1\ndef hasNumbers(inputString):\n    return any(char.isdigit() for char in inputString)\ndef loadTrainingData(filepath, n, remove_stopwords=True):\n    dataList = [[] for x in range(n)]\n    rfile = open(filepath, \"r\")",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "documentation": {}
    },
    {
        "label": "hasNumbers",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "peekOfCode": "def hasNumbers(inputString):\n    return any(char.isdigit() for char in inputString)\ndef loadTrainingData(filepath, n, remove_stopwords=True):\n    dataList = [[] for x in range(n)]\n    rfile = open(filepath, \"r\")\n    for line in rfile:\n        line = line.lower().strip()\n        line = re.sub(\"[^a-zA-Z0-9'-]\",\" \", line)\n        stops = loadStopWords()\n        innerList = line.split()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "documentation": {}
    },
    {
        "label": "loadTrainingData",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "peekOfCode": "def loadTrainingData(filepath, n, remove_stopwords=True):\n    dataList = [[] for x in range(n)]\n    rfile = open(filepath, \"r\")\n    for line in rfile:\n        line = line.lower().strip()\n        line = re.sub(\"[^a-zA-Z0-9'-]\",\" \", line)\n        stops = loadStopWords()\n        innerList = line.split()\n        innerList = [w for w in innerList if not w in stops and not w.isdigit()]\n        dataList.append(innerList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "documentation": {}
    },
    {
        "label": "loadConceptList",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "peekOfCode": "def loadConceptList(filepath):\n    conceptWList = list()\n    conceptPList = list()\n    rfile = open(filepath, \"r\")\n    for line in rfile:\n        items = line.lower().strip().split(\" \")\n        for item in items:\n            if item not in conceptWList:\n                conceptWList.append(item)\n        # if line.find(\" \") is not -1:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "documentation": {}
    },
    {
        "label": "loadStopWords",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "peekOfCode": "def loadStopWords():\n    filePath = \"/Users/sarahmasudpreum/Desktop/PythonTCode/general/stopwords.txt\"\n    rfile = open(filePath, \"r\")\n    stopWords = list()\n    for line in rfile:\n        stopWords.append(line.strip())\n    rfile.close()\n    return stopWords\ndef loadGTList (filePath):\n    gtWList = list()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "documentation": {}
    },
    {
        "label": "loadGTLis",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "peekOfCode": "def loadGTList (filePath):\n    gtWList = list()\n    gtPList = list()\n    rfile = open(filePath, \"r\")\n    itemList = list()\n##    table = str.maketrans({key: None for key in string.punctuation})\n    for line in rfile:\n        line = line.lower().strip()\n        line = re.sub(\"[^a-zA-Z0-9 ]\",\" \", line)\n        stops = loadStopWords()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "documentation": {}
    },
    {
        "label": "computeFrequenc",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "peekOfCode": "def computeFrequency (gtConceptList, frequencyList, wordList):\n    wordFrequencyList = list(repeat(0,len(gtConceptList)))\n    for item in wordList:\n        word = item.strip().lower()\n        for i in range (0,len(gtConceptList)):\n                if word.find(gtConceptList[i]) is not -1: #and gtConceptList[i].find(word) is not -1: #\n                    wordFrequencyList[i] = frequencyList[i]\n                    break\n    print(\"^^^^^^^Frequency \"+str(sum(wordFrequencyList)))\n    return wordFrequencyList",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "documentation": {}
    },
    {
        "label": "computeF",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "peekOfCode": "def computeFP (filePath, conceptList, gtConceptList):\n    filteredConceptList = [x.strip() for x in conceptList if x not in gtConceptList]\n    filteredConceptList = list(set(filteredConceptList))\n    print(\"#######FPListLength \"+str(len(filteredConceptList)))\n    with open(filePath, 'r') as rfile:\n        data = rfile.read()\n    # rfile = open(filePath, \"r\")\n    frequencyList = list(repeat(0,len(filteredConceptList)))\n    for i in range (0,len(filteredConceptList)):\n        # print(data+\"\\n\"+str(filteredConceptList[i]))",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "documentation": {}
    },
    {
        "label": "computePrecisio",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "peekOfCode": "def computePrecision (TPlist, FPList):\n    t = sum(TPlist) + sum(FPList)\n    p = float(0)\n    if t>0:\n        p = sum(TPlist)/float( t )\n    return round( p,4 )\ndef computeRecall (TPlist, denom):\n    p = float(0)\n    if denom>0:\n        p = sum(TPlist)/float(denom)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "documentation": {}
    },
    {
        "label": "computeRecal",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "peekOfCode": "def computeRecall (TPlist, denom):\n    p = float(0)\n    if denom>0:\n        p = sum(TPlist)/float(denom)\n    return round( p,4 )\n##load groundTruth\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/FormattedTags/allInOne_new.txt\"\n##gtConceptList = loadSimpleList(filePath, True)\ngtWList, gtPList = loadGTList(filePath)\ngtConceptList = gtWList",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "peekOfCode": "filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/FormattedTags/allInOne_new.txt\"\n##gtConceptList = loadSimpleList(filePath, True)\ngtWList, gtPList = loadGTList(filePath)\ngtConceptList = gtWList\nfrequencyList = list(repeat(0,len(gtConceptList)))\n##load originalConceptList\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/conceptList.txt\"\nconceptWList, conceptPList = loadConceptList(filePath)\nstops = loadStopWords()\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/version2/MetaMapOutputsConciseW.txt\" ## MetaMapOutputsConciseW MetaMapOutputs_FilteredST_ConciseW",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "documentation": {}
    },
    {
        "label": "##gtConceptList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "peekOfCode": "##gtConceptList = loadSimpleList(filePath, True)\ngtWList, gtPList = loadGTList(filePath)\ngtConceptList = gtWList\nfrequencyList = list(repeat(0,len(gtConceptList)))\n##load originalConceptList\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/conceptList.txt\"\nconceptWList, conceptPList = loadConceptList(filePath)\nstops = loadStopWords()\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/version2/MetaMapOutputsConciseW.txt\" ## MetaMapOutputsConciseW MetaMapOutputs_FilteredST_ConciseW\nrfile1 = open(filePath,\"r\")",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "documentation": {}
    },
    {
        "label": "gtConceptList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "peekOfCode": "gtConceptList = gtWList\nfrequencyList = list(repeat(0,len(gtConceptList)))\n##load originalConceptList\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/conceptList.txt\"\nconceptWList, conceptPList = loadConceptList(filePath)\nstops = loadStopWords()\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/version2/MetaMapOutputsConciseW.txt\" ## MetaMapOutputsConciseW MetaMapOutputs_FilteredST_ConciseW\nrfile1 = open(filePath,\"r\")\nmmWordList = list()\nfor line in rfile1:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "documentation": {}
    },
    {
        "label": "frequencyList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "peekOfCode": "frequencyList = list(repeat(0,len(gtConceptList)))\n##load originalConceptList\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/conceptList.txt\"\nconceptWList, conceptPList = loadConceptList(filePath)\nstops = loadStopWords()\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/version2/MetaMapOutputsConciseW.txt\" ## MetaMapOutputsConciseW MetaMapOutputs_FilteredST_ConciseW\nrfile1 = open(filePath,\"r\")\nmmWordList = list()\nfor line in rfile1:\n    word = line.strip()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "peekOfCode": "filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/conceptList.txt\"\nconceptWList, conceptPList = loadConceptList(filePath)\nstops = loadStopWords()\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/version2/MetaMapOutputsConciseW.txt\" ## MetaMapOutputsConciseW MetaMapOutputs_FilteredST_ConciseW\nrfile1 = open(filePath,\"r\")\nmmWordList = list()\nfor line in rfile1:\n    word = line.strip()\n    if not word.isdigit():\n        if word not in mmWordList:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "documentation": {}
    },
    {
        "label": "stops",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "peekOfCode": "stops = loadStopWords()\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/version2/MetaMapOutputsConciseW.txt\" ## MetaMapOutputsConciseW MetaMapOutputs_FilteredST_ConciseW\nrfile1 = open(filePath,\"r\")\nmmWordList = list()\nfor line in rfile1:\n    word = line.strip()\n    if not word.isdigit():\n        if word not in mmWordList:\n            mmWordList.append(word)\nrfile1.close()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "peekOfCode": "filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/version2/MetaMapOutputsConciseW.txt\" ## MetaMapOutputsConciseW MetaMapOutputs_FilteredST_ConciseW\nrfile1 = open(filePath,\"r\")\nmmWordList = list()\nfor line in rfile1:\n    word = line.strip()\n    if not word.isdigit():\n        if word not in mmWordList:\n            mmWordList.append(word)\nrfile1.close()\n##count ground truth in testData",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "documentation": {}
    },
    {
        "label": "rfile1",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "peekOfCode": "rfile1 = open(filePath,\"r\")\nmmWordList = list()\nfor line in rfile1:\n    word = line.strip()\n    if not word.isdigit():\n        if word not in mmWordList:\n            mmWordList.append(word)\nrfile1.close()\n##count ground truth in testData\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData_new.txt\"",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "documentation": {}
    },
    {
        "label": "mmWordList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "peekOfCode": "mmWordList = list()\nfor line in rfile1:\n    word = line.strip()\n    if not word.isdigit():\n        if word not in mmWordList:\n            mmWordList.append(word)\nrfile1.close()\n##count ground truth in testData\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData_new.txt\"\nrfile = open(filePath, \"r\")",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "peekOfCode": "filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/testData/testData_new.txt\"\nrfile = open(filePath, \"r\")\nfor line in rfile:\n    for i in range (0,len(gtConceptList)):\n        wordTokens = line.lower().strip().split(\" \")\n        for word in wordTokens:\n            if word.find(gtConceptList[i]) is not -1 and gtConceptList[i].find(word) is not -1:\n                frequencyList[i] += 1            \nrfile.close()\n##create Output file",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "documentation": {}
    },
    {
        "label": "rfile",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "peekOfCode": "rfile = open(filePath, \"r\")\nfor line in rfile:\n    for i in range (0,len(gtConceptList)):\n        wordTokens = line.lower().strip().split(\" \")\n        for word in wordTokens:\n            if word.find(gtConceptList[i]) is not -1 and gtConceptList[i].find(word) is not -1:\n                frequencyList[i] += 1            \nrfile.close()\n##create Output file\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/W2VTuning/Results_W2V_SG_MM_words_1e-3_10.txt\" #DomainKnowledge_MM_",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "peekOfCode": "filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/W2VTuning/Results_W2V_SG_MM_words_1e-3_10.txt\" #DomainKnowledge_MM_\nwfile = open (filePath, \"w\")\n##wfile.write(\"cbow_mean\\tvectorSize\\twindowLength\\ttopN\\thfListSize\\tRecall\\tPrecision\\n\")\nwfile.write(\"cbow_mean\\tvectorSize\\twindowLength\\ttopN\\tRecall\\tPrecision\\tRecall_MM\\tPrecision_MM\\n\")\ndenom = sum(frequencyList)\n##createModel\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/wordEmbeddingModel/Data/EMSallDataIn1.txt\" \nn= fileLength(filePath)\nsentenceList = loadTrainingData(filePath, n, True)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "documentation": {}
    },
    {
        "label": "wfile",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "peekOfCode": "wfile = open (filePath, \"w\")\n##wfile.write(\"cbow_mean\\tvectorSize\\twindowLength\\ttopN\\thfListSize\\tRecall\\tPrecision\\n\")\nwfile.write(\"cbow_mean\\tvectorSize\\twindowLength\\ttopN\\tRecall\\tPrecision\\tRecall_MM\\tPrecision_MM\\n\")\ndenom = sum(frequencyList)\n##createModel\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/wordEmbeddingModel/Data/EMSallDataIn1.txt\" \nn= fileLength(filePath)\nsentenceList = loadTrainingData(filePath, n, True)\nconceptList = conceptWList",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "documentation": {}
    },
    {
        "label": "denom",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "peekOfCode": "denom = sum(frequencyList)\n##createModel\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/wordEmbeddingModel/Data/EMSallDataIn1.txt\" \nn= fileLength(filePath)\nsentenceList = loadTrainingData(filePath, n, True)\nconceptList = conceptWList\n##iterate parameters\nmin_count = 10\nvectorSize = 50",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "peekOfCode": "filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/wordEmbeddingModel/Data/EMSallDataIn1.txt\" \nn= fileLength(filePath)\nsentenceList = loadTrainingData(filePath, n, True)\nconceptList = conceptWList\n##iterate parameters\nmin_count = 10\nvectorSize = 50\nwindowSize = 5\ncbow_mean = 0\ndownsampling = 1e-3",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "documentation": {}
    },
    {
        "label": "sentenceList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "peekOfCode": "sentenceList = loadTrainingData(filePath, n, True)\nconceptList = conceptWList\n##iterate parameters\nmin_count = 10\nvectorSize = 50\nwindowSize = 5\ncbow_mean = 0\ndownsampling = 1e-3\nstops = loadStopWords()##set(stopwords.words(\"english\"))\n# for cbow_mean in [0, 1]:#[1,0]:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "documentation": {}
    },
    {
        "label": "conceptList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "peekOfCode": "conceptList = conceptWList\n##iterate parameters\nmin_count = 10\nvectorSize = 50\nwindowSize = 5\ncbow_mean = 0\ndownsampling = 1e-3\nstops = loadStopWords()##set(stopwords.words(\"english\"))\n# for cbow_mean in [0, 1]:#[1,0]:\nfor vectorSize in [50, 100, 150, 200, 250, 300]:#[50, 100, 150, 200, 250, 300]:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "documentation": {}
    },
    {
        "label": "min_count",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "peekOfCode": "min_count = 10\nvectorSize = 50\nwindowSize = 5\ncbow_mean = 0\ndownsampling = 1e-3\nstops = loadStopWords()##set(stopwords.words(\"english\"))\n# for cbow_mean in [0, 1]:#[1,0]:\nfor vectorSize in [50, 100, 150, 200, 250, 300]:#[50, 100, 150, 200, 250, 300]:\n    for windowSize in [5, 10, 15, 20]: #, 15, 20\n        for TOPN in [25, 50, 100]:#[100, 150, 200]:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "documentation": {}
    },
    {
        "label": "vectorSize",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "peekOfCode": "vectorSize = 50\nwindowSize = 5\ncbow_mean = 0\ndownsampling = 1e-3\nstops = loadStopWords()##set(stopwords.words(\"english\"))\n# for cbow_mean in [0, 1]:#[1,0]:\nfor vectorSize in [50, 100, 150, 200, 250, 300]:#[50, 100, 150, 200, 250, 300]:\n    for windowSize in [5, 10, 15, 20]: #, 15, 20\n        for TOPN in [25, 50, 100]:#[100, 150, 200]:\n##                for hfListSize in [hfListSize-100, hfListSize]:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "documentation": {}
    },
    {
        "label": "windowSize",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "peekOfCode": "windowSize = 5\ncbow_mean = 0\ndownsampling = 1e-3\nstops = loadStopWords()##set(stopwords.words(\"english\"))\n# for cbow_mean in [0, 1]:#[1,0]:\nfor vectorSize in [50, 100, 150, 200, 250, 300]:#[50, 100, 150, 200, 250, 300]:\n    for windowSize in [5, 10, 15, 20]: #, 15, 20\n        for TOPN in [25, 50, 100]:#[100, 150, 200]:\n##                for hfListSize in [hfListSize-100, hfListSize]:\n            model = gensim.models.Word2Vec(sentenceList, sg=1, min_count=min_count, size=vectorSize, window=windowSize, cbow_mean = cbow_mean,sample = downsampling)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "documentation": {}
    },
    {
        "label": "cbow_mean",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "peekOfCode": "cbow_mean = 0\ndownsampling = 1e-3\nstops = loadStopWords()##set(stopwords.words(\"english\"))\n# for cbow_mean in [0, 1]:#[1,0]:\nfor vectorSize in [50, 100, 150, 200, 250, 300]:#[50, 100, 150, 200, 250, 300]:\n    for windowSize in [5, 10, 15, 20]: #, 15, 20\n        for TOPN in [25, 50, 100]:#[100, 150, 200]:\n##                for hfListSize in [hfListSize-100, hfListSize]:\n            model = gensim.models.Word2Vec(sentenceList, sg=1, min_count=min_count, size=vectorSize, window=windowSize, cbow_mean = cbow_mean,sample = downsampling)\n##                model = gensim.models.Word2Vec(sentenceList, min_count=min_count, size=vectorSize, window=windowSize, sample = downsampling)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "documentation": {}
    },
    {
        "label": "downsampling",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "peekOfCode": "downsampling = 1e-3\nstops = loadStopWords()##set(stopwords.words(\"english\"))\n# for cbow_mean in [0, 1]:#[1,0]:\nfor vectorSize in [50, 100, 150, 200, 250, 300]:#[50, 100, 150, 200, 250, 300]:\n    for windowSize in [5, 10, 15, 20]: #, 15, 20\n        for TOPN in [25, 50, 100]:#[100, 150, 200]:\n##                for hfListSize in [hfListSize-100, hfListSize]:\n            model = gensim.models.Word2Vec(sentenceList, sg=1, min_count=min_count, size=vectorSize, window=windowSize, cbow_mean = cbow_mean,sample = downsampling)\n##                model = gensim.models.Word2Vec(sentenceList, min_count=min_count, size=vectorSize, window=windowSize, sample = downsampling)\n            ##createConcepts from model",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "documentation": {}
    },
    {
        "label": "stops",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "peekOfCode": "stops = loadStopWords()##set(stopwords.words(\"english\"))\n# for cbow_mean in [0, 1]:#[1,0]:\nfor vectorSize in [50, 100, 150, 200, 250, 300]:#[50, 100, 150, 200, 250, 300]:\n    for windowSize in [5, 10, 15, 20]: #, 15, 20\n        for TOPN in [25, 50, 100]:#[100, 150, 200]:\n##                for hfListSize in [hfListSize-100, hfListSize]:\n            model = gensim.models.Word2Vec(sentenceList, sg=1, min_count=min_count, size=vectorSize, window=windowSize, cbow_mean = cbow_mean,sample = downsampling)\n##                model = gensim.models.Word2Vec(sentenceList, min_count=min_count, size=vectorSize, window=windowSize, sample = downsampling)\n            ##createConcepts from model\n            w2vList = set()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.word2Vec_ModelIteration_Words_Oct2018",
        "documentation": {}
    },
    {
        "label": "hasNumbers",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Oct2018",
        "peekOfCode": "def hasNumbers(inputString):\n    return any(char.isdigit() for char in inputString)\ndef isNumber(inputString):\n    try:\n        float(inputString)\n        return True\n    except ValueError:\n        return False\ndef loadOriginalConceptList(filepath):\n    conceptList = list()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Oct2018",
        "documentation": {}
    },
    {
        "label": "isNumber",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Oct2018",
        "peekOfCode": "def isNumber(inputString):\n    try:\n        float(inputString)\n        return True\n    except ValueError:\n        return False\ndef loadOriginalConceptList(filepath):\n    conceptList = list()\n    rfile = open(filepath, \"r\")\n    for line in rfile:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Oct2018",
        "documentation": {}
    },
    {
        "label": "loadOriginalConceptList",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Oct2018",
        "peekOfCode": "def loadOriginalConceptList(filepath):\n    conceptList = list()\n    rfile = open(filepath, \"r\")\n    for line in rfile:\n        item = line.split(\"\\t\")[0].lower().strip()\n        if item not in conceptList:\n            conceptList.append(item)\n    rfile.close()\n    return conceptList\ndef loadConceptList(filepath):",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Oct2018",
        "documentation": {}
    },
    {
        "label": "loadConceptList",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Oct2018",
        "peekOfCode": "def loadConceptList(filepath):\n    conceptList = list()\n    rfile = open(filepath, \"r\")\n    for line in rfile:\n        items = line.lower().strip().split(\" \")\n        for item in items:\n            if item not in conceptList:\n                conceptList.append(item)\n    rfile.close()\n    return conceptList",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Oct2018",
        "documentation": {}
    },
    {
        "label": "createFileName",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Oct2018",
        "peekOfCode": "def createFileName(modelName, n=25):\n    filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/PhraseResults/\"\n    if \"RAA\" in modelName:\n        filePath1 = filePath + str(n)+ \"candidateConcepts_RAA.txt\"\n        filePath2 = filePath + str(n)+ \"candidateConceptsWDistance_RAA.txt\"\n    elif \"Forum\" in modelName:\n        filePath1 = filePath + str(n)+ \"candidateConcepts_Forum.txt\"\n        filePath2 = filePath + str(n)+ \"candidateConceptsWDistance_Forum.txt\"\n    elif \"I2B2\" in modelName:\n        filePath1 = filePath + str(n)+ \"candidateConcepts_I2B2.txt\"",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Oct2018",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Oct2018",
        "peekOfCode": "filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/newConceptList_Sept2018.txt\" # newConceptList_Sept2018 conceptList\n# conceptList = loadOriginalConceptList(filePath)\nconceptList = loadConceptList(filePath)\nprint(len(conceptList))\n    # Load pre-trained Word2Vec model.\n##model = gensim.models.Word2Vec.load(\"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/w2v_280_5.model\")\n##model= Doc2Vec.load(\"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/d2v.model\") ##d2v_250_10\n##model= Doc2Vec.load(\"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/d2v_250_10.model\")\n##vl = model.wv.vocab\n##model = gensim.models.Word2Vec.load(\"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/w2v_250_10.model\")",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Oct2018",
        "documentation": {}
    },
    {
        "label": "conceptList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Oct2018",
        "peekOfCode": "conceptList = loadConceptList(filePath)\nprint(len(conceptList))\n    # Load pre-trained Word2Vec model.\n##model = gensim.models.Word2Vec.load(\"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/w2v_280_5.model\")\n##model= Doc2Vec.load(\"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/d2v.model\") ##d2v_250_10\n##model= Doc2Vec.load(\"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/d2v_250_10.model\")\n##vl = model.wv.vocab\n##model = gensim.models.Word2Vec.load(\"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/w2v_250_10.model\")\n##vl = model.wv.vocab\nmodel = gensim.models.KeyedVectors.load_word2vec_format('/Users/sarahmasudpreum/src/word2vecModels/GoogleNews-vectors-negative300.bin', binary=True)  ",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Oct2018",
        "documentation": {}
    },
    {
        "label": "##model",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Oct2018",
        "peekOfCode": "##model = gensim.models.Word2Vec.load(\"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/w2v_280_5.model\")\n##model= Doc2Vec.load(\"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/d2v.model\") ##d2v_250_10\n##model= Doc2Vec.load(\"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/d2v_250_10.model\")\n##vl = model.wv.vocab\n##model = gensim.models.Word2Vec.load(\"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/w2v_250_10.model\")\n##vl = model.wv.vocab\nmodel = gensim.models.KeyedVectors.load_word2vec_format('/Users/sarahmasudpreum/src/word2vecModels/GoogleNews-vectors-negative300.bin', binary=True)  \n# word2vec_RAA_Data_150_10 word2vec_Forum_Data_150_10 word2vec_MIMIC3_Data_150_10 word2vec_I2B2_Data_150_10\n# word2vec_Med_Data_150_10 word2vec_EMS_Data_150_10 word2vec_MedEMS_Data_150_10\n# model = gensim.models.Word2Vec.load('/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/wordEmbeddingModel/Model/word2vec_MedEMS_Data_150_10.model')  ",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Oct2018",
        "documentation": {}
    },
    {
        "label": "##vl",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Oct2018",
        "peekOfCode": "##vl = model.wv.vocab\n##model = gensim.models.Word2Vec.load(\"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/w2v_250_10.model\")\n##vl = model.wv.vocab\nmodel = gensim.models.KeyedVectors.load_word2vec_format('/Users/sarahmasudpreum/src/word2vecModels/GoogleNews-vectors-negative300.bin', binary=True)  \n# word2vec_RAA_Data_150_10 word2vec_Forum_Data_150_10 word2vec_MIMIC3_Data_150_10 word2vec_I2B2_Data_150_10\n# word2vec_Med_Data_150_10 word2vec_EMS_Data_150_10 word2vec_MedEMS_Data_150_10\n# model = gensim.models.Word2Vec.load('/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/wordEmbeddingModel/Model/word2vec_MedEMS_Data_150_10.model')  \nvl = model.vocab ##model.vocab (for google) model.wv.vocab\nf1, f2 = createFileName(\"Google\") \n## RAA Forum MIMIC3 I2B2 Med EMS all Google Glove",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Oct2018",
        "documentation": {}
    },
    {
        "label": "##model",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Oct2018",
        "peekOfCode": "##model = gensim.models.Word2Vec.load(\"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/w2v_250_10.model\")\n##vl = model.wv.vocab\nmodel = gensim.models.KeyedVectors.load_word2vec_format('/Users/sarahmasudpreum/src/word2vecModels/GoogleNews-vectors-negative300.bin', binary=True)  \n# word2vec_RAA_Data_150_10 word2vec_Forum_Data_150_10 word2vec_MIMIC3_Data_150_10 word2vec_I2B2_Data_150_10\n# word2vec_Med_Data_150_10 word2vec_EMS_Data_150_10 word2vec_MedEMS_Data_150_10\n# model = gensim.models.Word2Vec.load('/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/wordEmbeddingModel/Model/word2vec_MedEMS_Data_150_10.model')  \nvl = model.vocab ##model.vocab (for google) model.wv.vocab\nf1, f2 = createFileName(\"Google\") \n## RAA Forum MIMIC3 I2B2 Med EMS all Google Glove\nwfile = open(f1, \"w\")",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Oct2018",
        "documentation": {}
    },
    {
        "label": "##vl",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Oct2018",
        "peekOfCode": "##vl = model.wv.vocab\nmodel = gensim.models.KeyedVectors.load_word2vec_format('/Users/sarahmasudpreum/src/word2vecModels/GoogleNews-vectors-negative300.bin', binary=True)  \n# word2vec_RAA_Data_150_10 word2vec_Forum_Data_150_10 word2vec_MIMIC3_Data_150_10 word2vec_I2B2_Data_150_10\n# word2vec_Med_Data_150_10 word2vec_EMS_Data_150_10 word2vec_MedEMS_Data_150_10\n# model = gensim.models.Word2Vec.load('/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/wordEmbeddingModel/Model/word2vec_MedEMS_Data_150_10.model')  \nvl = model.vocab ##model.vocab (for google) model.wv.vocab\nf1, f2 = createFileName(\"Google\") \n## RAA Forum MIMIC3 I2B2 Med EMS all Google Glove\nwfile = open(f1, \"w\")\nwfile2 = open(f2, \"w\")",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Oct2018",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Oct2018",
        "peekOfCode": "model = gensim.models.KeyedVectors.load_word2vec_format('/Users/sarahmasudpreum/src/word2vecModels/GoogleNews-vectors-negative300.bin', binary=True)  \n# word2vec_RAA_Data_150_10 word2vec_Forum_Data_150_10 word2vec_MIMIC3_Data_150_10 word2vec_I2B2_Data_150_10\n# word2vec_Med_Data_150_10 word2vec_EMS_Data_150_10 word2vec_MedEMS_Data_150_10\n# model = gensim.models.Word2Vec.load('/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/wordEmbeddingModel/Model/word2vec_MedEMS_Data_150_10.model')  \nvl = model.vocab ##model.vocab (for google) model.wv.vocab\nf1, f2 = createFileName(\"Google\") \n## RAA Forum MIMIC3 I2B2 Med EMS all Google Glove\nwfile = open(f1, \"w\")\nwfile2 = open(f2, \"w\")\ncandidateList = list()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Oct2018",
        "documentation": {}
    },
    {
        "label": "vl",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Oct2018",
        "peekOfCode": "vl = model.vocab ##model.vocab (for google) model.wv.vocab\nf1, f2 = createFileName(\"Google\") \n## RAA Forum MIMIC3 I2B2 Med EMS all Google Glove\nwfile = open(f1, \"w\")\nwfile2 = open(f2, \"w\")\ncandidateList = list()\nTOPN = 20\nfor concept in conceptList:\n    if concept in vl:        \n        candidateList = model.most_similar(concept, topn = TOPN)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Oct2018",
        "documentation": {}
    },
    {
        "label": "wfile",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Oct2018",
        "peekOfCode": "wfile = open(f1, \"w\")\nwfile2 = open(f2, \"w\")\ncandidateList = list()\nTOPN = 20\nfor concept in conceptList:\n    if concept in vl:        \n        candidateList = model.most_similar(concept, topn = TOPN)\n        if len(candidateList) > 0:\n            wfile.write(concept+\"\\t\")\n            wfile2.write(concept+\"\\t\")",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Oct2018",
        "documentation": {}
    },
    {
        "label": "wfile2",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Oct2018",
        "peekOfCode": "wfile2 = open(f2, \"w\")\ncandidateList = list()\nTOPN = 20\nfor concept in conceptList:\n    if concept in vl:        \n        candidateList = model.most_similar(concept, topn = TOPN)\n        if len(candidateList) > 0:\n            wfile.write(concept+\"\\t\")\n            wfile2.write(concept+\"\\t\")\n        for item in candidateList:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Oct2018",
        "documentation": {}
    },
    {
        "label": "candidateList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Oct2018",
        "peekOfCode": "candidateList = list()\nTOPN = 20\nfor concept in conceptList:\n    if concept in vl:        \n        candidateList = model.most_similar(concept, topn = TOPN)\n        if len(candidateList) > 0:\n            wfile.write(concept+\"\\t\")\n            wfile2.write(concept+\"\\t\")\n        for item in candidateList:\n            if not isNumber(item[0]) and item[0].find(\"numberToken\") is -1: ",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Oct2018",
        "documentation": {}
    },
    {
        "label": "TOPN",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Oct2018",
        "peekOfCode": "TOPN = 20\nfor concept in conceptList:\n    if concept in vl:        \n        candidateList = model.most_similar(concept, topn = TOPN)\n        if len(candidateList) > 0:\n            wfile.write(concept+\"\\t\")\n            wfile2.write(concept+\"\\t\")\n        for item in candidateList:\n            if not isNumber(item[0]) and item[0].find(\"numberToken\") is -1: \n                wfile.write(str(item[0])+\"\\t\")",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Oct2018",
        "documentation": {}
    },
    {
        "label": "##model",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Oct2018",
        "peekOfCode": "##model = gensim.models.KeyedVectors.load_word2vec_format('/Users/sarahmasudpreum/src/word2vecModels/vec_alli2b2Data.bin', binary=True)  \n##print(\"Does it include the stop words like \\'a\\', \\'and\\', \\'the\\'? %d %d %d\" % ('a' in model.vocab, 'and' in model.vocab, 'the' in model.vocab))\n##model.wv.most_similar(positive=['woman', 'king'], negative=['man'])",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Oct2018",
        "documentation": {}
    },
    {
        "label": "hasNumbers",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "def hasNumbers(inputString):\n    return any(char.isdigit() for char in inputString)\ndef hasRedundantToken(inputString):\n    tokens = inputString.split(\" \")\n    for token in tokens:\n        if token.find(\"patient\") is not -1 or token.find(\"pt\") is not -1 or len(token)==1:\n            return True\n    return False\ndef hasSpecialCharacter(inputString):\n    # len(set('[~!@#$%^&*()_-+{}<>\"=:,.;?/|\\']+$').intersection(w)) == 0",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "hasRedundantToken",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "def hasRedundantToken(inputString):\n    tokens = inputString.split(\" \")\n    for token in tokens:\n        if token.find(\"patient\") is not -1 or token.find(\"pt\") is not -1 or len(token)==1:\n            return True\n    return False\ndef hasSpecialCharacter(inputString):\n    # len(set('[~!@#$%^&*()_-+{}<>\"=:,.;?/|\\']+$').intersection(w)) == 0\n    temp = re.sub(\"[^a-zA-Z0-9 ]\",\"*\", inputString)\n    if len(set('*').intersection(temp)) > 0:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "hasSpecialCharacter",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "def hasSpecialCharacter(inputString):\n    # len(set('[~!@#$%^&*()_-+{}<>\"=:,.;?/|\\']+$').intersection(w)) == 0\n    temp = re.sub(\"[^a-zA-Z0-9 ]\",\"*\", inputString)\n    if len(set('*').intersection(temp)) > 0:\n        return True\n    return False\ndef isNumber(inputString):\n    try:\n        float(inputString)\n        return True",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "isNumber",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "def isNumber(inputString):\n    try:\n        float(inputString)\n        return True\n    except ValueError:\n        return False\ndef hasStopwords(inputString):\n    stops = loadStopWords()\n    tokens = inputString.split(\" \")\n    for item in stops:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "hasStopwords",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "def hasStopwords(inputString):\n    stops = loadStopWords()\n    tokens = inputString.split(\" \")\n    for item in stops:\n        if item in tokens:\n            return True\n    return False\ndef loadData(filepath, remove_stopwords=False):\n##    dataList = list()\n    dataList = [[] for x in range(11891)]",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "loadData",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "def loadData(filepath, remove_stopwords=False):\n##    dataList = list()\n    dataList = [[] for x in range(11891)]\n    rfile = open(filepath, \"r\")\n    for line in rfile:\n        line2 = line.lower().strip()\n        line2 = re.sub(\"[^a-zA-Z0-9_]\",\" \", line2)\n        innerList = line2.split()\n        if remove_stopwords:\n            stops = loadStopWords()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "loadStringData",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "def loadStringData(filepath):\n##    dataList = list()\n##    dataList = [[] for x in range(11891)]\n    dataString = \"\"\n    rfile = open(filepath, \"r\")\n    for line in rfile:\n        line2 = line.lower().strip()\n        line2 = re.sub(\"[^a-zA-Z0-9_]\",\" \", line2)\n        dataString += \" \"+ line2\n    rfile.close()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "loadStringData2",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "def loadStringData2(filepath):\n##    dataList = list()\n##    dataList = [[] for x in range(11891)]\n    dataString = \"\"\n    rfile = open(filepath, \"r\")\n    for line in rfile:\n        line = line.lower()\n        dataString += line\n    rfile.close()\n    return dataString",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "loadStopWords",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "def loadStopWords():\n    filePath = \"/Users/sarahmasudpreum/Desktop/PythonTCode/general/stopwords.txt\"\n    rfile = open(filePath, \"r\")\n    stopWords = list()\n    for line in rfile:\n        stopWords.append(line.strip())\n    rfile.close()\n    return stopWords\ndef loadOriginalConceptListPhrase(filepath):\n    conceptList = list()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "loadOriginalConceptListPhrase",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "def loadOriginalConceptListPhrase(filepath):\n    conceptList = list()\n    rfile = open(filepath, \"r\")\n    for line in rfile:\n        item = line.strip()\n        if item not in conceptList:\n            conceptList.append(item)\n    rfile.close()\n    print(conceptList)\n    return conceptList",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "loadOriginalConceptList",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "def loadOriginalConceptList(filepath):\n    conceptList = list()\n    rfile = open(filepath, \"r\")\n    for line in rfile:\n        concept = line.split(\"\\t\")[0].lower().strip()\n        items = concept.split(\" \")\n        for item in items: \n            if item not in conceptList:\n                conceptList.append(item)\n    stops = loadStopWords()##set(stopwords.words(\"english\"))",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "loadSimpleListPhras",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "def loadSimpleListPhrase (filePath):\n    rfile = open(filePath, \"r\")\n    itemList = list()\n##    table = str.maketrans({key: None for key in string.punctuation})\n    for line in rfile:\n##        line = re.sub(\"[^a-zA-Z0-9_]\",\" \", line)\n        item = line.lower().strip()\n        if item not in itemList:\n            itemList.append(item)\n    rfile.close()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "computeFrequenc",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "def computeFrequency (gtConceptList, frequencyList, wordList,fileName=\"\"):\n    wordFrequencyList = list(repeat(0,len(gtConceptList)))\n    for item in wordList:\n        for i in range (0,len(gtConceptList)):\n            gTerm = str(gtConceptList[i]).strip()\n            item = item.strip()\n##            if len(gTerm.split(\" \")) > 1:\n            if item.find(gTerm) is not -1: #or gTerm.find(item) is not -1:\n                wordFrequencyList[i] = frequencyList[i]\n    # filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/candidatePhrases/\"",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "computeF",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "def computeFP (filePath, conceptList, gtConceptList):\n##    filteredConceptList = [x.strip() for x in conceptList if x not in gtConceptList]\n    tpConceptList = list()\n    for item in conceptList:\n        for i in range (0,len(gtConceptList)):\n            gTerm = str(gtConceptList[i]).strip()\n            item = item.strip()\n##            if len(gTerm.split(\" \")) > 1:\n            if item.find(gTerm) is not -1: #or gTerm.find(item) is not -1:\n                tpConceptList.append(item)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "computePrecisio",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "def computePrecision (TPlist, FPList):\n    t = sum(TPlist) + sum(FPList)\n    p = float(0)\n    if t>0:\n        p = sum(TPlist)/float( t )\n    return round( p,4 )\ndef computeRecall (TPlist, denom):\n    p = sum(TPlist)/float(denom)\n    return round( p,4 )\ndef score(seq1, seq2, model):",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "computeRecal",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "def computeRecall (TPlist, denom):\n    p = sum(TPlist)/float(denom)\n    return round( p,4 )\ndef score(seq1, seq2, model):\n\t'''prepares inputs for scoring'''\n\tseq1_word_list = word_tokenize(seq1.strip().lower())\n\tseq2_word_list = word_tokenize(seq2.strip().lower())\n\treturn sim_score(seq1_word_list, seq2_word_list,model)\ndef sim_score(wordlist1, wordlist2,model):\n\t'''calculates average maximum similarity between two phrase inputs'''",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "score",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "def score(seq1, seq2, model):\n\t'''prepares inputs for scoring'''\n\tseq1_word_list = word_tokenize(seq1.strip().lower())\n\tseq2_word_list = word_tokenize(seq2.strip().lower())\n\treturn sim_score(seq1_word_list, seq2_word_list,model)\ndef sim_score(wordlist1, wordlist2,model):\n\t'''calculates average maximum similarity between two phrase inputs'''\n\tmaxes = []\n\tfor word in wordlist1:\n\t\tcur_max = 0",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "sim_score",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "def sim_score(wordlist1, wordlist2,model):\n\t'''calculates average maximum similarity between two phrase inputs'''\n\tmaxes = []\n\tfor word in wordlist1:\n\t\tcur_max = 0\n\t\tfor word2 in wordlist2:\n\t\t\tif word == word2: #case where words are identical\n\t\t\t\tsim = 1\n\t\t\t\tcur_max = sim\n\t\t\telif word in model and word2 in model:\t",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "sim_sum_score",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "def sim_sum_score(wordlist1, wordlist2,model):\n    '''calculates similarity between two phrase inputs'''\n    x1 = np.zeros(100)\n    x2 = np.zeros(100)\n    for word in wordlist1:\n        if word in model:\n            x1 = x1 + model[word]\n    if len(wordlist1) > 0:\n        x1 = x1/len(wordlist1)\n    for word in wordlist2:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "sim_avg_score",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "def sim_avg_score(wordlist1, wordlist2,model):\n\tavgdist = []\n\tfor word in wordlist1:\n            cur_dist = 0\n            for word2 in wordlist2:\n                if word == word2:\n                    sim = 1\n                    cur_dist += sim\n                elif word in model and word2 in model:\t\n                    sim = model.similarity(word, word2) #calculate cosine similarity score for words",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "model_most_similar_phrases",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "def model_most_similar_phrases(model, inputSpace, seedConcept, topn):\n    candidateSet = set()\n    scoreList = list(repeat(0,len(inputSpace)))\n    for i in range(0,len(inputSpace)):\n        ip = inputSpace[i]\n        scoreList[i] = score(ip, seedConcept, model)\n    finalList = list(zip(inputSpace, scoreList))\n    finalList.sort(key=lambda x: x[1], reverse=True)\n    finalList = finalList[:topn]\n    return finalList",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "createFileName",
        "kind": 2,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "def createFileName(modelName, n=25):\n    filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/Oct2018/ODEMSA/Phrase/\"\n    if \"RAA\" in modelName:\n        filePath1 = filePath + str(n)+ \"candidateConcepts_RAA.txt\"\n        filePath2 = filePath + str(n)+ \"candidateConceptsWDistance_RAA.txt\"\n    elif \"Forum\" in modelName:\n        filePath1 = filePath + str(n)+ \"candidateConcepts_Forum.txt\"\n        filePath2 = filePath + str(n)+ \"candidateConceptsWDistance_Forum.txt\"\n    elif \"I2B2\" in modelName:\n        filePath1 = filePath + str(n)+ \"candidateConcepts_I2B2.txt\"",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "\tseq1_word_list",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "\tseq1_word_list = word_tokenize(seq1.strip().lower())\n\tseq2_word_list = word_tokenize(seq2.strip().lower())\n\treturn sim_score(seq1_word_list, seq2_word_list,model)\ndef sim_score(wordlist1, wordlist2,model):\n\t'''calculates average maximum similarity between two phrase inputs'''\n\tmaxes = []\n\tfor word in wordlist1:\n\t\tcur_max = 0\n\t\tfor word2 in wordlist2:\n\t\t\tif word == word2: #case where words are identical",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "\tseq2_word_list",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "\tseq2_word_list = word_tokenize(seq2.strip().lower())\n\treturn sim_score(seq1_word_list, seq2_word_list,model)\ndef sim_score(wordlist1, wordlist2,model):\n\t'''calculates average maximum similarity between two phrase inputs'''\n\tmaxes = []\n\tfor word in wordlist1:\n\t\tcur_max = 0\n\t\tfor word2 in wordlist2:\n\t\t\tif word == word2: #case where words are identical\n\t\t\t\tsim = 1",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "\tmaxes",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "\tmaxes = []\n\tfor word in wordlist1:\n\t\tcur_max = 0\n\t\tfor word2 in wordlist2:\n\t\t\tif word == word2: #case where words are identical\n\t\t\t\tsim = 1\n\t\t\t\tcur_max = sim\n\t\t\telif word in model and word2 in model:\t\n\t\t\t\tsim = model.similarity(word, word2) #calculate cosine similarity score for words\n\t\t\t\tif sim > cur_max:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "\t\tcur_max",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "\t\tcur_max = 0\n\t\tfor word2 in wordlist2:\n\t\t\tif word == word2: #case where words are identical\n\t\t\t\tsim = 1\n\t\t\t\tcur_max = sim\n\t\t\telif word in model and word2 in model:\t\n\t\t\t\tsim = model.similarity(word, word2) #calculate cosine similarity score for words\n\t\t\t\tif sim > cur_max:\n\t\t\t\t\tcur_max = sim\n\t\tif cur_max != 0:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "\t\t\t\tsim",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "\t\t\t\tsim = 1\n\t\t\t\tcur_max = sim\n\t\t\telif word in model and word2 in model:\t\n\t\t\t\tsim = model.similarity(word, word2) #calculate cosine similarity score for words\n\t\t\t\tif sim > cur_max:\n\t\t\t\t\tcur_max = sim\n\t\tif cur_max != 0:\n\t\t\tmaxes.append(cur_max)\n\tif sum(maxes) == 0:\n\t    return 0",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "\t\t\t\tcur_max",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "\t\t\t\tcur_max = sim\n\t\t\telif word in model and word2 in model:\t\n\t\t\t\tsim = model.similarity(word, word2) #calculate cosine similarity score for words\n\t\t\t\tif sim > cur_max:\n\t\t\t\t\tcur_max = sim\n\t\tif cur_max != 0:\n\t\t\tmaxes.append(cur_max)\n\tif sum(maxes) == 0:\n\t    return 0\n\treturn float(sum(maxes)) / len(maxes)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "\t\t\t\tsim",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "\t\t\t\tsim = model.similarity(word, word2) #calculate cosine similarity score for words\n\t\t\t\tif sim > cur_max:\n\t\t\t\t\tcur_max = sim\n\t\tif cur_max != 0:\n\t\t\tmaxes.append(cur_max)\n\tif sum(maxes) == 0:\n\t    return 0\n\treturn float(sum(maxes)) / len(maxes)\ndef sim_sum_score(wordlist1, wordlist2,model):\n    '''calculates similarity between two phrase inputs'''",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "\t\t\t\t\tcur_max",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "\t\t\t\t\tcur_max = sim\n\t\tif cur_max != 0:\n\t\t\tmaxes.append(cur_max)\n\tif sum(maxes) == 0:\n\t    return 0\n\treturn float(sum(maxes)) / len(maxes)\ndef sim_sum_score(wordlist1, wordlist2,model):\n    '''calculates similarity between two phrase inputs'''\n    x1 = np.zeros(100)\n    x2 = np.zeros(100)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "\tavgdist",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "\tavgdist = []\n\tfor word in wordlist1:\n            cur_dist = 0\n            for word2 in wordlist2:\n                if word == word2:\n                    sim = 1\n                    cur_dist += sim\n                elif word in model and word2 in model:\t\n                    sim = model.similarity(word, word2) #calculate cosine similarity score for words\n                    cur_dist += sim",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "datafileRootPath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "datafileRootPath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/wordEmbeddingModel/Data/\"\ndataFileName = \"EMSallDataIn1.txt\" #EMSallDataIn1 MedallDataIn1 MedEMSallDataIn1\n# filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/trainingData/DataLines.txt\" \nfilePath = datafileRootPath + dataFileName\ndataString = loadStringData2(filePath)\n# dataString = re.sub(\"[^a-zA-Z0-9]\",\" \", dataString)\n##        tokens = line.split(\" \")\n##        tokens = [w for w in tokens if w.find(\" \") is -1 and not w.isdigit() and not w in stops]\n##        line = \" \".join(tokens)\nsent_tokenize_list = sent_tokenize(dataString)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "dataFileName",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "dataFileName = \"EMSallDataIn1.txt\" #EMSallDataIn1 MedallDataIn1 MedEMSallDataIn1\n# filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/trainingData/DataLines.txt\" \nfilePath = datafileRootPath + dataFileName\ndataString = loadStringData2(filePath)\n# dataString = re.sub(\"[^a-zA-Z0-9]\",\" \", dataString)\n##        tokens = line.split(\" \")\n##        tokens = [w for w in tokens if w.find(\" \") is -1 and not w.isdigit() and not w in stops]\n##        line = \" \".join(tokens)\nsent_tokenize_list = sent_tokenize(dataString)\nprint(str(len(sent_tokenize_list)))",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "filePath = datafileRootPath + dataFileName\ndataString = loadStringData2(filePath)\n# dataString = re.sub(\"[^a-zA-Z0-9]\",\" \", dataString)\n##        tokens = line.split(\" \")\n##        tokens = [w for w in tokens if w.find(\" \") is -1 and not w.isdigit() and not w in stops]\n##        line = \" \".join(tokens)\nsent_tokenize_list = sent_tokenize(dataString)\nprint(str(len(sent_tokenize_list)))\n# tokens = nltk.wordpunct_tokenize(dataString) #word_tokenize\ntokens = word_tokenize(dataString) #word_tokenize",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "dataString",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "dataString = loadStringData2(filePath)\n# dataString = re.sub(\"[^a-zA-Z0-9]\",\" \", dataString)\n##        tokens = line.split(\" \")\n##        tokens = [w for w in tokens if w.find(\" \") is -1 and not w.isdigit() and not w in stops]\n##        line = \" \".join(tokens)\nsent_tokenize_list = sent_tokenize(dataString)\nprint(str(len(sent_tokenize_list)))\n# tokens = nltk.wordpunct_tokenize(dataString) #word_tokenize\ntokens = word_tokenize(dataString) #word_tokenize\nprint(str(len(tokens)))",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "sent_tokenize_list",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "sent_tokenize_list = sent_tokenize(dataString)\nprint(str(len(sent_tokenize_list)))\n# tokens = nltk.wordpunct_tokenize(dataString) #word_tokenize\ntokens = word_tokenize(dataString) #word_tokenize\nprint(str(len(tokens)))\ntokens = [w for w in tokens if not w.isdigit()]\nunigramList = list(set(tokens))\nstops = loadStopWords()\n        # filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/newConceptList_November2018.txt\" # newConceptList_Sept2018 conceptList\n        # conceptList = loadOriginalConceptList(filePath)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "tokens = word_tokenize(dataString) #word_tokenize\nprint(str(len(tokens)))\ntokens = [w for w in tokens if not w.isdigit()]\nunigramList = list(set(tokens))\nstops = loadStopWords()\n        # filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/newConceptList_November2018.txt\" # newConceptList_Sept2018 conceptList\n        # conceptList = loadOriginalConceptList(filePath)\n        # conceptList = [w for w in conceptList if w.find(\" \") is -1]\nfinder = BigramCollocationFinder.from_words(tokens)\nbigram_measures = nltk.collocations.BigramAssocMeasures()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "tokens = [w for w in tokens if not w.isdigit()]\nunigramList = list(set(tokens))\nstops = loadStopWords()\n        # filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/newConceptList_November2018.txt\" # newConceptList_Sept2018 conceptList\n        # conceptList = loadOriginalConceptList(filePath)\n        # conceptList = [w for w in conceptList if w.find(\" \") is -1]\nfinder = BigramCollocationFinder.from_words(tokens)\nbigram_measures = nltk.collocations.BigramAssocMeasures()\n##sorted(finder.nbest(bigram_measures.raw_freq, 100))\nscored = finder.score_ngrams(bigram_measures.raw_freq)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "unigramList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "unigramList = list(set(tokens))\nstops = loadStopWords()\n        # filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/newConceptList_November2018.txt\" # newConceptList_Sept2018 conceptList\n        # conceptList = loadOriginalConceptList(filePath)\n        # conceptList = [w for w in conceptList if w.find(\" \") is -1]\nfinder = BigramCollocationFinder.from_words(tokens)\nbigram_measures = nltk.collocations.BigramAssocMeasures()\n##sorted(finder.nbest(bigram_measures.raw_freq, 100))\nscored = finder.score_ngrams(bigram_measures.raw_freq)\nbigramSet = set(sorted(bigram for bigram, score in scored))  ",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "stops",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "stops = loadStopWords()\n        # filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/newConceptList_November2018.txt\" # newConceptList_Sept2018 conceptList\n        # conceptList = loadOriginalConceptList(filePath)\n        # conceptList = [w for w in conceptList if w.find(\" \") is -1]\nfinder = BigramCollocationFinder.from_words(tokens)\nbigram_measures = nltk.collocations.BigramAssocMeasures()\n##sorted(finder.nbest(bigram_measures.raw_freq, 100))\nscored = finder.score_ngrams(bigram_measures.raw_freq)\nbigramSet = set(sorted(bigram for bigram, score in scored))  \nbigramList = list(bigramSet)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "finder",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "finder = BigramCollocationFinder.from_words(tokens)\nbigram_measures = nltk.collocations.BigramAssocMeasures()\n##sorted(finder.nbest(bigram_measures.raw_freq, 100))\nscored = finder.score_ngrams(bigram_measures.raw_freq)\nbigramSet = set(sorted(bigram for bigram, score in scored))  \nbigramList = list(bigramSet)\nlen(bigramList)\nfiltBigramList = list()\n# filtBigramList= [u+\" \"+v for u,v in bigramList if not u in stops and not v in stops and (u in conceptList or v in conceptList)]\nfiltBigramList= [u+\" \"+v for u,v in bigramList if not u in stops and not v in stops]",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "bigram_measures",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "bigram_measures = nltk.collocations.BigramAssocMeasures()\n##sorted(finder.nbest(bigram_measures.raw_freq, 100))\nscored = finder.score_ngrams(bigram_measures.raw_freq)\nbigramSet = set(sorted(bigram for bigram, score in scored))  \nbigramList = list(bigramSet)\nlen(bigramList)\nfiltBigramList = list()\n# filtBigramList= [u+\" \"+v for u,v in bigramList if not u in stops and not v in stops and (u in conceptList or v in conceptList)]\nfiltBigramList= [u+\" \"+v for u,v in bigramList if not u in stops and not v in stops]\nlen(filtBigramList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "scored",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "scored = finder.score_ngrams(bigram_measures.raw_freq)\nbigramSet = set(sorted(bigram for bigram, score in scored))  \nbigramList = list(bigramSet)\nlen(bigramList)\nfiltBigramList = list()\n# filtBigramList= [u+\" \"+v for u,v in bigramList if not u in stops and not v in stops and (u in conceptList or v in conceptList)]\nfiltBigramList= [u+\" \"+v for u,v in bigramList if not u in stops and not v in stops]\nlen(filtBigramList)\nfiltBigramList= [w for w in filtBigramList if not hasSpecialCharacter(w) and not hasRedundantToken(w)]\n# filtBigramList= [w for w in filtBigramList if len(set('[~!@#$%^&*()_-+\\{}<>\"=\\\\\\:,.;?/|\\']+$').intersection(w)) == 0 ]",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "bigramSet",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "bigramSet = set(sorted(bigram for bigram, score in scored))  \nbigramList = list(bigramSet)\nlen(bigramList)\nfiltBigramList = list()\n# filtBigramList= [u+\" \"+v for u,v in bigramList if not u in stops and not v in stops and (u in conceptList or v in conceptList)]\nfiltBigramList= [u+\" \"+v for u,v in bigramList if not u in stops and not v in stops]\nlen(filtBigramList)\nfiltBigramList= [w for w in filtBigramList if not hasSpecialCharacter(w) and not hasRedundantToken(w)]\n# filtBigramList= [w for w in filtBigramList if len(set('[~!@#$%^&*()_-+\\{}<>\"=\\\\\\:,.;?/|\\']+$').intersection(w)) == 0 ]\n# filtBigramList= [w for w in filtBigramList if w.find(\"patient\") is -1 and w.find(\" pt\") is -1]",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "bigramList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "bigramList = list(bigramSet)\nlen(bigramList)\nfiltBigramList = list()\n# filtBigramList= [u+\" \"+v for u,v in bigramList if not u in stops and not v in stops and (u in conceptList or v in conceptList)]\nfiltBigramList= [u+\" \"+v for u,v in bigramList if not u in stops and not v in stops]\nlen(filtBigramList)\nfiltBigramList= [w for w in filtBigramList if not hasSpecialCharacter(w) and not hasRedundantToken(w)]\n# filtBigramList= [w for w in filtBigramList if len(set('[~!@#$%^&*()_-+\\{}<>\"=\\\\\\:,.;?/|\\']+$').intersection(w)) == 0 ]\n# filtBigramList= [w for w in filtBigramList if w.find(\"patient\") is -1 and w.find(\" pt\") is -1]\nlen(filtBigramList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "filtBigramList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "filtBigramList = list()\n# filtBigramList= [u+\" \"+v for u,v in bigramList if not u in stops and not v in stops and (u in conceptList or v in conceptList)]\nfiltBigramList= [u+\" \"+v for u,v in bigramList if not u in stops and not v in stops]\nlen(filtBigramList)\nfiltBigramList= [w for w in filtBigramList if not hasSpecialCharacter(w) and not hasRedundantToken(w)]\n# filtBigramList= [w for w in filtBigramList if len(set('[~!@#$%^&*()_-+\\{}<>\"=\\\\\\:,.;?/|\\']+$').intersection(w)) == 0 ]\n# filtBigramList= [w for w in filtBigramList if w.find(\"patient\") is -1 and w.find(\" pt\") is -1]\nlen(filtBigramList)\nfiltBigramList = list(set(filtBigramList))\nfinder = TrigramCollocationFinder.from_words(tokens)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "filtBigramList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "filtBigramList = list(set(filtBigramList))\nfinder = TrigramCollocationFinder.from_words(tokens)\ntrigram_measures = nltk.collocations.TrigramAssocMeasures()\nscored = finder.score_ngrams(trigram_measures.raw_freq)\ntrigramSet = set(sorted(trigram for trigram, score in scored))  \ntrigramList = list(trigramSet)\nlen(trigramList)\nfiltTrigramList = list()\n# filtTrigramList= [u+\" \"+v+\" \"+w for u,v,w in trigramList if not u in stops and not v in stops and not w in stops and (u in conceptList or v in conceptList or w in conceptList)]\nfiltTrigramList= [u+\" \"+v+\" \"+w for u,v,w in trigramList if not u in stops and not v in stops and not w in stops]",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "finder",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "finder = TrigramCollocationFinder.from_words(tokens)\ntrigram_measures = nltk.collocations.TrigramAssocMeasures()\nscored = finder.score_ngrams(trigram_measures.raw_freq)\ntrigramSet = set(sorted(trigram for trigram, score in scored))  \ntrigramList = list(trigramSet)\nlen(trigramList)\nfiltTrigramList = list()\n# filtTrigramList= [u+\" \"+v+\" \"+w for u,v,w in trigramList if not u in stops and not v in stops and not w in stops and (u in conceptList or v in conceptList or w in conceptList)]\nfiltTrigramList= [u+\" \"+v+\" \"+w for u,v,w in trigramList if not u in stops and not v in stops and not w in stops]\nlen(filtTrigramList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "trigram_measures",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "trigram_measures = nltk.collocations.TrigramAssocMeasures()\nscored = finder.score_ngrams(trigram_measures.raw_freq)\ntrigramSet = set(sorted(trigram for trigram, score in scored))  \ntrigramList = list(trigramSet)\nlen(trigramList)\nfiltTrigramList = list()\n# filtTrigramList= [u+\" \"+v+\" \"+w for u,v,w in trigramList if not u in stops and not v in stops and not w in stops and (u in conceptList or v in conceptList or w in conceptList)]\nfiltTrigramList= [u+\" \"+v+\" \"+w for u,v,w in trigramList if not u in stops and not v in stops and not w in stops]\nlen(filtTrigramList)\nfiltTrigramList= [w for w in filtTrigramList if not hasSpecialCharacter(w) and not hasRedundantToken(w)]",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "scored",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "scored = finder.score_ngrams(trigram_measures.raw_freq)\ntrigramSet = set(sorted(trigram for trigram, score in scored))  \ntrigramList = list(trigramSet)\nlen(trigramList)\nfiltTrigramList = list()\n# filtTrigramList= [u+\" \"+v+\" \"+w for u,v,w in trigramList if not u in stops and not v in stops and not w in stops and (u in conceptList or v in conceptList or w in conceptList)]\nfiltTrigramList= [u+\" \"+v+\" \"+w for u,v,w in trigramList if not u in stops and not v in stops and not w in stops]\nlen(filtTrigramList)\nfiltTrigramList= [w for w in filtTrigramList if not hasSpecialCharacter(w) and not hasRedundantToken(w)]\n# filtTrigramList= [w for w in filtTrigramList if len(set('[~!@#$%^&*()_-+{}<>\"=:,.;?/|\\']+$').intersection(w)) == 0 ]",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "trigramSet",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "trigramSet = set(sorted(trigram for trigram, score in scored))  \ntrigramList = list(trigramSet)\nlen(trigramList)\nfiltTrigramList = list()\n# filtTrigramList= [u+\" \"+v+\" \"+w for u,v,w in trigramList if not u in stops and not v in stops and not w in stops and (u in conceptList or v in conceptList or w in conceptList)]\nfiltTrigramList= [u+\" \"+v+\" \"+w for u,v,w in trigramList if not u in stops and not v in stops and not w in stops]\nlen(filtTrigramList)\nfiltTrigramList= [w for w in filtTrigramList if not hasSpecialCharacter(w) and not hasRedundantToken(w)]\n# filtTrigramList= [w for w in filtTrigramList if len(set('[~!@#$%^&*()_-+{}<>\"=:,.;?/|\\']+$').intersection(w)) == 0 ]\n# filtTrigramList = [w for w in filtTrigramList if w.find(\"patient\") is -1 and w.find(\" pt\") is -1] ",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "trigramList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "trigramList = list(trigramSet)\nlen(trigramList)\nfiltTrigramList = list()\n# filtTrigramList= [u+\" \"+v+\" \"+w for u,v,w in trigramList if not u in stops and not v in stops and not w in stops and (u in conceptList or v in conceptList or w in conceptList)]\nfiltTrigramList= [u+\" \"+v+\" \"+w for u,v,w in trigramList if not u in stops and not v in stops and not w in stops]\nlen(filtTrigramList)\nfiltTrigramList= [w for w in filtTrigramList if not hasSpecialCharacter(w) and not hasRedundantToken(w)]\n# filtTrigramList= [w for w in filtTrigramList if len(set('[~!@#$%^&*()_-+{}<>\"=:,.;?/|\\']+$').intersection(w)) == 0 ]\n# filtTrigramList = [w for w in filtTrigramList if w.find(\"patient\") is -1 and w.find(\" pt\") is -1] \n##filtTrigramList= [u+\" \"+v+\" \"+w for u,v,w in trigramList if not u in stops and not w in stops and (u in conceptList or v in conceptList or w in conceptList)]",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "filtTrigramList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "filtTrigramList = list()\n# filtTrigramList= [u+\" \"+v+\" \"+w for u,v,w in trigramList if not u in stops and not v in stops and not w in stops and (u in conceptList or v in conceptList or w in conceptList)]\nfiltTrigramList= [u+\" \"+v+\" \"+w for u,v,w in trigramList if not u in stops and not v in stops and not w in stops]\nlen(filtTrigramList)\nfiltTrigramList= [w for w in filtTrigramList if not hasSpecialCharacter(w) and not hasRedundantToken(w)]\n# filtTrigramList= [w for w in filtTrigramList if len(set('[~!@#$%^&*()_-+{}<>\"=:,.;?/|\\']+$').intersection(w)) == 0 ]\n# filtTrigramList = [w for w in filtTrigramList if w.find(\"patient\") is -1 and w.find(\" pt\") is -1] \n##filtTrigramList= [u+\" \"+v+\" \"+w for u,v,w in trigramList if not u in stops and not w in stops and (u in conceptList or v in conceptList or w in conceptList)]\nfiltTrigramList = list(set(filtTrigramList))\nlen(filtTrigramList)",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "filtTrigramList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "filtTrigramList = list(set(filtTrigramList))\nlen(filtTrigramList)\n##load GT annotation\n##load protocol concept list\nfilePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/newConceptList_November2018.txt\" # newConceptList_Sept2018 conceptList\nconceptList = loadOriginalConceptListPhrase(filePath)\nprint(len(conceptList))\n##load metaMap extended concept list\n##count ground truth in testData\n# filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/trainingData/DataLines.txt\" ",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "filePath",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Outputs/newConceptList_November2018.txt\" # newConceptList_Sept2018 conceptList\nconceptList = loadOriginalConceptListPhrase(filePath)\nprint(len(conceptList))\n##load metaMap extended concept list\n##count ground truth in testData\n# filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/trainingData/DataLines.txt\" \n# filePath = datafileRootPath + dataFileName\n# sentenceList = loadData(filePath, True)\n# min_count = 1\n# vectorSize = 150",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "conceptList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "conceptList = loadOriginalConceptListPhrase(filePath)\nprint(len(conceptList))\n##load metaMap extended concept list\n##count ground truth in testData\n# filePath = \"/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/trainingData/DataLines.txt\" \n# filePath = datafileRootPath + dataFileName\n# sentenceList = loadData(filePath, True)\n# min_count = 1\n# vectorSize = 150\n# windowSize = 10",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "candidateList",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "candidateList = list()\ninputSpace = set()\ninputSpace.update(filtTrigramList)\ninputSpace.update(filtBigramList)\n# inputSpace.update(unigramList)\ninputSpace = list(inputSpace)\ninputSpace = [w for w in inputSpace if not hasNumbers(w)]\ninputSpace = [w for w in inputSpace if not hasStopwords(w)]\ninputSpace= [w for w in inputSpace if not hasSpecialCharacter(w) and not hasRedundantToken(w)]\n# model = gensim.models.KeyedVectors.load_word2vec_format('/Users/sarahmasudpreum/src/word2vecModels/GoogleNews-vectors-negative300.bin', binary=True)  ",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "inputSpace",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "inputSpace = set()\ninputSpace.update(filtTrigramList)\ninputSpace.update(filtBigramList)\n# inputSpace.update(unigramList)\ninputSpace = list(inputSpace)\ninputSpace = [w for w in inputSpace if not hasNumbers(w)]\ninputSpace = [w for w in inputSpace if not hasStopwords(w)]\ninputSpace= [w for w in inputSpace if not hasSpecialCharacter(w) and not hasRedundantToken(w)]\n# model = gensim.models.KeyedVectors.load_word2vec_format('/Users/sarahmasudpreum/src/word2vecModels/GoogleNews-vectors-negative300.bin', binary=True)  \n# word2vec_RAA_Data_150_10 word2vec_Forum_Data_150_10 word2vec_MIMIC3_Data_150_10 word2vec_I2B2_Data_150_10",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "inputSpace",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "inputSpace = list(inputSpace)\ninputSpace = [w for w in inputSpace if not hasNumbers(w)]\ninputSpace = [w for w in inputSpace if not hasStopwords(w)]\ninputSpace= [w for w in inputSpace if not hasSpecialCharacter(w) and not hasRedundantToken(w)]\n# model = gensim.models.KeyedVectors.load_word2vec_format('/Users/sarahmasudpreum/src/word2vecModels/GoogleNews-vectors-negative300.bin', binary=True)  \n# word2vec_RAA_Data_150_10 word2vec_Forum_Data_150_10 word2vec_MIMIC3_Data_150_10 word2vec_I2B2_Data_150_10\n# word2vec_Med_Data_150_10 word2vec_EMS_Data_150_10 word2vec_MedEMS_Data_150_10\nmodel = gensim.models.Word2Vec.load('/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/wordEmbeddingModel/Model/word2vec_EMS_Data_150_10.model')  \nvl = model.wv.vocab ##model.vocab (for google) model.wv.vocab\ntopnDef = 50",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "inputSpace",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "inputSpace = [w for w in inputSpace if not hasNumbers(w)]\ninputSpace = [w for w in inputSpace if not hasStopwords(w)]\ninputSpace= [w for w in inputSpace if not hasSpecialCharacter(w) and not hasRedundantToken(w)]\n# model = gensim.models.KeyedVectors.load_word2vec_format('/Users/sarahmasudpreum/src/word2vecModels/GoogleNews-vectors-negative300.bin', binary=True)  \n# word2vec_RAA_Data_150_10 word2vec_Forum_Data_150_10 word2vec_MIMIC3_Data_150_10 word2vec_I2B2_Data_150_10\n# word2vec_Med_Data_150_10 word2vec_EMS_Data_150_10 word2vec_MedEMS_Data_150_10\nmodel = gensim.models.Word2Vec.load('/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/wordEmbeddingModel/Model/word2vec_EMS_Data_150_10.model')  \nvl = model.wv.vocab ##model.vocab (for google) model.wv.vocab\ntopnDef = 50\nf1, f2 = createFileName(\"EMS\", topnDef) ",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "inputSpace",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "inputSpace = [w for w in inputSpace if not hasStopwords(w)]\ninputSpace= [w for w in inputSpace if not hasSpecialCharacter(w) and not hasRedundantToken(w)]\n# model = gensim.models.KeyedVectors.load_word2vec_format('/Users/sarahmasudpreum/src/word2vecModels/GoogleNews-vectors-negative300.bin', binary=True)  \n# word2vec_RAA_Data_150_10 word2vec_Forum_Data_150_10 word2vec_MIMIC3_Data_150_10 word2vec_I2B2_Data_150_10\n# word2vec_Med_Data_150_10 word2vec_EMS_Data_150_10 word2vec_MedEMS_Data_150_10\nmodel = gensim.models.Word2Vec.load('/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/wordEmbeddingModel/Model/word2vec_EMS_Data_150_10.model')  \nvl = model.wv.vocab ##model.vocab (for google) model.wv.vocab\ntopnDef = 50\nf1, f2 = createFileName(\"EMS\", topnDef) \n## RAA Forum MIMIC3 I2B2 Med EMS all Google Glove",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "model = gensim.models.Word2Vec.load('/Users/sarahmasudpreum/Dropbox/NIST_Project/Data/wordEmbeddingModel/Model/word2vec_EMS_Data_150_10.model')  \nvl = model.wv.vocab ##model.vocab (for google) model.wv.vocab\ntopnDef = 50\nf1, f2 = createFileName(\"EMS\", topnDef) \n## RAA Forum MIMIC3 I2B2 Med EMS all Google Glove\nwfile = open(f1, \"w\")\nwfile2 = open(f2, \"w\")\n# candidateList = list()\nTOPN = 100\nfor concept in conceptList:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "vl",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "vl = model.wv.vocab ##model.vocab (for google) model.wv.vocab\ntopnDef = 50\nf1, f2 = createFileName(\"EMS\", topnDef) \n## RAA Forum MIMIC3 I2B2 Med EMS all Google Glove\nwfile = open(f1, \"w\")\nwfile2 = open(f2, \"w\")\n# candidateList = list()\nTOPN = 100\nfor concept in conceptList:\n    # candidateList = list()",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "topnDef",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "topnDef = 50\nf1, f2 = createFileName(\"EMS\", topnDef) \n## RAA Forum MIMIC3 I2B2 Med EMS all Google Glove\nwfile = open(f1, \"w\")\nwfile2 = open(f2, \"w\")\n# candidateList = list()\nTOPN = 100\nfor concept in conceptList:\n    # candidateList = list()\n    # if concept in vl:",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "wfile",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "wfile = open(f1, \"w\")\nwfile2 = open(f2, \"w\")\n# candidateList = list()\nTOPN = 100\nfor concept in conceptList:\n    # candidateList = list()\n    # if concept in vl:\n    # print(concept)\n    # candidateList, scoreTupleList = model_most_similar_phrases(model, inputSpace, concept, topn=topnDef)\n    # candidateList = [w for w in candidateList if not hasNumbers(w)]",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "wfile2",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "wfile2 = open(f2, \"w\")\n# candidateList = list()\nTOPN = 100\nfor concept in conceptList:\n    # candidateList = list()\n    # if concept in vl:\n    # print(concept)\n    # candidateList, scoreTupleList = model_most_similar_phrases(model, inputSpace, concept, topn=topnDef)\n    # candidateList = [w for w in candidateList if not hasNumbers(w)]\n    # candidateList = [w for w in candidateList if not hasStopwords(w)]",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "TOPN",
        "kind": 5,
        "importPath": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "description": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "peekOfCode": "TOPN = 100\nfor concept in conceptList:\n    # candidateList = list()\n    # if concept in vl:\n    # print(concept)\n    # candidateList, scoreTupleList = model_most_similar_phrases(model, inputSpace, concept, topn=topnDef)\n    # candidateList = [w for w in candidateList if not hasNumbers(w)]\n    # candidateList = [w for w in candidateList if not hasStopwords(w)]\n    # # candidateList = [w for w in candidateList if w.find(\"patient\") is -1 or w.find(\" pt\") is -1 or w.find(\" t \") is -1 or w.find(\" s \") is -1] \n    # candidateList= [w for w in candidateList if not hasSpecialCharacter(w) and not hasRedundantToken(w)]",
        "detail": "ETC.EMS_conceptExtension.versionOctober2018.wordEmbeddingCandidateListGeneration_Phrase_Oct2018",
        "documentation": {}
    },
    {
        "label": "TextComp",
        "kind": 6,
        "importPath": "ETC.Performance Evaluation.WERCalc",
        "description": "ETC.Performance Evaluation.WERCalc",
        "peekOfCode": "class TextComp(object):\n    def __init__(self, original_path, recognition_path, encoding='utf-8'):\n        # original_path: path of the original text\n        # recognition_path: path of the recognized text\n        # encoding: specifies the encoding which is to be used for the file\n        self.original_path = original_path\n        self.recognition_path = recognition_path\n        self.encoding = encoding\n        self.I = 0\n        self.S = 0",
        "detail": "ETC.Performance Evaluation.WERCalc",
        "documentation": {}
    },
    {
        "label": "set_filename",
        "kind": 2,
        "importPath": "ETC.Performance Evaluation.WERCalc",
        "description": "ETC.Performance Evaluation.WERCalc",
        "peekOfCode": "def set_filename(name, num):\n    return \"C:/Users/Student/OneDrive/Documents/Summer 2019 Research/Week 2-Testing/Transcripts/\" + name + str(num) + \".txt\"\ndef main():\n    path = \"C:/Users/Student/OneDrive/Documents/Summer 2019 Research/Week 2-Testing/Transcripts/\"\n    for i in range(0, 3):\n        compare = TextComp(path + \"Orig\" + str(i) + \".txt\", path + \"API\" + str(i) + \".txt\")\n        print(\"Recording 00\" + str(i) + \" WER: \" + str(compare.WER()))\nmain()",
        "detail": "ETC.Performance Evaluation.WERCalc",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "ETC.Performance Evaluation.WERCalc",
        "description": "ETC.Performance Evaluation.WERCalc",
        "peekOfCode": "def main():\n    path = \"C:/Users/Student/OneDrive/Documents/Summer 2019 Research/Week 2-Testing/Transcripts/\"\n    for i in range(0, 3):\n        compare = TextComp(path + \"Orig\" + str(i) + \".txt\", path + \"API\" + str(i) + \".txt\")\n        print(\"Recording 00\" + str(i) + \" WER: \" + str(compare.WER()))\nmain()",
        "detail": "ETC.Performance Evaluation.WERCalc",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": "ETC.Performance Evaluation.WERCalc",
        "description": "ETC.Performance Evaluation.WERCalc",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\n# compare two text\nclass TextComp(object):\n    def __init__(self, original_path, recognition_path, encoding='utf-8'):\n        # original_path: path of the original text\n        # recognition_path: path of the recognized text\n        # encoding: specifies the encoding which is to be used for the file\n        self.original_path = original_path\n        self.recognition_path = recognition_path\n        self.encoding = encoding",
        "detail": "ETC.Performance Evaluation.WERCalc",
        "documentation": {}
    },
    {
        "label": "get_truth_concepts",
        "kind": 2,
        "importPath": "ETC.Performance Evaluation.concepts",
        "description": "ETC.Performance Evaluation.concepts",
        "peekOfCode": "def get_truth_concepts(path):\n    \"\"\"\n    Converts the truth, in a text file, into a master list of concepts per case\n    :param path: the location of the .txt file with the pipeline output\n    :return: truth: a list of lists of lists. Master list: contains all data. Each element of master list: a list of concepts for each case. Each case element: a concept\n    \"\"\"\n    file = open(path)\n    truth = []\n    all_text = file.read()\n    file.close()",
        "detail": "ETC.Performance Evaluation.concepts",
        "documentation": {}
    },
    {
        "label": "get_results_concepts",
        "kind": 2,
        "importPath": "ETC.Performance Evaluation.concepts",
        "description": "ETC.Performance Evaluation.concepts",
        "peekOfCode": "def get_results_concepts(path):\n    \"\"\"\n    Converts the pipeline output into a master list of concepts per case\n    :param path: the location of the .txt file with the pipeline output\n    :return: results: a list of lists of lists. Master list: contains all data. Each element of master list: a list of concepts for each case. Each case element: a concept\n    \"\"\"\n    file = open(path)\n    all_text = file.read()\n    file.close()\n    results = []",
        "detail": "ETC.Performance Evaluation.concepts",
        "documentation": {}
    },
    {
        "label": "process_with_text",
        "kind": 2,
        "importPath": "ETC.Performance Evaluation.concepts",
        "description": "ETC.Performance Evaluation.concepts",
        "peekOfCode": "def process_with_text(concepts):\n    \"\"\"\n    The strictest way to compare concepts; looks at concept, negation, value, and text signal\n    :param concepts: the truth or results master list\n    :return: concepts: the master list, but each concept pared down to only [concept, T/F, value, text], and converted to string\n    \"\"\"\n    for i in range(len(concepts)):\n        for j in range(len(concepts[i])):\n            concepts[i][j] = str(concepts[i][j][:4])\n    return concepts",
        "detail": "ETC.Performance Evaluation.concepts",
        "documentation": {}
    },
    {
        "label": "process_no_text",
        "kind": 2,
        "importPath": "ETC.Performance Evaluation.concepts",
        "description": "ETC.Performance Evaluation.concepts",
        "peekOfCode": "def process_no_text(concepts):\n    \"\"\"\n    An intermediate-strictness way to compare concepts; compares the concept, negation, and value but not (for numerical values) the surrounding text\n    :param concepts: the truth or results master list\n    :return: concepts: the master list, but each concept pared down to only [concept, T/F, value] and converted to string\n    \"\"\"\n    for i in range(len(concepts)):\n        for j in range(len(concepts[i])):\n            concepts[i][j] = str(concepts[i][j][:3])\n    return concepts",
        "detail": "ETC.Performance Evaluation.concepts",
        "documentation": {}
    },
    {
        "label": "process_tf",
        "kind": 2,
        "importPath": "ETC.Performance Evaluation.concepts",
        "description": "ETC.Performance Evaluation.concepts",
        "peekOfCode": "def process_tf(concepts):\n    \"\"\"\n    The least strict way to compare concepts; compares only the concept and its negation; useful for whether or not the concept and negation is correct and disregards differences in context\n    :param concepts:\n    :return: concepts: the master list, but each concept pared down to only [concept, T/F] and converted to string\n    \"\"\"\n    for i in range(len(concepts)):\n        for j in range(len(concepts[i])):\n            concepts[i][j] = str(concepts[i][j][:2])\n    return concepts",
        "detail": "ETC.Performance Evaluation.concepts",
        "documentation": {}
    },
    {
        "label": "main2",
        "kind": 2,
        "importPath": "ETC.Performance Evaluation.concepts",
        "description": "ETC.Performance Evaluation.concepts",
        "peekOfCode": "def main2():\n    path_results = \"C:/Users/Student/OneDrive/Documents/Summer 2019 Research/Week 3-Testing/extracted.txt\"\n    path_truth = \"C:/Users/Student/OneDrive/Documents/Summer 2019 Research/Week 3-Testing/concepts.txt\"\n    # Get results\n    results = get_results_concepts(path_results)\n    # print(len(results))\n    # Process results\n    results = process_with_text(results)\n    # results = process_no_text(results)\n    # results = process_tf(results)",
        "detail": "ETC.Performance Evaluation.concepts",
        "documentation": {}
    },
    {
        "label": "get_results_interventions",
        "kind": 2,
        "importPath": "ETC.Performance Evaluation.interventions",
        "description": "ETC.Performance Evaluation.interventions",
        "peekOfCode": "def get_results_interventions(path):\n    \"\"\"\n    Converts the pipeline output into a master list of interventions per case\n    :param path: the location of the .txt file with the pipeline output\n    :return: results: list of lists; each element of the overall list includes all the interventions for single case\n            all_r: a set of all interventions in all cases (for a pooled calculation)\n    \"\"\"\n    results = []\n    all = set()\n    file = open(path)",
        "detail": "ETC.Performance Evaluation.interventions",
        "documentation": {}
    },
    {
        "label": "get_truth_interventions",
        "kind": 2,
        "importPath": "ETC.Performance Evaluation.interventions",
        "description": "ETC.Performance Evaluation.interventions",
        "peekOfCode": "def get_truth_interventions(path):\n    \"\"\"\n    Converts the truth, in a text file, into a master list of interventions per case\n    :param path: the location of the .txt file with the ground truth\n    :return: results: list of lists; each element of the overall list includes all the interventions for single case\n            all_r: a set of all interventions in all cases (for a pooled calculation)\n    \"\"\"\n    truth = []\n    all = set()\n    file = open(path)",
        "detail": "ETC.Performance Evaluation.interventions",
        "documentation": {}
    },
    {
        "label": "get_tp",
        "kind": 2,
        "importPath": "ETC.Performance Evaluation.interventions",
        "description": "ETC.Performance Evaluation.interventions",
        "peekOfCode": "def get_tp(truth, results):\n    \"\"\"\n    :param truth: a single list of ground truth interventions\n    :param results: a single list of pipeline-returned interventions\n    :return: a list of true positives (in the ground truth, returned by pipeline)\n    \"\"\"\n    # print(\"Truth: \" + str(truth) + \"\\t Results: \" + str(results))\n    tP = []\n    for i in results:\n        if i in truth:",
        "detail": "ETC.Performance Evaluation.interventions",
        "documentation": {}
    },
    {
        "label": "get_fp",
        "kind": 2,
        "importPath": "ETC.Performance Evaluation.interventions",
        "description": "ETC.Performance Evaluation.interventions",
        "peekOfCode": "def get_fp(truth, results):\n    \"\"\"\n    :param truth: a single list of ground truth interventions\n    :param results: a single list of pipeline-returned interventions\n    :return: a list of false positives (not in the ground truth, returned by pipeline)\n    \"\"\"\n    fP = []\n    for i in results:\n        if i not in truth:\n            fP.append(i)",
        "detail": "ETC.Performance Evaluation.interventions",
        "documentation": {}
    },
    {
        "label": "get_fn",
        "kind": 2,
        "importPath": "ETC.Performance Evaluation.interventions",
        "description": "ETC.Performance Evaluation.interventions",
        "peekOfCode": "def get_fn(truth, results):\n    \"\"\"\n    :param truth: a single list of ground truth interventions\n    :param results: a single list of pipeline-returned interventions\n    :return: a list of false negatives (in the ground truth, not returned by pipeline)\n    \"\"\"\n    fN = []\n    for i in truth:\n        if i not in results:\n            fN.append(i)",
        "detail": "ETC.Performance Evaluation.interventions",
        "documentation": {}
    },
    {
        "label": "calc_precision",
        "kind": 2,
        "importPath": "ETC.Performance Evaluation.interventions",
        "description": "ETC.Performance Evaluation.interventions",
        "peekOfCode": "def calc_precision(truth, results):\n    tP = get_tp(truth, results)\n    fP = get_fp(truth, results)\n    return len(tP)/(len(tP)+len(fP))\ndef calc_recall(truth, results):\n    tP = get_tp(truth, results)\n    fN = get_fn(truth, results)\n    return len(tP)/(len(tP)+len(fN))\ndef calc_f1(truth, results):\n    prec = calc_precision(truth, results)",
        "detail": "ETC.Performance Evaluation.interventions",
        "documentation": {}
    },
    {
        "label": "calc_recall",
        "kind": 2,
        "importPath": "ETC.Performance Evaluation.interventions",
        "description": "ETC.Performance Evaluation.interventions",
        "peekOfCode": "def calc_recall(truth, results):\n    tP = get_tp(truth, results)\n    fN = get_fn(truth, results)\n    return len(tP)/(len(tP)+len(fN))\ndef calc_f1(truth, results):\n    prec = calc_precision(truth, results)\n    rec = calc_recall(truth, results)\n    denom = (prec + rec)\n    if denom == 0:\n        return 0",
        "detail": "ETC.Performance Evaluation.interventions",
        "documentation": {}
    },
    {
        "label": "calc_f1",
        "kind": 2,
        "importPath": "ETC.Performance Evaluation.interventions",
        "description": "ETC.Performance Evaluation.interventions",
        "peekOfCode": "def calc_f1(truth, results):\n    prec = calc_precision(truth, results)\n    rec = calc_recall(truth, results)\n    denom = (prec + rec)\n    if denom == 0:\n        return 0\n    return 2*((prec * rec)/denom)\ndef main():\n    path_results = \"C:/Users/Student/OneDrive/Documents/Summer 2019 Research/Week 2-Testing/interv.txt\"\n    path_truth = \"C:/Users/Student/OneDrive/Documents/Summer 2019 Research/Week 2-Testing/ground.txt\"",
        "detail": "ETC.Performance Evaluation.interventions",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "ETC.Performance Evaluation.interventions",
        "description": "ETC.Performance Evaluation.interventions",
        "peekOfCode": "def main():\n    path_results = \"C:/Users/Student/OneDrive/Documents/Summer 2019 Research/Week 2-Testing/interv.txt\"\n    path_truth = \"C:/Users/Student/OneDrive/Documents/Summer 2019 Research/Week 2-Testing/ground.txt\"\n    # Get results\n    results, all_r = get_results_interventions(path_results)\n    # print(results)\n    # print(len(results))\n    # Get truth\n    truth, all_t = get_truth_interventions(path_truth)\n    # print(truth)",
        "detail": "ETC.Performance Evaluation.interventions",
        "documentation": {}
    },
    {
        "label": "pull_files",
        "kind": 2,
        "importPath": "ETC.Performance Evaluation.processtranscripts",
        "description": "ETC.Performance Evaluation.processtranscripts",
        "peekOfCode": "def pull_files(masterlist, savepath):\n    \"\"\"\n    :param: masterlist- a text file with all transcripts copied from Excel/Google Sheets\n    :param: savepath- the path where these files should go\n    \"\"\"\n    file = open(masterlist, \"r\")\n    transcripts = []\n    for thing in file:\n        transcripts.append(thing)\n    file.close()",
        "detail": "ETC.Performance Evaluation.processtranscripts",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "ETC.Primary_Survey.pyDatalog",
        "description": "ETC.Primary_Survey.pyDatalog",
        "peekOfCode": "result = op['Rescuer_Hazard','','','uBreathing','uPulse','uBleeding'] == Y\n# show response\nprint result[0][0]",
        "detail": "ETC.Primary_Survey.pyDatalog",
        "documentation": {}
    },
    {
        "label": "TextComp",
        "kind": 6,
        "importPath": "ETC.Word Error Rate Calculation.TextCompare",
        "description": "ETC.Word Error Rate Calculation.TextCompare",
        "peekOfCode": "class TextComp(object):\n    def __init__(self,original_path, recognition_path, encoding = 'utf-8'):\n        # original_path: path of the original text\n        # recognition_path: path of the recognized text\n        # encoding: specifies the encoding which is to be used for the file\n        self.original_path = original_path\n        self.recognition_path = recognition_path\n        self.encoding = encoding\n        self.I = 0\n        self.S = 0",
        "detail": "ETC.Word Error Rate Calculation.TextCompare",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": "ETC.Word Error Rate Calculation.TextCompare",
        "description": "ETC.Word Error Rate Calculation.TextCompare",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\n# compare two text\nclass TextComp(object):\n    def __init__(self,original_path, recognition_path, encoding = 'utf-8'):\n        # original_path: path of the original text\n        # recognition_path: path of the recognized text\n        # encoding: specifies the encoding which is to be used for the file\n        self.original_path = original_path\n        self.recognition_path = recognition_path\n        self.encoding = encoding",
        "detail": "ETC.Word Error Rate Calculation.TextCompare",
        "documentation": {}
    },
    {
        "label": "get_tokens",
        "kind": 2,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "def get_tokens(text):\n\tstops = set(stopwords.words('english'))\n\tregex = re.compile('[%s]' % re.escape('!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~'))\n\t# Get the sentences\n\tsentences = regex.sub(' ', text)\n\t# Get the words\n\traw_tokens = list(set(word_tokenize(unicode(sentences, errors='ignore'))))\n\t# Filter numbers and characters\n\t#tokens= [str(t) for t in raw_tokens if str(t) not in stops and not str(t).isdigit() and len(str(t))>1]\n\ttokens= [str(t) for t in raw_tokens if not str(t).isdigit() and len(str(t))>1]",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "get_pos_tags",
        "kind": 2,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "def get_pos_tags(tokens):\n\tregex = re.compile('[%s]' % re.escape('!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~'))\n\ttags = [];\n\tstarti = 0\n\tendi = 0\n\tno_chunks = len(tokens)/5000+1;\n\tprint 'Process '+str(len(tokens))+' tokens in '+str(no_chunks)+ ' chunks..' \n\tfor l in range(0, no_chunks):\n\t\tendi =  min((starti + (len(tokens)/no_chunks) ), len(tokens))\n\t\tprint \"Tagging #\" + str(l) + \": from \" + str(starti)+ \" to \"+str(endi-1)",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "cleanseNN",
        "kind": 2,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "def cleanseNN(list):\n\tfor i in range(0, len(list)):\n\t\tfor k in range(0, len(list[i])):\n\t\t\tif(\"NN\" in list[i][k]):\n\t\t\t\tlist[i][k] = \"NN\"\n\treturn list\n# Look for longest n-gram appearing in each sentence with the patterns of technical terms\ndef get_tech_ngrams(text, tag_set):\t\n\t# Normalization and Punctuation filtering=> keep sentence separators\n\ttext = text.lower()",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "get_tech_ngrams",
        "kind": 2,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "def get_tech_ngrams(text, tag_set):\t\n\t# Normalization and Punctuation filtering=> keep sentence separators\n\ttext = text.lower()\n\tsentences = re.split('\\.(?!\\d)', text)\n\t#print sentences\n\tresults = {'ngram':'tags'};\n\tn_gram = []\n\ttags = []\n\tn_gram_str = ''\n\ttag_str = ''",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "lexicon_expansion",
        "kind": 2,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "def lexicon_expansion(seed_terms):\n\tprint(\"\\n Total seed terms = \" + str(len(seed_terms))+\"\\n\")\n\tterms_scores = []\n\tfor c in seed_terms:\n\t\tterms_scores.append((str(c),str(1.0)))\n\tfor c in seed_terms:\n\t\tif not(c.isdigit()) and (len(c) > 3):\n\t\t\tprint(\"\\nSeed Term:\" + c)\n\t\t\t# Get the word2vec representation of the ngram by adding word vectors\n\t\t\tc_array = [s for s in c.split(' ') if s in googlenews_model.vocab]",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "def main():\n\tos.chdir(\"./dataset\")\n\t# Set default encoding of python to utf8\n\treload(sys)  \n\tsys.setdefaultencoding('utf8')\n\tngrams_set = set()\n\twith open('output.csv', 'w') as output:\n\t\tcsvwriter = csv.writer(output)\n\t\tcsvwriter.writerow([\"Data File\", \"Narrative\", \"N-grams\"])\n\t\tfor file in glob.glob(\"*.txt\"):",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "bigram_measures",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "bigram_measures = nltk.collocations.BigramAssocMeasures()\ntrigram_measures = nltk.collocations.TrigramAssocMeasures()\n#pos = StanfordPOSTagger('./stanford-postagger-2013-06-20/models/english-left3words-distsim.tagger', \n#                './stanford-postagger-2013-06-20/stanford-postagger.jar')\n# N-gram patterns for technical terms\npatterns = [['NN'],['JJ'], ['JJ','JJ'],['JJ', 'NN'], ['NN','NN'], ['JJ', 'JJ', 'NN'], ['JJ', 'NN', 'NN'], \n            ['NN', 'JJ', 'NN'], ['NN', 'NN', 'NN'],['NN', 'IN', 'NN'],['JJ','JJ','JJ'],\n\t\t\t['JJ', 'JJ', 'JJ','NN'], ['JJ', 'JJ', 'NN','NN'], ['JJ', 'NN', 'NN','NN'],['JJ', 'NN', 'JJ','NN'],['NN', 'JJ', 'JJ','NN'], \n\t\t\t['NN', 'NN', 'NN','NN'],['NN','JJ', 'NN','NN'], ['NN', 'NN','JJ', 'NN']]\n# For filtering junk",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "trigram_measures",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n#pos = StanfordPOSTagger('./stanford-postagger-2013-06-20/models/english-left3words-distsim.tagger', \n#                './stanford-postagger-2013-06-20/stanford-postagger.jar')\n# N-gram patterns for technical terms\npatterns = [['NN'],['JJ'], ['JJ','JJ'],['JJ', 'NN'], ['NN','NN'], ['JJ', 'JJ', 'NN'], ['JJ', 'NN', 'NN'], \n            ['NN', 'JJ', 'NN'], ['NN', 'NN', 'NN'],['NN', 'IN', 'NN'],['JJ','JJ','JJ'],\n\t\t\t['JJ', 'JJ', 'JJ','NN'], ['JJ', 'JJ', 'NN','NN'], ['JJ', 'NN', 'NN','NN'],['JJ', 'NN', 'JJ','NN'],['NN', 'JJ', 'JJ','NN'], \n\t\t\t['NN', 'NN', 'NN','NN'],['NN','JJ', 'NN','NN'], ['NN', 'NN','JJ', 'NN']]\n# For filtering junk\nnon_tech_words = []",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "#pos",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "#pos = StanfordPOSTagger('./stanford-postagger-2013-06-20/models/english-left3words-distsim.tagger', \n#                './stanford-postagger-2013-06-20/stanford-postagger.jar')\n# N-gram patterns for technical terms\npatterns = [['NN'],['JJ'], ['JJ','JJ'],['JJ', 'NN'], ['NN','NN'], ['JJ', 'JJ', 'NN'], ['JJ', 'NN', 'NN'], \n            ['NN', 'JJ', 'NN'], ['NN', 'NN', 'NN'],['NN', 'IN', 'NN'],['JJ','JJ','JJ'],\n\t\t\t['JJ', 'JJ', 'JJ','NN'], ['JJ', 'JJ', 'NN','NN'], ['JJ', 'NN', 'NN','NN'],['JJ', 'NN', 'JJ','NN'],['NN', 'JJ', 'JJ','NN'], \n\t\t\t['NN', 'NN', 'NN','NN'],['NN','JJ', 'NN','NN'], ['NN', 'NN','JJ', 'NN']]\n# For filtering junk\nnon_tech_words = []\ngooglenews_model =  Word2Vec.load_word2vec_format('./word2vec/GoogleNews-vectors-negative300.bin', binary=True)",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "patterns",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "patterns = [['NN'],['JJ'], ['JJ','JJ'],['JJ', 'NN'], ['NN','NN'], ['JJ', 'JJ', 'NN'], ['JJ', 'NN', 'NN'], \n            ['NN', 'JJ', 'NN'], ['NN', 'NN', 'NN'],['NN', 'IN', 'NN'],['JJ','JJ','JJ'],\n\t\t\t['JJ', 'JJ', 'JJ','NN'], ['JJ', 'JJ', 'NN','NN'], ['JJ', 'NN', 'NN','NN'],['JJ', 'NN', 'JJ','NN'],['NN', 'JJ', 'JJ','NN'], \n\t\t\t['NN', 'NN', 'NN','NN'],['NN','JJ', 'NN','NN'], ['NN', 'NN','JJ', 'NN']]\n# For filtering junk\nnon_tech_words = []\ngooglenews_model =  Word2Vec.load_word2vec_format('./word2vec/GoogleNews-vectors-negative300.bin', binary=True)\ngooglenews_model.syn0norm = googlenews_model.syn0\n# Tokenization\ndef get_tokens(text):",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "non_tech_words",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "non_tech_words = []\ngooglenews_model =  Word2Vec.load_word2vec_format('./word2vec/GoogleNews-vectors-negative300.bin', binary=True)\ngooglenews_model.syn0norm = googlenews_model.syn0\n# Tokenization\ndef get_tokens(text):\n\tstops = set(stopwords.words('english'))\n\tregex = re.compile('[%s]' % re.escape('!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~'))\n\t# Get the sentences\n\tsentences = regex.sub(' ', text)\n\t# Get the words",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "googlenews_model",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "googlenews_model =  Word2Vec.load_word2vec_format('./word2vec/GoogleNews-vectors-negative300.bin', binary=True)\ngooglenews_model.syn0norm = googlenews_model.syn0\n# Tokenization\ndef get_tokens(text):\n\tstops = set(stopwords.words('english'))\n\tregex = re.compile('[%s]' % re.escape('!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~'))\n\t# Get the sentences\n\tsentences = regex.sub(' ', text)\n\t# Get the words\n\traw_tokens = list(set(word_tokenize(unicode(sentences, errors='ignore'))))",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "googlenews_model.syn0norm",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "googlenews_model.syn0norm = googlenews_model.syn0\n# Tokenization\ndef get_tokens(text):\n\tstops = set(stopwords.words('english'))\n\tregex = re.compile('[%s]' % re.escape('!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~'))\n\t# Get the sentences\n\tsentences = regex.sub(' ', text)\n\t# Get the words\n\traw_tokens = list(set(word_tokenize(unicode(sentences, errors='ignore'))))\n\t# Filter numbers and characters",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "\tstops",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "\tstops = set(stopwords.words('english'))\n\tregex = re.compile('[%s]' % re.escape('!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~'))\n\t# Get the sentences\n\tsentences = regex.sub(' ', text)\n\t# Get the words\n\traw_tokens = list(set(word_tokenize(unicode(sentences, errors='ignore'))))\n\t# Filter numbers and characters\n\t#tokens= [str(t) for t in raw_tokens if str(t) not in stops and not str(t).isdigit() and len(str(t))>1]\n\ttokens= [str(t) for t in raw_tokens if not str(t).isdigit() and len(str(t))>1]\n\ttokens = [t for t in tokens if t not in non_tech_words and t.isalpha()]",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "\tregex",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "\tregex = re.compile('[%s]' % re.escape('!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~'))\n\t# Get the sentences\n\tsentences = regex.sub(' ', text)\n\t# Get the words\n\traw_tokens = list(set(word_tokenize(unicode(sentences, errors='ignore'))))\n\t# Filter numbers and characters\n\t#tokens= [str(t) for t in raw_tokens if str(t) not in stops and not str(t).isdigit() and len(str(t))>1]\n\ttokens= [str(t) for t in raw_tokens if not str(t).isdigit() and len(str(t))>1]\n\ttokens = [t for t in tokens if t not in non_tech_words and t.isalpha()]\n\treturn tokens",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "\tsentences",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "\tsentences = regex.sub(' ', text)\n\t# Get the words\n\traw_tokens = list(set(word_tokenize(unicode(sentences, errors='ignore'))))\n\t# Filter numbers and characters\n\t#tokens= [str(t) for t in raw_tokens if str(t) not in stops and not str(t).isdigit() and len(str(t))>1]\n\ttokens= [str(t) for t in raw_tokens if not str(t).isdigit() and len(str(t))>1]\n\ttokens = [t for t in tokens if t not in non_tech_words and t.isalpha()]\n\treturn tokens\n# Part of Speech Tagging \ndef get_pos_tags(tokens):",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "\traw_tokens",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "\traw_tokens = list(set(word_tokenize(unicode(sentences, errors='ignore'))))\n\t# Filter numbers and characters\n\t#tokens= [str(t) for t in raw_tokens if str(t) not in stops and not str(t).isdigit() and len(str(t))>1]\n\ttokens= [str(t) for t in raw_tokens if not str(t).isdigit() and len(str(t))>1]\n\ttokens = [t for t in tokens if t not in non_tech_words and t.isalpha()]\n\treturn tokens\n# Part of Speech Tagging \ndef get_pos_tags(tokens):\n\tregex = re.compile('[%s]' % re.escape('!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~'))\n\ttags = [];",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "\ttokens",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "\ttokens = [t for t in tokens if t not in non_tech_words and t.isalpha()]\n\treturn tokens\n# Part of Speech Tagging \ndef get_pos_tags(tokens):\n\tregex = re.compile('[%s]' % re.escape('!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~'))\n\ttags = [];\n\tstarti = 0\n\tendi = 0\n\tno_chunks = len(tokens)/5000+1;\n\tprint 'Process '+str(len(tokens))+' tokens in '+str(no_chunks)+ ' chunks..' ",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "\tregex",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "\tregex = re.compile('[%s]' % re.escape('!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~'))\n\ttags = [];\n\tstarti = 0\n\tendi = 0\n\tno_chunks = len(tokens)/5000+1;\n\tprint 'Process '+str(len(tokens))+' tokens in '+str(no_chunks)+ ' chunks..' \n\tfor l in range(0, no_chunks):\n\t\tendi =  min((starti + (len(tokens)/no_chunks) ), len(tokens))\n\t\tprint \"Tagging #\" + str(l) + \": from \" + str(starti)+ \" to \"+str(endi-1)\n\t\ttags = tags + nltk.pos_tag(tokens[starti:endi]);",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "\ttags",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "\ttags = [];\n\tstarti = 0\n\tendi = 0\n\tno_chunks = len(tokens)/5000+1;\n\tprint 'Process '+str(len(tokens))+' tokens in '+str(no_chunks)+ ' chunks..' \n\tfor l in range(0, no_chunks):\n\t\tendi =  min((starti + (len(tokens)/no_chunks) ), len(tokens))\n\t\tprint \"Tagging #\" + str(l) + \": from \" + str(starti)+ \" to \"+str(endi-1)\n\t\ttags = tags + nltk.pos_tag(tokens[starti:endi]);\n\t\t#tags = tags + pos.tag(tokens[starti:endi])[0]; \t\t",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "\tstarti",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "\tstarti = 0\n\tendi = 0\n\tno_chunks = len(tokens)/5000+1;\n\tprint 'Process '+str(len(tokens))+' tokens in '+str(no_chunks)+ ' chunks..' \n\tfor l in range(0, no_chunks):\n\t\tendi =  min((starti + (len(tokens)/no_chunks) ), len(tokens))\n\t\tprint \"Tagging #\" + str(l) + \": from \" + str(starti)+ \" to \"+str(endi-1)\n\t\ttags = tags + nltk.pos_tag(tokens[starti:endi]);\n\t\t#tags = tags + pos.tag(tokens[starti:endi])[0]; \t\t\n\t\tstarti = endi;",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "\tendi",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "\tendi = 0\n\tno_chunks = len(tokens)/5000+1;\n\tprint 'Process '+str(len(tokens))+' tokens in '+str(no_chunks)+ ' chunks..' \n\tfor l in range(0, no_chunks):\n\t\tendi =  min((starti + (len(tokens)/no_chunks) ), len(tokens))\n\t\tprint \"Tagging #\" + str(l) + \": from \" + str(starti)+ \" to \"+str(endi-1)\n\t\ttags = tags + nltk.pos_tag(tokens[starti:endi]);\n\t\t#tags = tags + pos.tag(tokens[starti:endi])[0]; \t\t\n\t\tstarti = endi;\n\tprint str(len(tags))+\" words tagged..\"",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "\tno_chunks",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "\tno_chunks = len(tokens)/5000+1;\n\tprint 'Process '+str(len(tokens))+' tokens in '+str(no_chunks)+ ' chunks..' \n\tfor l in range(0, no_chunks):\n\t\tendi =  min((starti + (len(tokens)/no_chunks) ), len(tokens))\n\t\tprint \"Tagging #\" + str(l) + \": from \" + str(starti)+ \" to \"+str(endi-1)\n\t\ttags = tags + nltk.pos_tag(tokens[starti:endi]);\n\t\t#tags = tags + pos.tag(tokens[starti:endi])[0]; \t\t\n\t\tstarti = endi;\n\tprint str(len(tags))+\" words tagged..\"\n\t# Save all the Noun and Adjective unigrams in a hash table",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "\t\tendi",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "\t\tendi =  min((starti + (len(tokens)/no_chunks) ), len(tokens))\n\t\tprint \"Tagging #\" + str(l) + \": from \" + str(starti)+ \" to \"+str(endi-1)\n\t\ttags = tags + nltk.pos_tag(tokens[starti:endi]);\n\t\t#tags = tags + pos.tag(tokens[starti:endi])[0]; \t\t\n\t\tstarti = endi;\n\tprint str(len(tags))+\" words tagged..\"\n\t# Save all the Noun and Adjective unigrams in a hash table\n\ttag_set = {'Word':'Tag'}\n\tfor tag in tags:\n\t\tif (cleanseNN([str(tag[1])]) in patterns[0:2]):",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "\t\ttags",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "\t\ttags = tags + nltk.pos_tag(tokens[starti:endi]);\n\t\t#tags = tags + pos.tag(tokens[starti:endi])[0]; \t\t\n\t\tstarti = endi;\n\tprint str(len(tags))+\" words tagged..\"\n\t# Save all the Noun and Adjective unigrams in a hash table\n\ttag_set = {'Word':'Tag'}\n\tfor tag in tags:\n\t\tif (cleanseNN([str(tag[1])]) in patterns[0:2]):\n\t\t\ttag_set[str(tag[0])] = str(tag[1])\t\t\n\t\t#print cleanseNN([str(tag[1])])",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "\t\t#tags",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "\t\t#tags = tags + pos.tag(tokens[starti:endi])[0]; \t\t\n\t\tstarti = endi;\n\tprint str(len(tags))+\" words tagged..\"\n\t# Save all the Noun and Adjective unigrams in a hash table\n\ttag_set = {'Word':'Tag'}\n\tfor tag in tags:\n\t\tif (cleanseNN([str(tag[1])]) in patterns[0:2]):\n\t\t\ttag_set[str(tag[0])] = str(tag[1])\t\t\n\t\t#print cleanseNN([str(tag[1])])\n\t#print tags",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "\t\tstarti",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "\t\tstarti = endi;\n\tprint str(len(tags))+\" words tagged..\"\n\t# Save all the Noun and Adjective unigrams in a hash table\n\ttag_set = {'Word':'Tag'}\n\tfor tag in tags:\n\t\tif (cleanseNN([str(tag[1])]) in patterns[0:2]):\n\t\t\ttag_set[str(tag[0])] = str(tag[1])\t\t\n\t\t#print cleanseNN([str(tag[1])])\n\t#print tags\n\t#print '\\n'",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "\ttag_set",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "\ttag_set = {'Word':'Tag'}\n\tfor tag in tags:\n\t\tif (cleanseNN([str(tag[1])]) in patterns[0:2]):\n\t\t\ttag_set[str(tag[0])] = str(tag[1])\t\t\n\t\t#print cleanseNN([str(tag[1])])\n\t#print tags\n\t#print '\\n'\n\treturn tag_set\n# Clean part of speech tags\ndef cleanseNN(list):",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "\t\t\ttag_set[str(tag[0])]",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "\t\t\ttag_set[str(tag[0])] = str(tag[1])\t\t\n\t\t#print cleanseNN([str(tag[1])])\n\t#print tags\n\t#print '\\n'\n\treturn tag_set\n# Clean part of speech tags\ndef cleanseNN(list):\n\tfor i in range(0, len(list)):\n\t\tfor k in range(0, len(list[i])):\n\t\t\tif(\"NN\" in list[i][k]):",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "\t\t\t\tlist[i][k]",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "\t\t\t\tlist[i][k] = \"NN\"\n\treturn list\n# Look for longest n-gram appearing in each sentence with the patterns of technical terms\ndef get_tech_ngrams(text, tag_set):\t\n\t# Normalization and Punctuation filtering=> keep sentence separators\n\ttext = text.lower()\n\tsentences = re.split('\\.(?!\\d)', text)\n\t#print sentences\n\tresults = {'ngram':'tags'};\n\tn_gram = []",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "\ttext",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "\ttext = text.lower()\n\tsentences = re.split('\\.(?!\\d)', text)\n\t#print sentences\n\tresults = {'ngram':'tags'};\n\tn_gram = []\n\ttags = []\n\tn_gram_str = ''\n\ttag_str = ''\n\tfor s in sentences:\n\t\tregex = re.compile('[%s]' % re.escape('!\"#$%&\\'()*+/:<=>?@[\\\\]^_`{|}~-'))",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "\tsentences",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "\tsentences = re.split('\\.(?!\\d)', text)\n\t#print sentences\n\tresults = {'ngram':'tags'};\n\tn_gram = []\n\ttags = []\n\tn_gram_str = ''\n\ttag_str = ''\n\tfor s in sentences:\n\t\tregex = re.compile('[%s]' % re.escape('!\"#$%&\\'()*+/:<=>?@[\\\\]^_`{|}~-'))\n\t\tText = regex.sub(' ',s)",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "\tresults",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "\tresults = {'ngram':'tags'};\n\tn_gram = []\n\ttags = []\n\tn_gram_str = ''\n\ttag_str = ''\n\tfor s in sentences:\n\t\tregex = re.compile('[%s]' % re.escape('!\"#$%&\\'()*+/:<=>?@[\\\\]^_`{|}~-'))\n\t\tText = regex.sub(' ',s)\n\t\t# Get the words\n\t\twords = word_tokenize(unicode(s, errors='ignore'))",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "\tn_gram",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "\tn_gram = []\n\ttags = []\n\tn_gram_str = ''\n\ttag_str = ''\n\tfor s in sentences:\n\t\tregex = re.compile('[%s]' % re.escape('!\"#$%&\\'()*+/:<=>?@[\\\\]^_`{|}~-'))\n\t\tText = regex.sub(' ',s)\n\t\t# Get the words\n\t\twords = word_tokenize(unicode(s, errors='ignore'))\n\t\tw_i = -1;",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "\ttags",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "\ttags = []\n\tn_gram_str = ''\n\ttag_str = ''\n\tfor s in sentences:\n\t\tregex = re.compile('[%s]' % re.escape('!\"#$%&\\'()*+/:<=>?@[\\\\]^_`{|}~-'))\n\t\tText = regex.sub(' ',s)\n\t\t# Get the words\n\t\twords = word_tokenize(unicode(s, errors='ignore'))\n\t\tw_i = -1;\n\t\t# Filter numbers ",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "\tn_gram_str",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "\tn_gram_str = ''\n\ttag_str = ''\n\tfor s in sentences:\n\t\tregex = re.compile('[%s]' % re.escape('!\"#$%&\\'()*+/:<=>?@[\\\\]^_`{|}~-'))\n\t\tText = regex.sub(' ',s)\n\t\t# Get the words\n\t\twords = word_tokenize(unicode(s, errors='ignore'))\n\t\tw_i = -1;\n\t\t# Filter numbers \n\t\t#words= [str(t) for t in raw_tokens if not str(t).isdigit() and len(str(t))>1]",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "\ttag_str",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "\ttag_str = ''\n\tfor s in sentences:\n\t\tregex = re.compile('[%s]' % re.escape('!\"#$%&\\'()*+/:<=>?@[\\\\]^_`{|}~-'))\n\t\tText = regex.sub(' ',s)\n\t\t# Get the words\n\t\twords = word_tokenize(unicode(s, errors='ignore'))\n\t\tw_i = -1;\n\t\t# Filter numbers \n\t\t#words= [str(t) for t in raw_tokens if not str(t).isdigit() and len(str(t))>1]\n\t\t#print '--->' + Text + '\\n'",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "\t\tregex",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "\t\tregex = re.compile('[%s]' % re.escape('!\"#$%&\\'()*+/:<=>?@[\\\\]^_`{|}~-'))\n\t\tText = regex.sub(' ',s)\n\t\t# Get the words\n\t\twords = word_tokenize(unicode(s, errors='ignore'))\n\t\tw_i = -1;\n\t\t# Filter numbers \n\t\t#words= [str(t) for t in raw_tokens if not str(t).isdigit() and len(str(t))>1]\n\t\t#print '--->' + Text + '\\n'\n\t\ts_result = []\n\t\tfor w in words:",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "\t\tText",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "\t\tText = regex.sub(' ',s)\n\t\t# Get the words\n\t\twords = word_tokenize(unicode(s, errors='ignore'))\n\t\tw_i = -1;\n\t\t# Filter numbers \n\t\t#words= [str(t) for t in raw_tokens if not str(t).isdigit() and len(str(t))>1]\n\t\t#print '--->' + Text + '\\n'\n\t\ts_result = []\n\t\tfor w in words:\n\t\t\tw_i = w_i + 1;",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "\t\twords",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "\t\twords = word_tokenize(unicode(s, errors='ignore'))\n\t\tw_i = -1;\n\t\t# Filter numbers \n\t\t#words= [str(t) for t in raw_tokens if not str(t).isdigit() and len(str(t))>1]\n\t\t#print '--->' + Text + '\\n'\n\t\ts_result = []\n\t\tfor w in words:\n\t\t\tw_i = w_i + 1;\n\t\t\tif (tag_set.has_key(w)):\n\t\t\t\tn_gram.append(w)",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "\t\tw_i",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "\t\tw_i = -1;\n\t\t# Filter numbers \n\t\t#words= [str(t) for t in raw_tokens if not str(t).isdigit() and len(str(t))>1]\n\t\t#print '--->' + Text + '\\n'\n\t\ts_result = []\n\t\tfor w in words:\n\t\t\tw_i = w_i + 1;\n\t\t\tif (tag_set.has_key(w)):\n\t\t\t\tn_gram.append(w)\n\t\t\t\ttags.append(tag_set[w])",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "\t\ts_result",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "\t\ts_result = []\n\t\tfor w in words:\n\t\t\tw_i = w_i + 1;\n\t\t\tif (tag_set.has_key(w)):\n\t\t\t\tn_gram.append(w)\n\t\t\t\ttags.append(tag_set[w])\n\t\t\t\t#print \"n-gram = \"+n_gram[-1]\n\t\t\t#print w+'-'+str(words.index(w))\n\t\t\t# If this is the last word in the list or a non-NJ word, we finalize the n-gram\n\t\t\tif not(tag_set.has_key(w)) or (w_i == len(words)-1):",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "\t\t\tw_i",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "\t\t\tw_i = w_i + 1;\n\t\t\tif (tag_set.has_key(w)):\n\t\t\t\tn_gram.append(w)\n\t\t\t\ttags.append(tag_set[w])\n\t\t\t\t#print \"n-gram = \"+n_gram[-1]\n\t\t\t#print w+'-'+str(words.index(w))\n\t\t\t# If this is the last word in the list or a non-NJ word, we finalize the n-gram\n\t\t\tif not(tag_set.has_key(w)) or (w_i == len(words)-1):\n\t\t\t\t# Only if we found something\n\t\t\t\tif (len(n_gram) > 1):",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "\t\t\t\t\t\tn_gram_str",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "\t\t\t\t\t\tn_gram_str = ' '.join(n_gram)\n\t\t\t\t\t\ttag_str = ', '.join(tags)\n\t\t\t\t\t\tif not(n_gram_str in s_result) :\n\t\t\t\t\t\t\ts_result.append(n_gram_str)\n\t\t\t\tn_gram_str = n_gram_str.decode('utf-8')\n\t\t\t\tif (n_gram_str in Text):\n\t\t\t\t\tif not(results.has_key(n_gram_str)):\n\t\t\t\t\t\tresults[n_gram_str] = tag_str\n\t\t\t\telse:\n\t\t\t\t\tprint('ngram not found in text: '+n_gram_str)",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "\t\t\t\t\t\ttag_str",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "\t\t\t\t\t\ttag_str = ', '.join(tags)\n\t\t\t\t\t\tif not(n_gram_str in s_result) :\n\t\t\t\t\t\t\ts_result.append(n_gram_str)\n\t\t\t\tn_gram_str = n_gram_str.decode('utf-8')\n\t\t\t\tif (n_gram_str in Text):\n\t\t\t\t\tif not(results.has_key(n_gram_str)):\n\t\t\t\t\t\tresults[n_gram_str] = tag_str\n\t\t\t\telse:\n\t\t\t\t\tprint('ngram not found in text: '+n_gram_str)\n\t\t\t\t# Restart searching for next n-gram",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "\t\t\t\tn_gram_str",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "\t\t\t\tn_gram_str = n_gram_str.decode('utf-8')\n\t\t\t\tif (n_gram_str in Text):\n\t\t\t\t\tif not(results.has_key(n_gram_str)):\n\t\t\t\t\t\tresults[n_gram_str] = tag_str\n\t\t\t\telse:\n\t\t\t\t\tprint('ngram not found in text: '+n_gram_str)\n\t\t\t\t# Restart searching for next n-gram\n\t\t\t\tn_gram = []\n\t\t\t\ttags = []\n\t\t\t\tn_gram_str = ''",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "\t\t\t\t\t\tresults[n_gram_str]",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "\t\t\t\t\t\tresults[n_gram_str] = tag_str\n\t\t\t\telse:\n\t\t\t\t\tprint('ngram not found in text: '+n_gram_str)\n\t\t\t\t# Restart searching for next n-gram\n\t\t\t\tn_gram = []\n\t\t\t\ttags = []\n\t\t\t\tn_gram_str = ''\n\t\t\t\ttag_str = ''\n\tprint(str(len(results.keys()))+\" n-grams found..\")\t\n\tresults.pop(\"ngram\")",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "\t\t\t\tn_gram",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "\t\t\t\tn_gram = []\n\t\t\t\ttags = []\n\t\t\t\tn_gram_str = ''\n\t\t\t\ttag_str = ''\n\tprint(str(len(results.keys()))+\" n-grams found..\")\t\n\tresults.pop(\"ngram\")\n\treturn results\ndef lexicon_expansion(seed_terms):\n\tprint(\"\\n Total seed terms = \" + str(len(seed_terms))+\"\\n\")\n\tterms_scores = []",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "\t\t\t\ttags",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "\t\t\t\ttags = []\n\t\t\t\tn_gram_str = ''\n\t\t\t\ttag_str = ''\n\tprint(str(len(results.keys()))+\" n-grams found..\")\t\n\tresults.pop(\"ngram\")\n\treturn results\ndef lexicon_expansion(seed_terms):\n\tprint(\"\\n Total seed terms = \" + str(len(seed_terms))+\"\\n\")\n\tterms_scores = []\n\tfor c in seed_terms:",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "\t\t\t\tn_gram_str",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "\t\t\t\tn_gram_str = ''\n\t\t\t\ttag_str = ''\n\tprint(str(len(results.keys()))+\" n-grams found..\")\t\n\tresults.pop(\"ngram\")\n\treturn results\ndef lexicon_expansion(seed_terms):\n\tprint(\"\\n Total seed terms = \" + str(len(seed_terms))+\"\\n\")\n\tterms_scores = []\n\tfor c in seed_terms:\n\t\tterms_scores.append((str(c),str(1.0)))",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "\t\t\t\ttag_str",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "\t\t\t\ttag_str = ''\n\tprint(str(len(results.keys()))+\" n-grams found..\")\t\n\tresults.pop(\"ngram\")\n\treturn results\ndef lexicon_expansion(seed_terms):\n\tprint(\"\\n Total seed terms = \" + str(len(seed_terms))+\"\\n\")\n\tterms_scores = []\n\tfor c in seed_terms:\n\t\tterms_scores.append((str(c),str(1.0)))\n\tfor c in seed_terms:",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "\tterms_scores",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "\tterms_scores = []\n\tfor c in seed_terms:\n\t\tterms_scores.append((str(c),str(1.0)))\n\tfor c in seed_terms:\n\t\tif not(c.isdigit()) and (len(c) > 3):\n\t\t\tprint(\"\\nSeed Term:\" + c)\n\t\t\t# Get the word2vec representation of the ngram by adding word vectors\n\t\t\tc_array = [s for s in c.split(' ') if s in googlenews_model.vocab]\n\t\t\tif len(c_array):\n\t\t\t\tvector = np.sum(googlenews_model[c_array], axis=0)\t\t\t",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "\t\t\tc_array",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "\t\t\tc_array = [s for s in c.split(' ') if s in googlenews_model.vocab]\n\t\t\tif len(c_array):\n\t\t\t\tvector = np.sum(googlenews_model[c_array], axis=0)\t\t\t\n\t\t\t\t# Get the most similar vectors \n\t\t\t\tmost_similar = googlenews_model.similar_by_vector(vector, topn=20, restrict_vocab=None)\n\t\t\t\t# Filter highly similar vectors\n\t\t\t\tfor (term,score) in most_similar:\n\t\t\t\t\tterm = term.lower().replace(\"_\",\" \")\n\t\t\t\t\tif (score > 0.4):\n\t\t\t\t\t\tif not(term in seed_terms) and not(term.isdigit()) and (len(term.split(' ')) < 3):",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "\t\t\t\tvector",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "\t\t\t\tvector = np.sum(googlenews_model[c_array], axis=0)\t\t\t\n\t\t\t\t# Get the most similar vectors \n\t\t\t\tmost_similar = googlenews_model.similar_by_vector(vector, topn=20, restrict_vocab=None)\n\t\t\t\t# Filter highly similar vectors\n\t\t\t\tfor (term,score) in most_similar:\n\t\t\t\t\tterm = term.lower().replace(\"_\",\" \")\n\t\t\t\t\tif (score > 0.4):\n\t\t\t\t\t\tif not(term in seed_terms) and not(term.isdigit()) and (len(term.split(' ')) < 3):\n\t\t\t\t\t\t\tseed_terms.append(str(term))\n\t\t\t\t\t\t\tterms_scores.append((str(term),round(score,2)))\t",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "\t\t\t\tmost_similar",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "\t\t\t\tmost_similar = googlenews_model.similar_by_vector(vector, topn=20, restrict_vocab=None)\n\t\t\t\t# Filter highly similar vectors\n\t\t\t\tfor (term,score) in most_similar:\n\t\t\t\t\tterm = term.lower().replace(\"_\",\" \")\n\t\t\t\t\tif (score > 0.4):\n\t\t\t\t\t\tif not(term in seed_terms) and not(term.isdigit()) and (len(term.split(' ')) < 3):\n\t\t\t\t\t\t\tseed_terms.append(str(term))\n\t\t\t\t\t\t\tterms_scores.append((str(term),round(score,2)))\t\n\t\t\t\t\t\t\tprint(term)\n\tprint(len(seed_terms))",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "\t\t\t\t\tterm",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "\t\t\t\t\tterm = term.lower().replace(\"_\",\" \")\n\t\t\t\t\tif (score > 0.4):\n\t\t\t\t\t\tif not(term in seed_terms) and not(term.isdigit()) and (len(term.split(' ')) < 3):\n\t\t\t\t\t\t\tseed_terms.append(str(term))\n\t\t\t\t\t\t\tterms_scores.append((str(term),round(score,2)))\t\n\t\t\t\t\t\t\tprint(term)\n\tprint(len(seed_terms))\n\treturn seed_terms\ndef main():\n\tos.chdir(\"./dataset\")",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "\tngrams_set",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "\tngrams_set = set()\n\twith open('output.csv', 'w') as output:\n\t\tcsvwriter = csv.writer(output)\n\t\tcsvwriter.writerow([\"Data File\", \"Narrative\", \"N-grams\"])\n\t\tfor file in glob.glob(\"*.txt\"):\n\t\t\twith open(file, 'r') as reader:\n\t\t\t\ttext = ''\n\t\t\t\tfor line in reader:\n\t\t\t\t\ttext = text + line.rstrip('\\n\\r').lower()\n\t\t\t\tprint(\"\\nProcessing \"+file)",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "\t\tcsvwriter",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "\t\tcsvwriter = csv.writer(output)\n\t\tcsvwriter.writerow([\"Data File\", \"Narrative\", \"N-grams\"])\n\t\tfor file in glob.glob(\"*.txt\"):\n\t\t\twith open(file, 'r') as reader:\n\t\t\t\ttext = ''\n\t\t\t\tfor line in reader:\n\t\t\t\t\ttext = text + line.rstrip('\\n\\r').lower()\n\t\t\t\tprint(\"\\nProcessing \"+file)\n\t\t\t\t# Tokenization\n\t\t\t\ttokens = get_tokens(text)",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "\t\t\t\ttext",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "\t\t\t\ttext = ''\n\t\t\t\tfor line in reader:\n\t\t\t\t\ttext = text + line.rstrip('\\n\\r').lower()\n\t\t\t\tprint(\"\\nProcessing \"+file)\n\t\t\t\t# Tokenization\n\t\t\t\ttokens = get_tokens(text)\n\t\t\t\t# Part of speech Tagging\n\t\t\t\ttag_set = get_pos_tags(tokens)\n\t\t\t\t# Technical N-gram extraction\n\t\t\t\tngrams = get_tech_ngrams(text, tag_set)",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "\t\t\t\t\ttext",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "\t\t\t\t\ttext = text + line.rstrip('\\n\\r').lower()\n\t\t\t\tprint(\"\\nProcessing \"+file)\n\t\t\t\t# Tokenization\n\t\t\t\ttokens = get_tokens(text)\n\t\t\t\t# Part of speech Tagging\n\t\t\t\ttag_set = get_pos_tags(tokens)\n\t\t\t\t# Technical N-gram extraction\n\t\t\t\tngrams = get_tech_ngrams(text, tag_set)\n\t\t\t\t# Write to output\n\t\t\t\tcsvwriter.writerow([file, text, ngrams.keys()])",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "\t\t\t\ttokens",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "\t\t\t\ttokens = get_tokens(text)\n\t\t\t\t# Part of speech Tagging\n\t\t\t\ttag_set = get_pos_tags(tokens)\n\t\t\t\t# Technical N-gram extraction\n\t\t\t\tngrams = get_tech_ngrams(text, tag_set)\n\t\t\t\t# Write to output\n\t\t\t\tcsvwriter.writerow([file, text, ngrams.keys()])\n\t\t\t\tngrams_set = ngrams_set.union(set(ngrams.keys()))\n\tngrams_list = lexicon_expansion(list(ngrams_set))\n\twith open('ngrams.txt', 'w') as writer:",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "\t\t\t\ttag_set",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "\t\t\t\ttag_set = get_pos_tags(tokens)\n\t\t\t\t# Technical N-gram extraction\n\t\t\t\tngrams = get_tech_ngrams(text, tag_set)\n\t\t\t\t# Write to output\n\t\t\t\tcsvwriter.writerow([file, text, ngrams.keys()])\n\t\t\t\tngrams_set = ngrams_set.union(set(ngrams.keys()))\n\tngrams_list = lexicon_expansion(list(ngrams_set))\n\twith open('ngrams.txt', 'w') as writer:\n\t\tfor s in ngrams_list:\n\t\t\twriter.write(str(s)+'\\n')",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "\t\t\t\tngrams",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "\t\t\t\tngrams = get_tech_ngrams(text, tag_set)\n\t\t\t\t# Write to output\n\t\t\t\tcsvwriter.writerow([file, text, ngrams.keys()])\n\t\t\t\tngrams_set = ngrams_set.union(set(ngrams.keys()))\n\tngrams_list = lexicon_expansion(list(ngrams_set))\n\twith open('ngrams.txt', 'w') as writer:\n\t\tfor s in ngrams_list:\n\t\t\twriter.write(str(s)+'\\n')\nif __name__ == '__main__':\n    sys.exit(main())",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "\t\t\t\tngrams_set",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "\t\t\t\tngrams_set = ngrams_set.union(set(ngrams.keys()))\n\tngrams_list = lexicon_expansion(list(ngrams_set))\n\twith open('ngrams.txt', 'w') as writer:\n\t\tfor s in ngrams_list:\n\t\t\twriter.write(str(s)+'\\n')\nif __name__ == '__main__':\n    sys.exit(main())",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "\tngrams_list",
        "kind": 5,
        "importPath": "ETC.main",
        "description": "ETC.main",
        "peekOfCode": "\tngrams_list = lexicon_expansion(list(ngrams_set))\n\twith open('ngrams.txt', 'w') as writer:\n\t\tfor s in ngrams_list:\n\t\t\twriter.write(str(s)+'\\n')\nif __name__ == '__main__':\n    sys.exit(main())",
        "detail": "ETC.main",
        "documentation": {}
    },
    {
        "label": "get_tokens",
        "kind": 2,
        "importPath": "ETC.secondary",
        "description": "ETC.secondary",
        "peekOfCode": "def get_tokens(text):\n\tstops = set(stopwords.words('english'))\n\tregex = re.compile('[%s]' % re.escape('!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~'))\n\t# Get the sentences\n\tsentences = regex.sub(' ', text)\n\t# Get the words\n\traw_tokens = list(set(word_tokenize(unicode(sentences, errors='ignore'))))\n\t# Filter numbers and characters\n\t#tokens= [str(t) for t in raw_tokens if str(t) not in stops and not str(t).isdigit() and len(str(t))>1]\n\ttokens= [str(t) for t in raw_tokens if not str(t).isdigit() and len(str(t))>1]",
        "detail": "ETC.secondary",
        "documentation": {}
    },
    {
        "label": "get_pos_tags",
        "kind": 2,
        "importPath": "ETC.secondary",
        "description": "ETC.secondary",
        "peekOfCode": "def get_pos_tags(tokens):\n\tregex = re.compile('[%s]' % re.escape('!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~'))\n\ttags = [];\n\tstarti = 0\n\tendi = 0\n\tno_chunks = len(tokens)/5000+1;\n\tprint 'Process '+str(len(tokens))+' tokens in '+str(no_chunks)+ ' chunks..'\n\tfor l in range(0, no_chunks):\n\t\tendi =  min((starti + (len(tokens)/no_chunks) ), len(tokens))\n\t\tprint \"Tagging #\" + str(l) + \": from \" + str(starti)+ \" to \"+str(endi-1)",
        "detail": "ETC.secondary",
        "documentation": {}
    },
    {
        "label": "cleanseNN",
        "kind": 2,
        "importPath": "ETC.secondary",
        "description": "ETC.secondary",
        "peekOfCode": "def cleanseNN(list):\n\tfor i in range(0, len(list)):\n\t\tfor k in range(0, len(list[i])):\n\t\t\tif(\"NN\" in list[i][k]):\n\t\t\t\tlist[i][k] = \"NN\"\n\treturn list\n# Look for longest n-gram appearing in each sentence with the patterns of technical terms\ndef get_tech_ngrams(text, tag_set):\n\t# Normalization and Punctuation filtering=> keep sentence separators\n\ttext = text.lower()",
        "detail": "ETC.secondary",
        "documentation": {}
    },
    {
        "label": "get_tech_ngrams",
        "kind": 2,
        "importPath": "ETC.secondary",
        "description": "ETC.secondary",
        "peekOfCode": "def get_tech_ngrams(text, tag_set):\n\t# Normalization and Punctuation filtering=> keep sentence separators\n\ttext = text.lower()\n\tsentences = re.split('\\.(?!\\d)', text)\n\t#print sentences\n\tresults = {'ngram':'tags'};\n\tn_gram = []\n\ttags = []\n\tn_gram_str = ''\n\ttag_str = ''",
        "detail": "ETC.secondary",
        "documentation": {}
    },
    {
        "label": "mutual_inf",
        "kind": 2,
        "importPath": "ETC.secondary",
        "description": "ETC.secondary",
        "peekOfCode": "def mutual_info (file, n_gram):\n    N00=0\n    N10=0\n    N01=0\n    N11=0\n    N=N00+N10+N01+N11\n    mi_score=[]\n    file_ems=file[:10]\n    file_nonems=file[10:20]\n    result_mi={'ngram':'mi score'}",
        "detail": "ETC.secondary",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "ETC.secondary",
        "description": "ETC.secondary",
        "peekOfCode": "def main():\n\tos.chdir(\"./dataset\")\n\t# Set default encoding of python to utf8\n\treload(sys)\n\tsys.setdefaultencoding('utf8')\n\twith open('output.csv', 'w') as output:\n\t\tcsvwriter = csv.writer(output)\n\t\tcsvwriter.writerow([\"Data File\",  \"MI score\"])\n\t\tfor file in glob.glob(\"*.txt\"):\n\t\t\twith open(file, 'r') as reader:",
        "detail": "ETC.secondary",
        "documentation": {}
    },
    {
        "label": "bigram_measures",
        "kind": 5,
        "importPath": "ETC.secondary",
        "description": "ETC.secondary",
        "peekOfCode": "bigram_measures = nltk.collocations.BigramAssocMeasures()\ntrigram_measures = nltk.collocations.TrigramAssocMeasures()\n#pos = StanfordPOSTagger('./stanford-postagger-2013-06-20/models/english-left3words-distsim.tagger',\n#                './stanford-postagger-2013-06-20/stanford-postagger.jar')\n# N-gram patterns for technical terms\npatterns = [['NN'],['JJ'], ['JJ','JJ'],['JJ', 'NN'], ['NN','NN'], ['JJ', 'JJ', 'NN'], ['JJ', 'NN', 'NN'],\n            ['NN', 'JJ', 'NN'], ['NN', 'NN', 'NN'],['NN', 'IN', 'NN'],['JJ','JJ','JJ'],\n\t\t\t['JJ', 'JJ', 'JJ','NN'], ['JJ', 'JJ', 'NN','NN'], ['JJ', 'NN', 'NN','NN'],['JJ', 'NN', 'JJ','NN'],['NN', 'JJ', 'JJ','NN'],\n\t\t\t['NN', 'NN', 'NN','NN'],['NN','JJ', 'NN','NN'], ['NN', 'NN','JJ', 'NN']]\n# For filtering junk",
        "detail": "ETC.secondary",
        "documentation": {}
    },
    {
        "label": "trigram_measures",
        "kind": 5,
        "importPath": "ETC.secondary",
        "description": "ETC.secondary",
        "peekOfCode": "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n#pos = StanfordPOSTagger('./stanford-postagger-2013-06-20/models/english-left3words-distsim.tagger',\n#                './stanford-postagger-2013-06-20/stanford-postagger.jar')\n# N-gram patterns for technical terms\npatterns = [['NN'],['JJ'], ['JJ','JJ'],['JJ', 'NN'], ['NN','NN'], ['JJ', 'JJ', 'NN'], ['JJ', 'NN', 'NN'],\n            ['NN', 'JJ', 'NN'], ['NN', 'NN', 'NN'],['NN', 'IN', 'NN'],['JJ','JJ','JJ'],\n\t\t\t['JJ', 'JJ', 'JJ','NN'], ['JJ', 'JJ', 'NN','NN'], ['JJ', 'NN', 'NN','NN'],['JJ', 'NN', 'JJ','NN'],['NN', 'JJ', 'JJ','NN'],\n\t\t\t['NN', 'NN', 'NN','NN'],['NN','JJ', 'NN','NN'], ['NN', 'NN','JJ', 'NN']]\n# For filtering junk\nnon_tech_words = []",
        "detail": "ETC.secondary",
        "documentation": {}
    },
    {
        "label": "#pos",
        "kind": 5,
        "importPath": "ETC.secondary",
        "description": "ETC.secondary",
        "peekOfCode": "#pos = StanfordPOSTagger('./stanford-postagger-2013-06-20/models/english-left3words-distsim.tagger',\n#                './stanford-postagger-2013-06-20/stanford-postagger.jar')\n# N-gram patterns for technical terms\npatterns = [['NN'],['JJ'], ['JJ','JJ'],['JJ', 'NN'], ['NN','NN'], ['JJ', 'JJ', 'NN'], ['JJ', 'NN', 'NN'],\n            ['NN', 'JJ', 'NN'], ['NN', 'NN', 'NN'],['NN', 'IN', 'NN'],['JJ','JJ','JJ'],\n\t\t\t['JJ', 'JJ', 'JJ','NN'], ['JJ', 'JJ', 'NN','NN'], ['JJ', 'NN', 'NN','NN'],['JJ', 'NN', 'JJ','NN'],['NN', 'JJ', 'JJ','NN'],\n\t\t\t['NN', 'NN', 'NN','NN'],['NN','JJ', 'NN','NN'], ['NN', 'NN','JJ', 'NN']]\n# For filtering junk\nnon_tech_words = []\n# Tokenization",
        "detail": "ETC.secondary",
        "documentation": {}
    },
    {
        "label": "patterns",
        "kind": 5,
        "importPath": "ETC.secondary",
        "description": "ETC.secondary",
        "peekOfCode": "patterns = [['NN'],['JJ'], ['JJ','JJ'],['JJ', 'NN'], ['NN','NN'], ['JJ', 'JJ', 'NN'], ['JJ', 'NN', 'NN'],\n            ['NN', 'JJ', 'NN'], ['NN', 'NN', 'NN'],['NN', 'IN', 'NN'],['JJ','JJ','JJ'],\n\t\t\t['JJ', 'JJ', 'JJ','NN'], ['JJ', 'JJ', 'NN','NN'], ['JJ', 'NN', 'NN','NN'],['JJ', 'NN', 'JJ','NN'],['NN', 'JJ', 'JJ','NN'],\n\t\t\t['NN', 'NN', 'NN','NN'],['NN','JJ', 'NN','NN'], ['NN', 'NN','JJ', 'NN']]\n# For filtering junk\nnon_tech_words = []\n# Tokenization\ndef get_tokens(text):\n\tstops = set(stopwords.words('english'))\n\tregex = re.compile('[%s]' % re.escape('!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~'))",
        "detail": "ETC.secondary",
        "documentation": {}
    },
    {
        "label": "non_tech_words",
        "kind": 5,
        "importPath": "ETC.secondary",
        "description": "ETC.secondary",
        "peekOfCode": "non_tech_words = []\n# Tokenization\ndef get_tokens(text):\n\tstops = set(stopwords.words('english'))\n\tregex = re.compile('[%s]' % re.escape('!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~'))\n\t# Get the sentences\n\tsentences = regex.sub(' ', text)\n\t# Get the words\n\traw_tokens = list(set(word_tokenize(unicode(sentences, errors='ignore'))))\n\t# Filter numbers and characters",
        "detail": "ETC.secondary",
        "documentation": {}
    },
    {
        "label": "\tstops",
        "kind": 5,
        "importPath": "ETC.secondary",
        "description": "ETC.secondary",
        "peekOfCode": "\tstops = set(stopwords.words('english'))\n\tregex = re.compile('[%s]' % re.escape('!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~'))\n\t# Get the sentences\n\tsentences = regex.sub(' ', text)\n\t# Get the words\n\traw_tokens = list(set(word_tokenize(unicode(sentences, errors='ignore'))))\n\t# Filter numbers and characters\n\t#tokens= [str(t) for t in raw_tokens if str(t) not in stops and not str(t).isdigit() and len(str(t))>1]\n\ttokens= [str(t) for t in raw_tokens if not str(t).isdigit() and len(str(t))>1]\n\ttokens = [t for t in tokens if t not in non_tech_words and t.isalpha()]",
        "detail": "ETC.secondary",
        "documentation": {}
    },
    {
        "label": "\tregex",
        "kind": 5,
        "importPath": "ETC.secondary",
        "description": "ETC.secondary",
        "peekOfCode": "\tregex = re.compile('[%s]' % re.escape('!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~'))\n\t# Get the sentences\n\tsentences = regex.sub(' ', text)\n\t# Get the words\n\traw_tokens = list(set(word_tokenize(unicode(sentences, errors='ignore'))))\n\t# Filter numbers and characters\n\t#tokens= [str(t) for t in raw_tokens if str(t) not in stops and not str(t).isdigit() and len(str(t))>1]\n\ttokens= [str(t) for t in raw_tokens if not str(t).isdigit() and len(str(t))>1]\n\ttokens = [t for t in tokens if t not in non_tech_words and t.isalpha()]\n\treturn tokens",
        "detail": "ETC.secondary",
        "documentation": {}
    },
    {
        "label": "\tsentences",
        "kind": 5,
        "importPath": "ETC.secondary",
        "description": "ETC.secondary",
        "peekOfCode": "\tsentences = regex.sub(' ', text)\n\t# Get the words\n\traw_tokens = list(set(word_tokenize(unicode(sentences, errors='ignore'))))\n\t# Filter numbers and characters\n\t#tokens= [str(t) for t in raw_tokens if str(t) not in stops and not str(t).isdigit() and len(str(t))>1]\n\ttokens= [str(t) for t in raw_tokens if not str(t).isdigit() and len(str(t))>1]\n\ttokens = [t for t in tokens if t not in non_tech_words and t.isalpha()]\n\treturn tokens\n# Part of Speech Tagging\ndef get_pos_tags(tokens):",
        "detail": "ETC.secondary",
        "documentation": {}
    },
    {
        "label": "\traw_tokens",
        "kind": 5,
        "importPath": "ETC.secondary",
        "description": "ETC.secondary",
        "peekOfCode": "\traw_tokens = list(set(word_tokenize(unicode(sentences, errors='ignore'))))\n\t# Filter numbers and characters\n\t#tokens= [str(t) for t in raw_tokens if str(t) not in stops and not str(t).isdigit() and len(str(t))>1]\n\ttokens= [str(t) for t in raw_tokens if not str(t).isdigit() and len(str(t))>1]\n\ttokens = [t for t in tokens if t not in non_tech_words and t.isalpha()]\n\treturn tokens\n# Part of Speech Tagging\ndef get_pos_tags(tokens):\n\tregex = re.compile('[%s]' % re.escape('!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~'))\n\ttags = [];",
        "detail": "ETC.secondary",
        "documentation": {}
    },
    {
        "label": "\ttokens",
        "kind": 5,
        "importPath": "ETC.secondary",
        "description": "ETC.secondary",
        "peekOfCode": "\ttokens = [t for t in tokens if t not in non_tech_words and t.isalpha()]\n\treturn tokens\n# Part of Speech Tagging\ndef get_pos_tags(tokens):\n\tregex = re.compile('[%s]' % re.escape('!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~'))\n\ttags = [];\n\tstarti = 0\n\tendi = 0\n\tno_chunks = len(tokens)/5000+1;\n\tprint 'Process '+str(len(tokens))+' tokens in '+str(no_chunks)+ ' chunks..'",
        "detail": "ETC.secondary",
        "documentation": {}
    },
    {
        "label": "\tregex",
        "kind": 5,
        "importPath": "ETC.secondary",
        "description": "ETC.secondary",
        "peekOfCode": "\tregex = re.compile('[%s]' % re.escape('!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~'))\n\ttags = [];\n\tstarti = 0\n\tendi = 0\n\tno_chunks = len(tokens)/5000+1;\n\tprint 'Process '+str(len(tokens))+' tokens in '+str(no_chunks)+ ' chunks..'\n\tfor l in range(0, no_chunks):\n\t\tendi =  min((starti + (len(tokens)/no_chunks) ), len(tokens))\n\t\tprint \"Tagging #\" + str(l) + \": from \" + str(starti)+ \" to \"+str(endi-1)\n\t\ttags = tags + nltk.pos_tag(tokens[starti:endi]);",
        "detail": "ETC.secondary",
        "documentation": {}
    },
    {
        "label": "\ttags",
        "kind": 5,
        "importPath": "ETC.secondary",
        "description": "ETC.secondary",
        "peekOfCode": "\ttags = [];\n\tstarti = 0\n\tendi = 0\n\tno_chunks = len(tokens)/5000+1;\n\tprint 'Process '+str(len(tokens))+' tokens in '+str(no_chunks)+ ' chunks..'\n\tfor l in range(0, no_chunks):\n\t\tendi =  min((starti + (len(tokens)/no_chunks) ), len(tokens))\n\t\tprint \"Tagging #\" + str(l) + \": from \" + str(starti)+ \" to \"+str(endi-1)\n\t\ttags = tags + nltk.pos_tag(tokens[starti:endi]);\n\t\t#tags = tags + pos.tag(tokens[starti:endi])[0];",
        "detail": "ETC.secondary",
        "documentation": {}
    },
    {
        "label": "\tstarti",
        "kind": 5,
        "importPath": "ETC.secondary",
        "description": "ETC.secondary",
        "peekOfCode": "\tstarti = 0\n\tendi = 0\n\tno_chunks = len(tokens)/5000+1;\n\tprint 'Process '+str(len(tokens))+' tokens in '+str(no_chunks)+ ' chunks..'\n\tfor l in range(0, no_chunks):\n\t\tendi =  min((starti + (len(tokens)/no_chunks) ), len(tokens))\n\t\tprint \"Tagging #\" + str(l) + \": from \" + str(starti)+ \" to \"+str(endi-1)\n\t\ttags = tags + nltk.pos_tag(tokens[starti:endi]);\n\t\t#tags = tags + pos.tag(tokens[starti:endi])[0];\n\t\tstarti = endi;",
        "detail": "ETC.secondary",
        "documentation": {}
    },
    {
        "label": "\tendi",
        "kind": 5,
        "importPath": "ETC.secondary",
        "description": "ETC.secondary",
        "peekOfCode": "\tendi = 0\n\tno_chunks = len(tokens)/5000+1;\n\tprint 'Process '+str(len(tokens))+' tokens in '+str(no_chunks)+ ' chunks..'\n\tfor l in range(0, no_chunks):\n\t\tendi =  min((starti + (len(tokens)/no_chunks) ), len(tokens))\n\t\tprint \"Tagging #\" + str(l) + \": from \" + str(starti)+ \" to \"+str(endi-1)\n\t\ttags = tags + nltk.pos_tag(tokens[starti:endi]);\n\t\t#tags = tags + pos.tag(tokens[starti:endi])[0];\n\t\tstarti = endi;\n\tprint str(len(tags))+\" words tagged..\"",
        "detail": "ETC.secondary",
        "documentation": {}
    },
    {
        "label": "\tno_chunks",
        "kind": 5,
        "importPath": "ETC.secondary",
        "description": "ETC.secondary",
        "peekOfCode": "\tno_chunks = len(tokens)/5000+1;\n\tprint 'Process '+str(len(tokens))+' tokens in '+str(no_chunks)+ ' chunks..'\n\tfor l in range(0, no_chunks):\n\t\tendi =  min((starti + (len(tokens)/no_chunks) ), len(tokens))\n\t\tprint \"Tagging #\" + str(l) + \": from \" + str(starti)+ \" to \"+str(endi-1)\n\t\ttags = tags + nltk.pos_tag(tokens[starti:endi]);\n\t\t#tags = tags + pos.tag(tokens[starti:endi])[0];\n\t\tstarti = endi;\n\tprint str(len(tags))+\" words tagged..\"\n\t# Save all the Noun and Adjective unigrams in a hash table",
        "detail": "ETC.secondary",
        "documentation": {}
    },
    {
        "label": "\t\tendi",
        "kind": 5,
        "importPath": "ETC.secondary",
        "description": "ETC.secondary",
        "peekOfCode": "\t\tendi =  min((starti + (len(tokens)/no_chunks) ), len(tokens))\n\t\tprint \"Tagging #\" + str(l) + \": from \" + str(starti)+ \" to \"+str(endi-1)\n\t\ttags = tags + nltk.pos_tag(tokens[starti:endi]);\n\t\t#tags = tags + pos.tag(tokens[starti:endi])[0];\n\t\tstarti = endi;\n\tprint str(len(tags))+\" words tagged..\"\n\t# Save all the Noun and Adjective unigrams in a hash table\n\ttag_set = {'Word':'Tag'}\n\tfor tag in tags:\n\t\tif (cleanseNN([str(tag[1])]) in patterns[0:2]):",
        "detail": "ETC.secondary",
        "documentation": {}
    },
    {
        "label": "\t\ttags",
        "kind": 5,
        "importPath": "ETC.secondary",
        "description": "ETC.secondary",
        "peekOfCode": "\t\ttags = tags + nltk.pos_tag(tokens[starti:endi]);\n\t\t#tags = tags + pos.tag(tokens[starti:endi])[0];\n\t\tstarti = endi;\n\tprint str(len(tags))+\" words tagged..\"\n\t# Save all the Noun and Adjective unigrams in a hash table\n\ttag_set = {'Word':'Tag'}\n\tfor tag in tags:\n\t\tif (cleanseNN([str(tag[1])]) in patterns[0:2]):\n\t\t\ttag_set[str(tag[0])] = str(tag[1])\n\t\t#print cleanseNN([str(tag[1])])",
        "detail": "ETC.secondary",
        "documentation": {}
    },
    {
        "label": "\t\t#tags",
        "kind": 5,
        "importPath": "ETC.secondary",
        "description": "ETC.secondary",
        "peekOfCode": "\t\t#tags = tags + pos.tag(tokens[starti:endi])[0];\n\t\tstarti = endi;\n\tprint str(len(tags))+\" words tagged..\"\n\t# Save all the Noun and Adjective unigrams in a hash table\n\ttag_set = {'Word':'Tag'}\n\tfor tag in tags:\n\t\tif (cleanseNN([str(tag[1])]) in patterns[0:2]):\n\t\t\ttag_set[str(tag[0])] = str(tag[1])\n\t\t#print cleanseNN([str(tag[1])])\n\t#print tags",
        "detail": "ETC.secondary",
        "documentation": {}
    },
    {
        "label": "\t\tstarti",
        "kind": 5,
        "importPath": "ETC.secondary",
        "description": "ETC.secondary",
        "peekOfCode": "\t\tstarti = endi;\n\tprint str(len(tags))+\" words tagged..\"\n\t# Save all the Noun and Adjective unigrams in a hash table\n\ttag_set = {'Word':'Tag'}\n\tfor tag in tags:\n\t\tif (cleanseNN([str(tag[1])]) in patterns[0:2]):\n\t\t\ttag_set[str(tag[0])] = str(tag[1])\n\t\t#print cleanseNN([str(tag[1])])\n\t#print tags\n\t#print '\\n'",
        "detail": "ETC.secondary",
        "documentation": {}
    },
    {
        "label": "\ttag_set",
        "kind": 5,
        "importPath": "ETC.secondary",
        "description": "ETC.secondary",
        "peekOfCode": "\ttag_set = {'Word':'Tag'}\n\tfor tag in tags:\n\t\tif (cleanseNN([str(tag[1])]) in patterns[0:2]):\n\t\t\ttag_set[str(tag[0])] = str(tag[1])\n\t\t#print cleanseNN([str(tag[1])])\n\t#print tags\n\t#print '\\n'\n\treturn tag_set\n# Clean part of speech tags\ndef cleanseNN(list):",
        "detail": "ETC.secondary",
        "documentation": {}
    },
    {
        "label": "\t\t\ttag_set[str(tag[0])]",
        "kind": 5,
        "importPath": "ETC.secondary",
        "description": "ETC.secondary",
        "peekOfCode": "\t\t\ttag_set[str(tag[0])] = str(tag[1])\n\t\t#print cleanseNN([str(tag[1])])\n\t#print tags\n\t#print '\\n'\n\treturn tag_set\n# Clean part of speech tags\ndef cleanseNN(list):\n\tfor i in range(0, len(list)):\n\t\tfor k in range(0, len(list[i])):\n\t\t\tif(\"NN\" in list[i][k]):",
        "detail": "ETC.secondary",
        "documentation": {}
    },
    {
        "label": "\t\t\t\tlist[i][k]",
        "kind": 5,
        "importPath": "ETC.secondary",
        "description": "ETC.secondary",
        "peekOfCode": "\t\t\t\tlist[i][k] = \"NN\"\n\treturn list\n# Look for longest n-gram appearing in each sentence with the patterns of technical terms\ndef get_tech_ngrams(text, tag_set):\n\t# Normalization and Punctuation filtering=> keep sentence separators\n\ttext = text.lower()\n\tsentences = re.split('\\.(?!\\d)', text)\n\t#print sentences\n\tresults = {'ngram':'tags'};\n\tn_gram = []",
        "detail": "ETC.secondary",
        "documentation": {}
    },
    {
        "label": "\ttext",
        "kind": 5,
        "importPath": "ETC.secondary",
        "description": "ETC.secondary",
        "peekOfCode": "\ttext = text.lower()\n\tsentences = re.split('\\.(?!\\d)', text)\n\t#print sentences\n\tresults = {'ngram':'tags'};\n\tn_gram = []\n\ttags = []\n\tn_gram_str = ''\n\ttag_str = ''\n\tfor s in sentences:\n\t\tregex = re.compile('[%s]' % re.escape('!\"#$%&\\'()*+/:<=>?@[\\\\]^_`{|}~-'))",
        "detail": "ETC.secondary",
        "documentation": {}
    },
    {
        "label": "\tsentences",
        "kind": 5,
        "importPath": "ETC.secondary",
        "description": "ETC.secondary",
        "peekOfCode": "\tsentences = re.split('\\.(?!\\d)', text)\n\t#print sentences\n\tresults = {'ngram':'tags'};\n\tn_gram = []\n\ttags = []\n\tn_gram_str = ''\n\ttag_str = ''\n\tfor s in sentences:\n\t\tregex = re.compile('[%s]' % re.escape('!\"#$%&\\'()*+/:<=>?@[\\\\]^_`{|}~-'))\n\t\tText = regex.sub(' ',s)",
        "detail": "ETC.secondary",
        "documentation": {}
    },
    {
        "label": "\tresults",
        "kind": 5,
        "importPath": "ETC.secondary",
        "description": "ETC.secondary",
        "peekOfCode": "\tresults = {'ngram':'tags'};\n\tn_gram = []\n\ttags = []\n\tn_gram_str = ''\n\ttag_str = ''\n\tfor s in sentences:\n\t\tregex = re.compile('[%s]' % re.escape('!\"#$%&\\'()*+/:<=>?@[\\\\]^_`{|}~-'))\n\t\tText = regex.sub(' ',s)\n\t\t# Get the words\n\t\twords = word_tokenize(unicode(s, errors='ignore'))",
        "detail": "ETC.secondary",
        "documentation": {}
    },
    {
        "label": "\tn_gram",
        "kind": 5,
        "importPath": "ETC.secondary",
        "description": "ETC.secondary",
        "peekOfCode": "\tn_gram = []\n\ttags = []\n\tn_gram_str = ''\n\ttag_str = ''\n\tfor s in sentences:\n\t\tregex = re.compile('[%s]' % re.escape('!\"#$%&\\'()*+/:<=>?@[\\\\]^_`{|}~-'))\n\t\tText = regex.sub(' ',s)\n\t\t# Get the words\n\t\twords = word_tokenize(unicode(s, errors='ignore'))\n\t\tw_i = -1;",
        "detail": "ETC.secondary",
        "documentation": {}
    },
    {
        "label": "\ttags",
        "kind": 5,
        "importPath": "ETC.secondary",
        "description": "ETC.secondary",
        "peekOfCode": "\ttags = []\n\tn_gram_str = ''\n\ttag_str = ''\n\tfor s in sentences:\n\t\tregex = re.compile('[%s]' % re.escape('!\"#$%&\\'()*+/:<=>?@[\\\\]^_`{|}~-'))\n\t\tText = regex.sub(' ',s)\n\t\t# Get the words\n\t\twords = word_tokenize(unicode(s, errors='ignore'))\n\t\tw_i = -1;\n\t\t# Filter numbers",
        "detail": "ETC.secondary",
        "documentation": {}
    },
    {
        "label": "\tn_gram_str",
        "kind": 5,
        "importPath": "ETC.secondary",
        "description": "ETC.secondary",
        "peekOfCode": "\tn_gram_str = ''\n\ttag_str = ''\n\tfor s in sentences:\n\t\tregex = re.compile('[%s]' % re.escape('!\"#$%&\\'()*+/:<=>?@[\\\\]^_`{|}~-'))\n\t\tText = regex.sub(' ',s)\n\t\t# Get the words\n\t\twords = word_tokenize(unicode(s, errors='ignore'))\n\t\tw_i = -1;\n\t\t# Filter numbers\n\t\t#words= [str(t) for t in raw_tokens if not str(t).isdigit() and len(str(t))>1]",
        "detail": "ETC.secondary",
        "documentation": {}
    },
    {
        "label": "\ttag_str",
        "kind": 5,
        "importPath": "ETC.secondary",
        "description": "ETC.secondary",
        "peekOfCode": "\ttag_str = ''\n\tfor s in sentences:\n\t\tregex = re.compile('[%s]' % re.escape('!\"#$%&\\'()*+/:<=>?@[\\\\]^_`{|}~-'))\n\t\tText = regex.sub(' ',s)\n\t\t# Get the words\n\t\twords = word_tokenize(unicode(s, errors='ignore'))\n\t\tw_i = -1;\n\t\t# Filter numbers\n\t\t#words= [str(t) for t in raw_tokens if not str(t).isdigit() and len(str(t))>1]\n\t\t#print '--->' + Text + '\\n'",
        "detail": "ETC.secondary",
        "documentation": {}
    },
    {
        "label": "\t\tregex",
        "kind": 5,
        "importPath": "ETC.secondary",
        "description": "ETC.secondary",
        "peekOfCode": "\t\tregex = re.compile('[%s]' % re.escape('!\"#$%&\\'()*+/:<=>?@[\\\\]^_`{|}~-'))\n\t\tText = regex.sub(' ',s)\n\t\t# Get the words\n\t\twords = word_tokenize(unicode(s, errors='ignore'))\n\t\tw_i = -1;\n\t\t# Filter numbers\n\t\t#words= [str(t) for t in raw_tokens if not str(t).isdigit() and len(str(t))>1]\n\t\t#print '--->' + Text + '\\n'\n\t\ts_result = []\n\t\tfor w in words:",
        "detail": "ETC.secondary",
        "documentation": {}
    },
    {
        "label": "\t\tText",
        "kind": 5,
        "importPath": "ETC.secondary",
        "description": "ETC.secondary",
        "peekOfCode": "\t\tText = regex.sub(' ',s)\n\t\t# Get the words\n\t\twords = word_tokenize(unicode(s, errors='ignore'))\n\t\tw_i = -1;\n\t\t# Filter numbers\n\t\t#words= [str(t) for t in raw_tokens if not str(t).isdigit() and len(str(t))>1]\n\t\t#print '--->' + Text + '\\n'\n\t\ts_result = []\n\t\tfor w in words:\n\t\t\tw_i = w_i + 1;",
        "detail": "ETC.secondary",
        "documentation": {}
    },
    {
        "label": "\t\twords",
        "kind": 5,
        "importPath": "ETC.secondary",
        "description": "ETC.secondary",
        "peekOfCode": "\t\twords = word_tokenize(unicode(s, errors='ignore'))\n\t\tw_i = -1;\n\t\t# Filter numbers\n\t\t#words= [str(t) for t in raw_tokens if not str(t).isdigit() and len(str(t))>1]\n\t\t#print '--->' + Text + '\\n'\n\t\ts_result = []\n\t\tfor w in words:\n\t\t\tw_i = w_i + 1;\n\t\t\tif (tag_set.has_key(w)):\n\t\t\t\tn_gram.append(w)",
        "detail": "ETC.secondary",
        "documentation": {}
    },
    {
        "label": "\t\tw_i",
        "kind": 5,
        "importPath": "ETC.secondary",
        "description": "ETC.secondary",
        "peekOfCode": "\t\tw_i = -1;\n\t\t# Filter numbers\n\t\t#words= [str(t) for t in raw_tokens if not str(t).isdigit() and len(str(t))>1]\n\t\t#print '--->' + Text + '\\n'\n\t\ts_result = []\n\t\tfor w in words:\n\t\t\tw_i = w_i + 1;\n\t\t\tif (tag_set.has_key(w)):\n\t\t\t\tn_gram.append(w)\n\t\t\t\ttags.append(tag_set[w])",
        "detail": "ETC.secondary",
        "documentation": {}
    },
    {
        "label": "\t\ts_result",
        "kind": 5,
        "importPath": "ETC.secondary",
        "description": "ETC.secondary",
        "peekOfCode": "\t\ts_result = []\n\t\tfor w in words:\n\t\t\tw_i = w_i + 1;\n\t\t\tif (tag_set.has_key(w)):\n\t\t\t\tn_gram.append(w)\n\t\t\t\ttags.append(tag_set[w])\n\t\t\t\t#print \"n-gram = \"+n_gram[-1]\n\t\t\t#print w+'-'+str(words.index(w))\n\t\t\t# If this is the last word in the list or a non-NJ word, we finalize the n-gram\n\t\t\tif not(tag_set.has_key(w)) or (w_i == len(words)-1):",
        "detail": "ETC.secondary",
        "documentation": {}
    },
    {
        "label": "\t\t\tw_i",
        "kind": 5,
        "importPath": "ETC.secondary",
        "description": "ETC.secondary",
        "peekOfCode": "\t\t\tw_i = w_i + 1;\n\t\t\tif (tag_set.has_key(w)):\n\t\t\t\tn_gram.append(w)\n\t\t\t\ttags.append(tag_set[w])\n\t\t\t\t#print \"n-gram = \"+n_gram[-1]\n\t\t\t#print w+'-'+str(words.index(w))\n\t\t\t# If this is the last word in the list or a non-NJ word, we finalize the n-gram\n\t\t\tif not(tag_set.has_key(w)) or (w_i == len(words)-1):\n\t\t\t\t# Only if we found something\n\t\t\t\tif (len(n_gram) > 1):",
        "detail": "ETC.secondary",
        "documentation": {}
    },
    {
        "label": "\t\t\t\t\t\tn_gram_str",
        "kind": 5,
        "importPath": "ETC.secondary",
        "description": "ETC.secondary",
        "peekOfCode": "\t\t\t\t\t\tn_gram_str = ' '.join(n_gram)\n\t\t\t\t\t\ttag_str = ', '.join(tags)\n\t\t\t\t\t\tif not(n_gram_str in s_result) :\n\t\t\t\t\t\t\ts_result.append(n_gram_str)\n\t\t\t\tn_gram_str = n_gram_str.decode('utf-8')\n\t\t\t\tif (n_gram_str in Text):\n\t\t\t\t\tif not(results.has_key(n_gram_str)):\n\t\t\t\t\t\tresults[n_gram_str] = tag_str\n\t\t\t\telse:\n\t\t\t\t\tprint 'ngram not found in text: '+n_gram_str",
        "detail": "ETC.secondary",
        "documentation": {}
    },
    {
        "label": "\t\t\t\t\t\ttag_str",
        "kind": 5,
        "importPath": "ETC.secondary",
        "description": "ETC.secondary",
        "peekOfCode": "\t\t\t\t\t\ttag_str = ', '.join(tags)\n\t\t\t\t\t\tif not(n_gram_str in s_result) :\n\t\t\t\t\t\t\ts_result.append(n_gram_str)\n\t\t\t\tn_gram_str = n_gram_str.decode('utf-8')\n\t\t\t\tif (n_gram_str in Text):\n\t\t\t\t\tif not(results.has_key(n_gram_str)):\n\t\t\t\t\t\tresults[n_gram_str] = tag_str\n\t\t\t\telse:\n\t\t\t\t\tprint 'ngram not found in text: '+n_gram_str\n\t\t\t\t# Restart searching for next n-gram",
        "detail": "ETC.secondary",
        "documentation": {}
    },
    {
        "label": "\t\t\t\tn_gram_str",
        "kind": 5,
        "importPath": "ETC.secondary",
        "description": "ETC.secondary",
        "peekOfCode": "\t\t\t\tn_gram_str = n_gram_str.decode('utf-8')\n\t\t\t\tif (n_gram_str in Text):\n\t\t\t\t\tif not(results.has_key(n_gram_str)):\n\t\t\t\t\t\tresults[n_gram_str] = tag_str\n\t\t\t\telse:\n\t\t\t\t\tprint 'ngram not found in text: '+n_gram_str\n\t\t\t\t# Restart searching for next n-gram\n\t\t\t\tn_gram = []\n\t\t\t\ttags = []\n\t\t\t\tn_gram_str = ''",
        "detail": "ETC.secondary",
        "documentation": {}
    },
    {
        "label": "\t\t\t\t\t\tresults[n_gram_str]",
        "kind": 5,
        "importPath": "ETC.secondary",
        "description": "ETC.secondary",
        "peekOfCode": "\t\t\t\t\t\tresults[n_gram_str] = tag_str\n\t\t\t\telse:\n\t\t\t\t\tprint 'ngram not found in text: '+n_gram_str\n\t\t\t\t# Restart searching for next n-gram\n\t\t\t\tn_gram = []\n\t\t\t\ttags = []\n\t\t\t\tn_gram_str = ''\n\t\t\t\ttag_str = ''\n\tprint str(len(results.keys()))+\" n-grams found..\"\n\treturn results",
        "detail": "ETC.secondary",
        "documentation": {}
    },
    {
        "label": "\t\t\t\tn_gram",
        "kind": 5,
        "importPath": "ETC.secondary",
        "description": "ETC.secondary",
        "peekOfCode": "\t\t\t\tn_gram = []\n\t\t\t\ttags = []\n\t\t\t\tn_gram_str = ''\n\t\t\t\ttag_str = ''\n\tprint str(len(results.keys()))+\" n-grams found..\"\n\treturn results\ndef mutual_info (file, n_gram):\n    N00=0\n    N10=0\n    N01=0",
        "detail": "ETC.secondary",
        "documentation": {}
    },
    {
        "label": "\t\t\t\ttags",
        "kind": 5,
        "importPath": "ETC.secondary",
        "description": "ETC.secondary",
        "peekOfCode": "\t\t\t\ttags = []\n\t\t\t\tn_gram_str = ''\n\t\t\t\ttag_str = ''\n\tprint str(len(results.keys()))+\" n-grams found..\"\n\treturn results\ndef mutual_info (file, n_gram):\n    N00=0\n    N10=0\n    N01=0\n    N11=0",
        "detail": "ETC.secondary",
        "documentation": {}
    },
    {
        "label": "\t\t\t\tn_gram_str",
        "kind": 5,
        "importPath": "ETC.secondary",
        "description": "ETC.secondary",
        "peekOfCode": "\t\t\t\tn_gram_str = ''\n\t\t\t\ttag_str = ''\n\tprint str(len(results.keys()))+\" n-grams found..\"\n\treturn results\ndef mutual_info (file, n_gram):\n    N00=0\n    N10=0\n    N01=0\n    N11=0\n    N=N00+N10+N01+N11",
        "detail": "ETC.secondary",
        "documentation": {}
    },
    {
        "label": "\t\t\t\ttag_str",
        "kind": 5,
        "importPath": "ETC.secondary",
        "description": "ETC.secondary",
        "peekOfCode": "\t\t\t\ttag_str = ''\n\tprint str(len(results.keys()))+\" n-grams found..\"\n\treturn results\ndef mutual_info (file, n_gram):\n    N00=0\n    N10=0\n    N01=0\n    N11=0\n    N=N00+N10+N01+N11\n    mi_score=[]",
        "detail": "ETC.secondary",
        "documentation": {}
    },
    {
        "label": "\t\tcsvwriter",
        "kind": 5,
        "importPath": "ETC.secondary",
        "description": "ETC.secondary",
        "peekOfCode": "\t\tcsvwriter = csv.writer(output)\n\t\tcsvwriter.writerow([\"Data File\",  \"MI score\"])\n\t\tfor file in glob.glob(\"*.txt\"):\n\t\t\twith open(file, 'r') as reader:\n\t\t\t\ttext = ''\n\t\t\t\tfor line in reader:\n\t\t\t\t\ttext = text + line.rstrip('\\n\\r').lower()\n\t\t\t\tprint \"\\nProcessing \"+file\n\t\t\t\t# Tokenization\n\t\t\t\ttokens = get_tokens(text)",
        "detail": "ETC.secondary",
        "documentation": {}
    },
    {
        "label": "\t\t\t\ttext",
        "kind": 5,
        "importPath": "ETC.secondary",
        "description": "ETC.secondary",
        "peekOfCode": "\t\t\t\ttext = ''\n\t\t\t\tfor line in reader:\n\t\t\t\t\ttext = text + line.rstrip('\\n\\r').lower()\n\t\t\t\tprint \"\\nProcessing \"+file\n\t\t\t\t# Tokenization\n\t\t\t\ttokens = get_tokens(text)\n\t\t\t\t# Part of speech Tagging\n\t\t\t\ttag_set = get_pos_tags(tokens)\n\t\t\t\t# Technical N-gram extraction\n\t\t\t\tngrams = get_tech_ngrams(file, tag_set)",
        "detail": "ETC.secondary",
        "documentation": {}
    },
    {
        "label": "\t\t\t\t\ttext",
        "kind": 5,
        "importPath": "ETC.secondary",
        "description": "ETC.secondary",
        "peekOfCode": "\t\t\t\t\ttext = text + line.rstrip('\\n\\r').lower()\n\t\t\t\tprint \"\\nProcessing \"+file\n\t\t\t\t# Tokenization\n\t\t\t\ttokens = get_tokens(text)\n\t\t\t\t# Part of speech Tagging\n\t\t\t\ttag_set = get_pos_tags(tokens)\n\t\t\t\t# Technical N-gram extraction\n\t\t\t\tngrams = get_tech_ngrams(file, tag_set)\n        # mutual info extraction\n        mi= mutual info (text, n_gram)",
        "detail": "ETC.secondary",
        "documentation": {}
    },
    {
        "label": "\t\t\t\ttokens",
        "kind": 5,
        "importPath": "ETC.secondary",
        "description": "ETC.secondary",
        "peekOfCode": "\t\t\t\ttokens = get_tokens(text)\n\t\t\t\t# Part of speech Tagging\n\t\t\t\ttag_set = get_pos_tags(tokens)\n\t\t\t\t# Technical N-gram extraction\n\t\t\t\tngrams = get_tech_ngrams(file, tag_set)\n        # mutual info extraction\n        mi= mutual info (text, n_gram)\n\t\t\t\t# Write to output\n\t\t\t\tcsvwriter.writerow([file, mi_score.keys()])\nif __name__ == '__main__':",
        "detail": "ETC.secondary",
        "documentation": {}
    },
    {
        "label": "\t\t\t\ttag_set",
        "kind": 5,
        "importPath": "ETC.secondary",
        "description": "ETC.secondary",
        "peekOfCode": "\t\t\t\ttag_set = get_pos_tags(tokens)\n\t\t\t\t# Technical N-gram extraction\n\t\t\t\tngrams = get_tech_ngrams(file, tag_set)\n        # mutual info extraction\n        mi= mutual info (text, n_gram)\n\t\t\t\t# Write to output\n\t\t\t\tcsvwriter.writerow([file, mi_score.keys()])\nif __name__ == '__main__':\n    sys.exit(main())",
        "detail": "ETC.secondary",
        "documentation": {}
    },
    {
        "label": "\t\t\t\tngrams",
        "kind": 5,
        "importPath": "ETC.secondary",
        "description": "ETC.secondary",
        "peekOfCode": "\t\t\t\tngrams = get_tech_ngrams(file, tag_set)\n        # mutual info extraction\n        mi= mutual info (text, n_gram)\n\t\t\t\t# Write to output\n\t\t\t\tcsvwriter.writerow([file, mi_score.keys()])\nif __name__ == '__main__':\n    sys.exit(main())",
        "detail": "ETC.secondary",
        "documentation": {}
    }
]